[{"title":"test","url":"/2021/05/27/test/","content":"æµ‹è¯•ç»ˆäºæå¥½äº†555\nå¥½å¼€å¿ƒâ€¦..\n","tags":["æµ‹è¯•","è¯•ä¸€ä¸‹èƒ½ä¸èƒ½å¤šä¸ªæ ‡ç­¾"]},{"title":"åˆ©ç”¨hexoæ¡†æ¶æ­å»ºä¸ªäººåšå®¢","url":"/2021/05/29/%E5%88%A9%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","content":"åˆ©ç”¨hexoæ¡†æ¶æ­å»ºä¸ªäººåšå®¢å®‰è£…gitç”±äºæˆ‘ä¹‹å‰å®‰è£…äº†å°±ä¸è¯´äº†\nå¯ä»¥ç™¾åº¦æœç´¢ğŸ”gitå®˜ç½‘ã€‚gitæ˜¯å¯ä»¥ä¸€ç›´æ— è„‘ä¸‹ä¸€æ­¥å®‰è£…çš„ã€‚ç›¸ä¿¡è¿™ä¸ä¼šéš¾å€’èªæ˜çš„ä½ \nå®‰è£…nodejsè¿›å…¥å®˜ç½‘é€‰æ‹©å¯¹åº”çš„ç‰ˆæœ¬è¿›è¡Œä¸‹è½½\nå®‰è£…æ—¶ä¸€è·¯nextå³å¯ï¼ˆè·¯    å¾„ä½ è‡ªå·±é€‰ï¼‰\nå®‰è£…æˆåŠŸåï¼Œwin+Rï¼Œè¾“å…¥cmdè¿›å…¥å‘½ä»¤è¡Œï¼Œè¾“å…¥\nnode -vnpm -v\n\næ£€æŸ¥æ˜¯å¦å®‰è£…æˆåŠŸ\nå¦‚æœå‘ˆç°è¿™æ ·çš„å°±ç•Œé¢ï¼Œåˆ™è¡¨ç¤ºå®‰è£…æˆåŠŸ\n\nå¦å¤–ï¼Œå¯ä»¥ç”¨git bashä»£æ›¿å‘½ä»¤è¡Œæ¥æ•²å‘½ä»¤\nå®‰è£…hexoè‡ªå·±åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæˆ‘è¿™é‡Œæ–°å»ºäº†ä¸€ä¸ªblogæ–‡ä»¶å¤¹ï¼Œç‚¹è¿›å»åå³é”®git bash here\ngit bashå†…è¾“å…¥hexo init nameï¼Œè¿™é‡Œçš„nameä½ çˆ±æ€ä¹ˆå¡«æ€ä¹ˆå¡«\nè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæŠ¥é”™OpenSSL SSL_read: Connection was reset, errno 10054\næœ€æœ‰å¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šï¼Œgithubç»å¸¸è¿™æ ·ï¼Œè‡³äºåŸå› ï¼Œå¤§å®¶æ‡‚å¾—éƒ½æ‡‚ï¼Œæœ‰æ—¶å€™ç§‘å­¦ä¸Šç½‘ä¹‹åä¹Ÿæ˜¯è¿™æ ·ã€‚\nè¿™é‡Œæˆ‘è¾“å…¥äº†hexo init Horace\n\n\nå‡ºç°è¿™æ ·çš„ç•Œé¢å³åˆå§‹åŒ–æˆåŠŸ\néšåï¼Œcd Horaceè¿›å…¥åˆšåˆšåˆå§‹åŒ–çš„æ–‡ä»¶å¤¹å†…ï¼Œè¾“å…¥npm install\n\néšåè¾“å…¥\nhexo ghexo server\n\næˆ–æ˜¯è¾“å…¥hexo s\næ‰“å¼€hexoæœåŠ¡ï¼Œæµè§ˆå™¨è¾“å…¥localhost:4000ï¼Œå³å¯çœ‹åˆ°åˆšåˆšåˆ›å»ºçš„åšå®¢äº†ã€‚é•¿è¿™æ ·å­ï¼š\n\nå¯ä»¥é€šè¿‡Ctrl+Cåœæ­¢hexoæœåŠ¡ã€‚\nåœæ­¢æœåŠ¡åï¼Œå†è¾“å…¥localhost:4000å°±ç™»ä¸ä¸Šç½‘ç«™äº†ã€‚\néƒ¨ç½²åˆ°äº‘ç«¯è¿™é‡Œå¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬è¾“å…¥çš„ç½‘å€æ˜¯localhost:4000ï¼Œå¹¶ä¸æ˜¯ä¸ªé™æ€é“¾æ¥\nè€Œæˆ‘ä»¬æ­ä¸ªäººåšå®¢æœ¬èº«å°±æ˜¯æƒ³åˆ«äººæ¥çœ‹çš„ï¼Œå¦‚æœåˆ«äººéƒ½ç‚¹ä¸è¿›æ¥ï¼Œé‚£æœ‰ä»€ä¹ˆæ„ä¹‰å•Šï¼Ÿ\næ‰€ä»¥ï¼Œè¦è®©æˆ‘ä»¬çš„ä¸ªäººåšå®¢å¯ä»¥è¢«åˆ«äººè®¿é—®åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„ç½‘ç«™éƒ¨ç½²åˆ°äº‘ç«¯ï¼ˆä¸æ˜¯å”¯ä¸€çš„æ–¹å¼ï¼‰\nè¿™é‡Œå°±è®©githubæ‰˜ç®¡æˆ‘ä»¬çš„åšå®¢ã€‚\nåˆ›å»ºgithubè´¦æˆ·ä¸åˆ›å»ºå¯¹åº”ä»“åº“æ³¨å†Œä¸€ä¸ªgithubè´¦æˆ·ï¼Œåˆ›å»ºï¼ˆnewï¼‰ä¸€ä¸ªä»“åº“ï¼Œä»“åº“åå«ç”¨æˆ·å+.github.io\næˆ‘çš„ç”¨æˆ·åæ˜¯horacehhtï¼Œå› æ­¤åˆ›å»ºä¸€ä¸ªhoracehht.github.ioçš„ä»“åº“\nç”ŸæˆSSHæ·»åŠ åˆ°githubå‚è€ƒå»–é›ªå³°åšå®¢gitæ–‡ç« \nå°†hexoéƒ¨ç½²åˆ°äº‘ç«¯é€šè¿‡npm install hexo-deployer-git --saveå‘½ä»¤ä¸‹è½½éƒ¨ç½²çš„ç›¸åº”æ’ä»¶\nç„¶å\nhexo cleanhexo generatehexo deploy\n\nä½†æ˜¯ä¹‹åè®¿é—®http://horacehht.github.ioæŠ¥é”™404ï¼Œè¯´There isn&#39;t a GitHub Pages site here.\næœ€åé€šè¿‡github pageçš„å®˜æ–¹æ–‡æ¡£å¾—çŸ¥ï¼Œå¦‚æœè¦ä½œä¸ºä¸€ä¸ªGithub Pagesä»“åº“ï¼Œéœ€æ»¡è¶³ä¸‰ä¸ªæ¡ä»¶ï¼š\n\nä»“åº“åä¸ºç”¨æˆ·å+github.io\nä»“åº“åº”è®¾ä¸ºpublicï¼ˆå…¬å¼€ï¼‰\nä»“åº“å†…è¦åˆ›å»ºä¸€ä¸ªREADMEæ–‡æ¡£\n\nhexoçš„åŸºæœ¬å‘½ä»¤\n\n\nå‘½ä»¤\nä½œç”¨\n\n\n\nHexo init\nåˆå§‹åŒ–åšå®¢\n\n\nHexo s\nè¿è¡Œåšå®¢\n\n\nHexo n title\nåˆ›å»ºä¸€ç¯‡æ–°çš„æ–‡ç« ï¼Œtitleä¸ºæ–‡ç« æ ‡é¢˜\n\n\nHexo c(clean)\næ¸…ç†æ–‡ä»¶\n\n\nHexo g(GENERATE)\nç”Ÿæˆé™æ€æ–‡ä»¶\n\n\nHexo d(deploy)\néƒ¨ç½²åšå®¢\n\n\n\n\nhexoé¡¹ç›®çš„_config.ymlæ˜¯æ•´ä¸ªhexoé¡¹ç›®çš„æ€»é…ç½®æ–‡ä»¶ï¼Œå¦‚æœéœ€è¦é…ç½®ä¸»é¢˜ï¼Œåˆ™è¿˜æœ‰å¯¹åº”ä¸»é¢˜çš„é…ç½®æ–‡ä»¶ã€‚\nä¸»é¢˜æŒ‘é€‰æŒ‘é€‰ä¸»é¢˜ä¸­â€¦selecting\n1.Yunä¸»é¢˜æ˜¯æˆ‘æœ€å–œæ¬¢çš„ä¸€ä¸ªä¸»é¢˜ï¼Œä½†æ˜¯å› ä¸ºä¸€äº›ç‰¹æ®ŠåŸå› ï¼Œæ€•å®¡ç¾ç–²åŠ³ï¼Œå°±ä¸ç”¨äº†ã€‚demoï¼šhttps://www.yunyoujun.cn/\ngithubåœ°å€ï¼šhttps://github.com/YunYouJun/hexo-theme-yun\n2.Anatoleä¸»é¢˜ä¹Ÿå¯ï¼Œå¾ˆç®€æ´ï¼Œä½†ä¸å–œæ¬¢ã€‚demoï¼šhttps://www.jixian.io/\ngithubåœ°å€ï¼šhttps://github.com/mrcore/hexo-theme-Anatole-Core\n3.Ayerä¸»é¢˜ã€‚å¾ˆå…¨é¢çš„ä¸€ä¸ªä¸»é¢˜ã€‚demoï¼šhttps://shen-yu.gitee.io/\ngithubåœ°å€ï¼šhttps://github.com/Shen-Yu/hexo-theme-ayer\næ„Ÿè§‰éå¸¸ä¸é”™ã€‚\n4.Particleä¸»é¢˜ã€‚ç®€æ´ã€‚ä½†æ–‡ç« æ²¡æœ‰ç›®å½•ã€‚demoï¼šhttps://korilin.com/\ngithubåœ°å€ï¼šhttps://github.com/korilin/hexo-theme-particle\n5.shokaä¸»é¢˜ã€‚äº¤äº’æ€§å¼ºï¼Œå­—ä½“å¥½çœ‹ï¼Œå¼•ç”¨å—å¥½çœ‹ã€‚demoï¼šhttps://shoka.lostyu.me/\ngithubåœ°å€ï¼šhttps://github.com/amehime/hexo-theme-shoka\næœ‰ç‚¹èŠ±\n6.Keepä¸»é¢˜ã€‚åˆ‡æ¢è‡ªç„¶ã€‚ä¸»é¡µé¢ç®€æ´ã€‚demoï¼šhttps://xpoet.cn/\ngithubåœ°å€ï¼šhttps://github.com/XPoet/hexo-theme-keep\næœ€ç»ˆé€‰å®šKeepè¿™ä¸ªä¸»é¢˜ï¼ç®€æ´åˆå¥½çœ‹ \nä½¿ç”¨Keepä¸»é¢˜æ³¨ï¼šé…ç½®æ–‡ä»¶æœ‰ä¸¤ä¸ªï¼šæ€»é…ç½®æ–‡ä»¶ï¼Œä¸»é¢˜é…ç½®æ–‡ä»¶ã€‚ä¸‹æ–‡æåˆ°æ—¶æ³¨æ„åŒºåˆ†\nå®‰è£…ä¸»é¢˜åœ¨git bashä¸­æ‰§è¡Œå‘½ä»¤\ngit clone https://github.com/XPoet/hexo-theme-keep themes/keep\n\nç„¶åthemesæ–‡ä»¶å¤¹ä¸‹å‡ºç°æˆ‘ä»¬æƒ³è¦çš„keepä¸»é¢˜ã€‚\n\nè¿™ä¸ªä¸ºä¸»é¢˜é…ç½®æ–‡ä»¶ï¼ˆè¿™é‡Œè¸©äº†ä¸€äº›å‘ï¼Œé€šè¿‡npmå®‰è£…çš„æ²¡æœ‰è¿™ä¸ªæ–‡ä»¶å¤¹ï¼‰\nä½¿ç”¨ä¸»é¢˜å®‰è£…å®Œæˆåï¼Œåœ¨ Hexoé¡¹ç›®çš„æ€»é…ç½®æ–‡ä»¶ä¸­å°† theme è®¾ç½®ä¸º keepã€‚\ntheme: keep\n\nè¿™ä¸ªæ–‡ä»¶çš„è·¯å¾„ä¸ºname\\_config.ymlï¼Œæˆ‘çš„ä¸ºHorace\\_config.yml\n\nkeepä¼šä¸å®šæœŸæ›´æ–°ç‰ˆæœ¬ï¼Œå¯é€šè¿‡å¦‚ä¸‹å‘½ä»¤æ›´æ–°Keepã€‚\n\né€šè¿‡ npm å®‰è£…æœ€æ–°ç‰ˆæœ¬ï¼š\n$ cd hexo-siteï¼ˆhexoé¡¹ç›®çš„ä½ç½®ï¼‰$ npm update hexo-theme-keep\n\næˆ–\n\né€šè¿‡ git æ›´æ–°åˆ°æœ€æ–°çš„ master åˆ†æ”¯ï¼š\n$ cd themes/keep$ git pull\n\né…ç½®æŒ‡å—å¤åˆ¶ä¸»é¢˜é…ç½®æ–‡ä»¶ã€‚å›åˆ°æ•´ä¸ªåšå®¢é¡¹ç›®ç›®å½•ä¸‹çš„sourceæ–‡ä»¶å¤¹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹_dataï¼Œå°†è¯¥æ–‡ä»¶ç²˜è´´è¿›å»\n\nå°†æ–‡ä»¶åæ›´æ”¹ä¸ºkeep.ymlï¼Œè¿™å¾ˆé‡è¦ï¼\nç„¶åé€šè¿‡è¯¥æ–‡ä»¶æ¥è¿›è¡Œç›¸åº”çš„ä¿®æ”¹å³å¯å®ç°å¯¹ä¸»é¢˜çš„é…ç½®ï¼\nè¿™äº›æ˜¯keepå®˜æ–¹æä¾›çš„é…ç½®èµ„æ–™ï¼Œå·²ç»å¾ˆå…¨é¢äº†ã€‚\nhttps://keep-docs.xpoet.cn/usage-tutorial/quick-start.html#%E5%AE%89%E8%A3%85\nhttps://keep-docs.xpoet.cn/usage-tutorial/configuration-guide.html#base-info\nhttps://keep-docs.xpoet.cn/usage-tutorial/advanced.html\nè¿™ä¸‰ä¸ªéƒ½æ˜¯å®˜æ–¹ç»™çš„keepä½¿ç”¨æ•™ç¨‹ï¼Œåˆ†åˆ«ä¸ºå¿«é€Ÿå¼€å§‹ï¼Œé…ç½®æŒ‡å—å’Œè¿›é˜¶ä½¿ç”¨ï¼Œèƒ½æ»¡è¶³å¤§éƒ¨åˆ†äººçš„éœ€æ±‚\n\nä¸‹é¢ä»…ä»¥base_infoä¸ºä¾‹è®²è§£ï¼Œå…¶ä»–çš„é…ç½®é¡¹è¯·å‚è€ƒå®˜æ–¹èµ„æ–™\nbase_infoé¡¹\næ ¹æ®è‡ªå·±çš„å†…å®¹è¿›è¡Œå¡«å†™\nbase_info:  title: Horaceã®äº‘ç«¯æ¢¦å¢ƒ  author: Horace  url: https://horacehht.github.io/  # è¿™é‡Œå¡«ä¸Šhttps://ç”¨æˆ·å.github.io\tå¦‚æœè‡ªå·±æ³¨å†Œäº†åŸŸåï¼Œå°±æ”¹æˆæ³¨å†Œçš„  # å›¾æ ‡çš„é“¾æ¥ï¼Œå¯ä»¥ç”¨æœ¬åœ°çš„å›¾ç‰‡ï¼Œä¹Ÿå¯ç”¨å›¾ç‰‡é“¾æ¥ï¼Œæˆ–è€…ä¸å¡«  logo_img: https://horacehhtbucket.oss-cn-guangzhou.aliyuncs.com/img/ç½‘ç«™å›¾æ ‡.jpg  # è¿™é‡Œæˆ‘å¡«äº†ä¸€ä¸ªç½‘é¡µé“¾æ¥\n\nè¯»è€…å¯ä»¥ç¨å¾®äº†è§£å›¾åºŠï¼Œè¿™é‡Œæˆ‘è´­ä¹°äº†é˜¿é‡Œäº‘çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ossä½œä¸ºå›¾åºŠæ”¾ç½®æˆ‘çš„å›¾ç‰‡\nè¯»è€…åŒæ ·ä¹Ÿå¯ä½¿ç”¨å…è´¹çš„å›¾åºŠï¼š\n\nsm.ms\n\nè·¯è¿‡å›¾åºŠ\n\n\nåˆ©ç”¨é˜¿é‡Œäº‘ä½œå›¾åºŠçš„æ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/138878534\nå°tipsæ–‡ç« éƒ½æ”¾åœ¨sourceçš„_postsæ–‡ä»¶å¤¹ä¸‹\næ–‡ç« å¦‚æœæƒ³æ”¾åœ¨å¤šä¸ªåˆ†ç±»æˆ–å¤šä¸ªæ ‡ç­¾ä¸‹ï¼ˆå‰ææ˜¯ä½ å¼€äº†åˆ†ç±»å’Œæ ‡ç­¾çš„åŠŸèƒ½ï¼‰ï¼Œéœ€è¦å†™æˆ[a,b,c]çš„æ ¼å¼ï¼Œå¦‚å›¾ï¼š\n\næ€»ç»“ä¹‹åæ¯æ¬¡å†™æ–°æ–‡ç« ï¼Œå°±è¿›git bashä¸­æ•²\nhexo cleanhexo generatehexo deploy\n\nè¿™æ ·å°±èƒ½åœ¨è‡ªå·±çš„åšå®¢ç½‘ç«™ä¸Šçœ‹åˆ°æ–°å‘å¸ƒçš„æ–‡ç« äº†ï¼\nå¦‚æœå«Œéº»çƒ¦çš„å¯ä»¥å‚è€ƒè¿™ç¯‡æ–‡ç« è¿›è¡Œè‡ªåŠ¨éƒ¨ç½²\næˆ‘çš„åšå®¢ç½‘ç«™ä¸ºï¼šhttps://horacehht.github.io\næ¬¢è¿æ¥è®¿\n","categories":["hexo"]},{"title":"è®¾ç½®ä»£ç†çˆ¬å–è±†ç“£ä¹¦ç±","url":"/2021/05/29/%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E4%B9%A6%E7%B1%8D/","content":"è®¾ç½®ä»£ç†çˆ¬å–è±†ç“£ä¹¦ç±ç®€ä»‹çˆ¬å–ç½‘é¡µå¤§éƒ½å¯åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š\n\nè®¿é—®ç½‘é¡µï¼šä¸€èˆ¬ä½¿ç”¨requestsåº“ï¼Œä¸å«Œéº»çƒ¦çš„è¯å¯ä»¥ä½¿ç”¨pythonå†…ç½®çš„urllibåº“\nè§£æç½‘é¡µï¼šä½¿ç”¨Xpathï¼ŒBeautifulSoupï¼Œæ­£åˆ™è¡¨è¾¾å¼ç­‰æ–¹å¼è¿›è¡Œç½‘é¡µä¿¡æ¯çš„æå–\nå­˜å‚¨æ•°æ®ï¼šâ‘ å­˜å…¥IOæµæ–‡ä»¶ï¼Œå¦‚txtï¼Œcsvç­‰æ–‡ä»¶    â‘¡å­˜å…¥æ•°æ®åº“ï¼Œä¸»æµçš„æœ‰MySQLï¼Œmongodb\n\næœ¬æ–‡å¼€å‘ç¯å¢ƒä¸ºpython3.8ï¼Œçˆ¬å–çš„æ•°æ®å­˜å…¥MySQLæ•°æ®åº“ä¸­\nè®¿é—®ç½‘é¡µimport requestsurl = &#x27;ç›®æ ‡ç½‘å€&#x27;res = requests.get(url)\n\nè¿™æ ·å³å¯å®Œæˆä¸€æ¬¡ç½‘é¡µçš„è®¿é—®ã€‚ä½†ä¸€èˆ¬éƒ½è¦åŠ ä¸Šè¯·æ±‚å¤´ï¼Œç§°ä¸ºheadersã€‚ä¸€èˆ¬çš„ç½‘ç«™è¯·æ±‚çš„headersä¸­åŠ å…¥User-Agenté¡¹å‚æ•°å³å¯ã€‚\nå¦‚æœä½ ç”¨çš„æ˜¯è°·æ­Œæµè§ˆå™¨ï¼Œå¯åœ¨ç½‘é¡µæ è¾“å…¥chrome::versionæŸ¥çœ‹è‡ªå·±çš„User-Agenté¡¹ï¼Œå«â€œç”¨æˆ·ä»£ç†â€\n\nå¦‚æœä¸æ˜¯ï¼Œä¹Ÿå¯ä»¥æŒ‰F12ï¼ŒCtrl+Rï¼Œéšä¾¿ç‚¹è¿›ä¸€ä¸ªè¯·æ±‚ï¼ŒRequests Headersé¡¹ä¸­çš„User-Agentå°±æ˜¯ä½ çš„User-Agentã€‚\näºæ˜¯ä»£ç åº”è¯¥æ”¹æˆè¿™æ ·\nheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;å¡«å…¥ä½ çš„user-agent&#x27;    ...æ ¹æ®ä¸åŒçš„ç½‘ç«™åŠ å…¥ä¸åŒçš„å‚æ•°&#125;res = requests.get(url, headers=headers)\n\nä¸åŠ çš„è¯ï¼Œä½ çš„è¯·æ±‚ä¸­User-Agentçš„å€¼åˆ™æ˜¯pythonçˆ¬è™«ï¼Œç½‘ç«™å°±ä¼šæ‹’ç»è®¿é—®ï¼Œè¿™æ ·ä½ å°±æ— æ³•å¾—åˆ°ç½‘ç«™è¿”å›çš„æ•°æ®ã€‚\nè®¾ç½®ä»£ç†ä¸ºä»€ä¹ˆè¦è®¾ç½®ä»£ç†ï¼ŸèƒŒæ™¯ï¼šæœ¬äººè¦åœ¨æŸæ¬¡è€ƒæ ¸ä»»åŠ¡æœ€åä¸€å¤©ä¸­çˆ¬å–æ•°æ®é‡è¶…è¿‡3k5çš„æ•°æ®ï¼Œæ—¶é—´ç´§ï¼Œæ•°æ®é‡è¯´å°ä¸å°ï¼Œè¯´å¤§ä¸å¤§ã€‚å¦‚æœé‡‡ç”¨å¹³å¸¸çš„çˆ¬è™«æ–¹æ³•ï¼Œæ¯ä¸€æ¬¡time.sleepå‡ ç§’ï¼Œè¿™æ ·ä¹…äº†ï¼Œè±†ç“£è‡ªç„¶ä¼šå‘ç°ï¼Œç„¶åæŠŠipç»™å°äº†ï¼Œè¿™æ ·å°±æ²¡åŠæ³•çˆ¬äº†ï¼Œè€ƒæ ¸ä»»åŠ¡å°±æ³¡æ±¤äº†â€¦\né¢˜å¤–è¯ï¼štime.sleep()è®¾ç½®çš„ç§’æ•°æœ€å¥½æ˜¯éšæœºæ•°ï¼Œå¦‚æœæ˜¯å›ºå®šçš„ç§’æ•°ï¼Œä¹…è€Œä¹…ä¹‹ä¹Ÿå¾ˆå®¹æ˜“è¢«å°ipã€‚\næ‰€ä»¥ï¼Œè®¾ç½®ä»£ç†è¿™ç§åˆ‡æ¢ipçš„æ–¹å¼å°±å¾ˆé€‚åˆçŸ­æ—¶é—´çˆ¬å–å¤§é‡æ•°æ®\nä»£ç†æœ‰å¤šç§\nâ‘ è‡ªå·±å»çˆ¬å–å…è´¹çš„ä»£ç†ï¼Œå»ºç«‹è‡ªå·±çš„ä»£ç†æ± ï¼Œéš¾åº¦å¤§ï¼ŒæŠ€æœ¯è¦æ±‚é«˜ï¼Œä¸”å¤§å¤šå…è´¹ä»£ç†éƒ½æ˜¯ç”¨ä¸äº†çš„ã€‚\nâ‘¡ä½¿ç”¨ä»˜è´¹ä»£ç†\næœ¬æ–‡ä¸­ä½¿ç”¨çš„æ˜¯ä»˜è´¹ä»£ç†ï¼Œå«å¤šè´äº‘ä»£ç†ï¼Œè´­ä¹°äº†httpéš§é“ä»£ç†ä¸­çš„å¥—é¤3ï¼Œå¥—é¤çš„ç‰¹ç‚¹æ˜¯ï¼šæ¯ä¸ªè¯·æ±‚éšæœºåˆ†é…IP\næ³¨ï¼šè´­ä¹°æ—¶éœ€è¦å®åè®¤è¯\nè·å–åˆ†é…çš„ipæ³¨ï¼šä¸åŒçš„ä»˜è´¹ä»£ç†ä¸åŒçš„å¥—é¤è·å–ipçš„æ–¹å¼ä¸åŒï¼Œæ ¹æ®å®˜æ–¹æŒ‡ç¤ºå³å¯\nè´­ä¹°åï¼Œå¤šè´äº‘ä¼šåˆ†é…ä¸€ä¸ªè´¦å·ï¼Œå¯†ç å’ŒæœåŠ¡å™¨åœ°å€ç»™ä½ \n\næ ¹æ®ä»–çš„æŒ‡å¼•æ„é€ ä»£ç†å‚æ•°å³å¯\n\näºæ˜¯æˆ‘ä»¬å‘æœåŠ¡å™¨åœ°å€ç«¯å£è¯·æ±‚ï¼ŒæœåŠ¡å™¨å³å¯è¿”å›ä¸€ä¸ªå¯ç”¨çš„ipæ¥ä¼ªè£…æˆ‘ä»¬çš„ip\nè®¿é—®ç½‘é¡µå‡½æ•°äºæ˜¯æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè®¿é—®ç½‘é¡µçš„å‡½æ•°ï¼Œè¿”å›å“åº”ã€‚\ndef visit(targetUrl):    &quot;&quot;&quot;è®¿é—®ç½‘å€ï¼Œè¿”å›å“åº”&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return res\n\nè¿™é‡Œæˆ‘è¿˜ç”¨äº†éšæœºUAï¼Œå°±æ˜¯æ‰¾äº†ä¸åŒçš„user-agentï¼Œæ¯æ¬¡è®¿é—®ä»ä¸­éšæœºé€‰å–ä¸€ä¸ªuaã€‚\nè¿™é‡Œçš„headersæ˜¯ä¸ªåˆ—è¡¨\n\nè§£æç½‘é¡µæŒ‰F12æˆ–å³é”®æ£€æŸ¥è¿›å…¥â€œå¼€å‘è€…é€‰é¡¹â€ï¼Œåˆ©ç”¨å›¾ç‰‡ä¸­çº¢æ¡†çš„åŠŸèƒ½å¯ä»¥è¿…é€Ÿå®šä½æ‰€æå–ä¿¡æ¯çš„èŠ‚ç‚¹ä½ç½®\n\næ­¤å¤„è¿ç”¨XPthå’ŒBeautifulSoupè¿›è¡Œç½‘é¡µçš„è§£æï¼Œå¹¶å°†è§£ææ–¹æ³•ç¼–å†™æˆç±»ã€‚\næ³¨ï¼šå»ºè®®å¯¹è‡ªå·±çš„è§£ææ–¹æ³•å¤šå¯¹å‡ æœ¬ä¹¦è¿›è¡Œå°è¯•ï¼Œå› ä¸ºä¸åŒç½‘é¡µæ’ç‰ˆå¯èƒ½ä¸åŒå™¢~\nçˆ¬å–çš„å­—æ®µä¸ºbook_nameï¼ˆä¹¦åï¼‰ï¼Œauthorï¼ˆä½œè€…ï¼‰ï¼Œpressï¼ˆå‡ºç‰ˆç¤¾ï¼‰ï¼Œpublishing_yearï¼ˆå‡ºç‰ˆå¹´ä»½ï¼‰ï¼Œpage_numï¼ˆé¡µæ•°ï¼‰ï¼Œpriceï¼ˆå®šä»·ï¼‰ï¼ŒISBNï¼Œscoreï¼ˆè¯„åˆ†ï¼‰ï¼Œrating_numï¼ˆè¯„åˆ†äººæ•°ï¼‰ï¼Œcontent_introductionï¼ˆå†…å®¹ç®€ä»‹ï¼‰ï¼Œcover_urlï¼ˆå°é¢å›¾ç‰‡ç½‘é¡µé“¾æ¥ï¼‰ï¼Œreadersï¼ˆè¯»è€…çš„ä¸ªäººä¸»é¡µé“¾æ¥ï¼‰\n# åº“å¯¼å…¥éƒ¨åˆ†import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # åˆå§‹åŒ–            self.html = etree.HTML(res.text)  # åˆå§‹åŒ–            self.data = dict()  # ç”Ÿæˆä¸€ä¸ªç©ºå­—å…¸            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;å‡ºç‰ˆç¤¾:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;å‡ºç‰ˆå¹´:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;é¡µæ•°:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;å®šä»·:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # æ‰¾åˆ°èŠ‚ç‚¹åä¸ºspanï¼Œå±æ€§propertyå€¼ä¸ºitemreviewdçš„èŠ‚ç‚¹ï¼Œ.stringè·å–å…¶æ–‡æœ¬å†…å®¹        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œä¸‹é¢çš„è§£ææ–¹æ³•å¤§åŒå°å¼‚    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?ä½œè€….*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;å‡ºç‰ˆç¤¾:&#x27;, &#x27;å‡ºç‰ˆå¹´:&#x27;, &#x27;é¡µæ•°:&#x27;, &#x27;å®šä»·:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # è·å–èŠ‚ç‚¹çš„srcå±æ€§å€¼        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # è¿™é‡Œç”¨çš„æ˜¯xpathçš„è§£ææ–¹æ³•ï¼Œè·å–æ‰€æœ‰å±æ€§å€¼ä¸ºcollectorä¸‹çš„æ‰€æœ‰divèŠ‚ç‚¹ä¸‹çš„ç¬¬äºŒä¸ªdivèŠ‚ç‚¹çš„aèŠ‚ç‚¹çš„hrefå±æ€§å€¼        self.data.setdefault(&#x27;readers&#x27;, readers)\n\næ³¨ï¼šå­—å…¸çš„setdefaultå‡½æ•°æ˜¯æ·»åŠ é”®å€¼å¯¹çš„ä¸€ç§æ–¹å¼ï¼Œå¦‚æœå·²æœ‰è¿™ä¸ªé”®åˆ™ä¸æ·»åŠ ï¼Œæ²¡æœ‰åˆ™æ·»åŠ ã€‚\nè¿™å¯ä»¥çª¥æ¢å‡ºæˆ‘ä¸ºä»€ä¹ˆåœ¨try-exceptè¯­å¥çš„exceptä¸­åŠ ä¸Šç›¸åº”å­—æ®µçš„setdefaultã€‚å› ä¸ºè±†ç“£çš„ä¹¦ç±é—´ç½‘é¡µæ’å¸ƒæ˜¯ä¸åŒçš„ï¼Œå®ƒå°±æ˜¯æ¯”è¾ƒç‰¹æ®Šï¼Œæœ‰äº›ä¹¦æ²¡æœ‰ISBNï¼Œæœ‰äº›ä¹¦æ²¡æœ‰è¯„åˆ†å’Œè¯„è®ºäººæ•°â€¦æ‰€ä»¥ä»¥å›ºå®šçš„æ–¹å¼å»æå–è¿™äº›å­—æ®µï¼Œå¿…ä¼šæŠ¥é”™ï¼Œæ‰€ä»¥é‡åˆ°è¿™ç§æŠ¥é”™æ—¶ï¼Œè¿›åˆ°exceptè¯­å¥ä¸ºæ•°æ®èµ‹ä¸Šä¸€äº›æ–¹ä¾¿å¤„ç†çš„ç©ºå€¼ã€‚è™½è¯´æˆ‘ä¹Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆä¹‹åè¿˜æ˜¯æœ‰ç©ºå€¼\nå°æ’å…¥ä¸€å¥ï¼Œå¼‚å¸¸å¤„ç†çœŸçš„å¾ˆé‡è¦ï¼ï¼ï¼\ntry:    ä¸€äº›å¯èƒ½ä¼šå‡ºé”™çš„è¯­å¥except Exception as e:    print(e)  # è¿™ä¸ªprint(e)å¯ä»¥è®©æˆ‘ä»¬çœ‹åˆ°å‡ºé”™çš„åŸå› ï¼Œ    pass  # è¿™é‡Œçš„passä½ å¯ä»¥å¡«å…¥ä½ å¼‚å¸¸å¤„ç†çš„è¯­å¥\n\nè¿™æ ·å­æ‰€æœ‰çš„å­—æ®µæ•°æ®éƒ½å­˜å…¥äº†dataä¸­ï¼Œä¹‹åæˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ªå¯¹è±¡ï¼Œå¯¹è±¡.dataå³å¯æŸ¥çœ‹æˆ‘ä»¬çˆ¬å–çš„æ•°æ®å•¦ã€‚\næˆ‘ä»¬ä»¥ã€Šè¿½é£ç­çš„äººã€‹ä¸ºä¾‹ï¼š\n\nå­˜å‚¨æ•°æ®ä½¿ç”¨MySQLæ•°æ®åº“è¿›è¡Œæ•°æ®çš„å­˜å‚¨\nå»ºè¡¨è¦æ ¹æ®çˆ¬å–çš„å­—æ®µå»ºç«‹ç›¸åº”çš„è¡¨ï¼ˆå¦‚æœæ˜¯æ–°ç”¨æˆ·è¿˜è¦æ–°å»ºè¿æ¥ï¼Œè¿™é‡Œå°±ä¸å¤šèµ˜è¿°äº†ï¼‰\nå¯ä»¥ç”¨pythonå»ºè¡¨ï¼Œä¹Ÿå¯ä»¥ç”¨navicatï¼ˆMySQLçš„ä¸€ä¸ªå¯è§†åŒ–å·¥å…·ï¼‰å»ºè¡¨ã€‚\nè¿™é‡Œç”¨pythonå»ºè¡¨\npythoné€šè¿‡ç¬¬ä¸‰æ–¹åº“pymysqlä¸MySQLä¸æ•°æ®åº“è¿›è¡Œäº¤äº’ï¼Œå¯¹æ•°æ®è¿›è¡Œå¢åˆ æŸ¥æ”¹ã€‚\né¦–å…ˆè¦import pymysqlï¼Œæ²¡æœ‰å®‰è£…åº“çš„å°±å»å®‰è£…ã€‚\nç›´æ¥è´´ä»£ç ï¼š\nimport pymysqlconn = pymysql.connect(  # è¿æ¥æœ¬åœ°æ•°æ®åº“    host=&quot;localhost&quot;,    user=&quot;root&quot;,  # è¦å¡«root    password=&quot;htht0928&quot;,  # å¡«ä¸Šè‡ªå·±çš„å¯†ç     database=&quot;doubanbook&quot;,  # æ•°æ®åº“å    charset=&quot;utf8&quot;)cur = conn.cursor()  # è·å¾—å…‰æ ‡create_books_table_sql = &quot;&quot;&quot;     CREATE TABLE `books`(    `book_name` VARCHAR(20) NOT NULL UNIQUE,    `author` VARCHAR(20) NOT NULL,    `press` VARCHAR(20),    `publishing_year` VARCHAR(10),    `score` FLOAT,    `rating_num` INTEGER,    `page_num` VARCHAR(10),    `price` VARCHAR(10),    `ISBN` VARCHAR(30),    `content_introduction` VARCHAR(2000),    `cover_url` VARCHAR(100),    `readers` VARCHAR (400)    )&quot;&quot;&quot;  # sqlè¯­å¥try:    cur.execute(create_books_table_sql)  # æ‰§è¡Œsqlè¯­å¥except Exception as e:    print(e)    conn.rollback()  # å‘ç”Ÿé”™è¯¯åˆ™å›æ»š\n\nè¿è¡Œåå³å¯å»ºç«‹ç›¸åº”çš„è¡¨ã€‚\næ’å…¥æ•°æ®ç¼–å†™save_to_mysqlå‡½æ•°\ndef save_to_mysql(data):    &quot;&quot;&quot;dataæ˜¯ä¹¦ç±çš„ä¿¡æ¯ï¼Œjsonæ ¼å¼ï¼Œè¦æ’å…¥åˆ°booksè¿™ä¸ªè¡¨&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;å‡ºç‰ˆç¤¾:&#x27;)    publishing_year = data.get(&#x27;å‡ºç‰ˆå¹´:&#x27;)    page_num = data.get(&#x27;é¡µæ•°:&#x27;)    price = data.get(&#x27;å®šä»·:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;  # å‘å­—æ®µä¸­å¢æ·»ç›¸åº”çš„æ•°æ®    try:        # æ‰§è¡Œsqlè¯­å¥        cur.execute(insert_sql, insert_data)        # æäº¤æ‰§è¡Œ        conn.commit()        print(&#x27;ã€Š&#x27; + book_name + &#x27;ã€‹&#x27; + &#x27;ä¿¡æ¯å·²å­˜å‚¨æˆåŠŸ!&#x27;)  # æ–¹ä¾¿æˆ‘ä»¬çœ‹åˆ°çˆ¬å–çš„è¿‡ç¨‹    except Exception as e:        print(e)        conn.rollback()        print(&#x27;å­˜å‚¨å¤±è´¥!&#x27;)\n\næ³¨ï¼šæå–å­—å…¸æ•°æ®æ—¶æ²¡ç”¨dict[â€˜keyâ€™]çš„æ–¹æ³•è·å–çš„åŸå› æ˜¯å¦‚æœæ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œä¼šç›´æ¥æŠ¥é”™ï¼Œæ•´ä¸ªç¨‹åºç›´æ¥åœä¸‹æ¥ã€‚\nå¦‚æœç”¨.getï¼Œå³æ˜¯æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œgetè¿™ä¸ªå­—æ®µä¼šè¿”å›Noneï¼Œè€Œä¸æ˜¯æŠ¥é”™ã€‚\nåœ¨çˆ¬å–è¿‡ç¨‹ä¸­ï¼Œæˆ‘é‡åˆ°äº†çˆ¬å–æˆåŠŸä½†æ˜¯å´å‘ç°æ•°æ®å¹¶æ²¡æœ‰å­˜å…¥æ•°æ®åº“ä¸­çš„æƒ…å†µï¼ŒæŸ¥é˜…èµ„æ–™åå‘ç°æ˜¯mysqlé”ä½äº†ï¼Œè¿›å…¥winç³»ç»Ÿé‡å¯MySQLæœåŠ¡å³å¯ã€‚\nè¿›è¡Œçˆ¬å–æµç¨‹ä»‹ç»çˆ¬å–çš„æ•´ä¸ªè¿‡ç¨‹ï¼šè¿›å…¥æ¯ä¸ªæ ‡ç­¾é¡µï¼Œè·å–è¯¥é¡µçš„20æœ¬ä¹¦çš„urlï¼Œå†è¿›å…¥æ¯æœ¬ä¹¦çš„urlè¿›è¡Œä¿¡æ¯çš„æå–\næå–æ ‡ç­¾\n\nè¿›å…¥æ ‡ç­¾é¡µæå–è¿™é¡µä¸­20æœ¬ä¹¦çš„url\n\néšåå°±æ˜¯è¿›å…¥ä¹¦ç±é¡µé¢çˆ¬å–æ•°æ®ã€‚\nç¼–å†™ç›¸å…³å‡½æ•°def crawl_tags(page):    &quot;&quot;&quot;è·å–æ¯ä¸ªæ ‡ç­¾ç½‘é¡µçš„ç¬¬pageé¡µ&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;å·²è·å–&#x27; + str(len(pages_url)) + &#x27;ä¸ªæ ‡ç­¾ç½‘é¡µ&#x27;)    return pages_url\n\ndef get_book_urls(tag_url):    &quot;&quot;&quot;è·å–æ¯ä¸ªæ ‡ç­¾ç½‘é¡µä¸­çš„ç¬¬ä¸€é¡µï¼Œ20æœ¬ä¹¦&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;å·²è·å–%dæœ¬ä¹¦&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;è¦åœä¸€ä¼š!ä¼‘æ¯2ç§’&#x27;)  # å› ä¸ºçˆ¬äº†ä¸€å®šæ•°æ®é‡åï¼Œä»£ç†ä¼šè·³å‡ºproxyerroré”™è¯¯ï¼Œåœä¸€ä¼šå³å¯        time.sleep(2)    return list(l)\n\nä¹‹å‰æ²¡try-exceptè¯­å¥æ€»æ˜¯çˆ¬ä¸€ä¼šå°±åœï¼Œéƒé—·æ­»æˆ‘äº†ã€‚è¿™ç§å†™æ³•æ˜¯æˆ‘é¡¿æ‚Ÿå‡ºæ¥çš„ï¼Œè¿™æ ·å†™ä¹‹åï¼ŒçœŸçš„æ˜¯é£å¿«åœ°çˆ¬ã€‚\nä¸‹é¢è¿˜ä¼šç”¨åˆ°è¿™æ ·çš„å†™æ³•\nä¸»å‡½æ•°\nif __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(1)  # è·å–æ¯ä¸ªæ ‡ç­¾é¡µçš„ç¬¬ä¸€é¡µ    # æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹crawl_tagsä¼ å…¥ä¸åŒçš„é¡µæ•°ï¼Œè·å–æ¯ä¸ªæ ‡ç­¾é¡µçš„ç¬¬né¡µ    for page in tags_url:        book_urls = get_book_urls(page)  # æŸä¸ªæ ‡ç­¾çš„ç¬¬ä¸€é¡µçš„ä¹¦é“¾æ¥        for book_url in book_urls:            try:                res = visit(book_url)  # å¯¹é‚£ä¸€é¡µçš„ä¸€æœ¬ä¹¦è¿›è¡Œè®¿é—®                book = CrawlBook(res)  # å»ºç«‹ä¸€ä¸ªä¹¦å¯¹è±¡ï¼Œdataå­˜æ”¾å…¶ä¿¡æ¯ï¼Œä»¥jsonå­˜å‚¨                save_to_mysql(book.data)  # å°†è¯¥ä¹¦ä¿¡æ¯æ’å…¥mysqlä¸­ï¼Œç»§ç»­ç¬¬äºŒæœ¬            except Exception as e:                print(e)                print(&#x27;æ­‡ä¸€ä¼šQAQï¼Œå°±2ç§’&#x27;)                time.sleep(2)    # æ¢åˆ°å¦å¤–ä¸€ä¸ªæ ‡ç­¾çš„ç¬¬1é¡µ\n\nçˆ¬å–è¿‡ç¨‹æˆªå›¾\nè¿™æ˜¯æˆ‘ä¿å­˜ä¸‹æ¥çš„æˆªå›¾ä¹‹ä¸€ï¼Œå¯è§ï¼Œå­˜å‚¨æ•°æ®æ—¶ç»å¸¸ä¼šé‡åˆ°ä¸€äº›æˆ‘ä»¬æ„å‘ä¸åˆ°çš„æŠ¥é”™ï¼Œæ‰€ä»¥å¼‚å¸¸å¤„ç†çœŸçš„å¾ˆé‡è¦å•Šï¼ï¼ï¼\nå°‘å¹´ï¼Œä¸€å®šè¦å­¦ä¼šç”¨try-exceptè¯­å¥å•Šï¼ï¼ï¼ä½ åˆšå¼€å§‹å­¦å¼‚å¸¸å¤„ç†è§‰å¾—æ²¡ä»€ä¹ˆç”¨ï¼Œç­‰ä½ é­å—è¿‡æ¯’æ‰“å°±çŸ¥é“æœ‰å¤šé‡è¦äº†ï¼ï¼ï¼\nè¿™æ˜¯æ•°æ®åº“ä¸­çš„éƒ¨åˆ†æ•°æ®ï¼ˆç”¨äº†navicatï¼‰\n\nä¸‹è½½ä¹¦ç±å›¾ç‰‡å› ä¸ºæˆ‘è€ƒæ ¸æœ‰ä¸€ä¸ªç•Œé¢è¦åšä¹¦ç±ä¿¡æ¯çš„å±•ç¤ºï¼Œè¦è´´ä¹¦çš„å°é¢å›¾ï¼Œæ‰€ä»¥è¿˜è¦ä¸‹è½½ä¸‹æ¥ã€‚\næˆ‘ä»¬æ•°æ®åº“ä¸­å­˜å‚¨çš„å­—æ®µé‡Œæœ‰å›¾ç‰‡é“¾æ¥ï¼ˆcover_urlï¼‰ï¼Œæˆ‘ä»¬æå–å‡ºæ¥ï¼Œå¯¹æ¯ä¸ªé“¾æ¥è¿›è¡Œè®¿é—®ï¼Œè¿›è¡Œå›¾ç‰‡çš„ä¸‹è½½ã€‚\nç›´æ¥è´´ä»£ç ï¼šï¼ˆvisitå‡½æ•°è·Ÿä¹‹å‰æ˜¯ä¸€æ ·çš„ï¼‰\nimport osif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot; # é€‰å–æ‰€æœ‰ä¹¦    try:        cur.execute(sql_f)        results = cur.fetchall()  # è·å¾—åŒ¹é…ç»“æœ        columnDes = cur.description  # è·å–è¿æ¥å¯¹è±¡çš„æè¿°ä¿¡æ¯        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # è·å–åˆ—å        # å¾—åˆ°çš„resultsä¸ºäºŒç»´å…ƒç»„ï¼Œé€è¡Œå–å‡ºï¼Œè½¬åŒ–ä¸ºåˆ—è¡¨ï¼Œå†è½¬åŒ–ä¸ºdf        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # å»é™¤cover_urlæœ‰ç¼ºå¤±å€¼çš„è¡Œ    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)  # åˆ›å»ºbooks_coverç›®å½•ï¼Œè´Ÿè´£å­˜æ”¾ä¹¦ç±å°é¢å›¾    os.chdir(&#x27;books_cover&#x27;)  # åˆ‡æ¢åˆ°books_coverç›®å½•    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])  # è®¿é—®å›¾ç‰‡é“¾æ¥        except Exception as e:            print(e)            print(&quot;æ­‡ä¸€ä¼šå§QAQï¼Œå°±2ç§’&quot;)            time.sleep(2)        try:            print(&quot;æ­£åœ¨ä¿å­˜ç¬¬&quot; + str(i + 1) + &quot;å¼ å›¾ç‰‡...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)  # ä»¥ä¹¦åä¸ºæ–‡ä»¶åä¸‹è½½å›¾ç‰‡        except:            print(&#x27;ä¿å­˜å¤±è´¥!&#x27;)    conn.close()  # å…³é—­pythonä¸mysqlçš„è¿æ¥\n\nç¤ºä¾‹æˆªå›¾ï¼š\n\n\næ•´ä½“ä»£ç crawl_bookæ–‡ä»¶# åº“å¯¼å…¥éƒ¨åˆ†import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # åˆå§‹åŒ–            self.html = etree.HTML(res.text)  # åˆå§‹åŒ–            self.data = dict()  # ç”Ÿæˆä¸€ä¸ªç©ºå­—å…¸            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;å‡ºç‰ˆç¤¾:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;å‡ºç‰ˆå¹´:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;é¡µæ•°:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;å®šä»·:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # æ‰¾åˆ°èŠ‚ç‚¹åä¸ºspanï¼Œå±æ€§propertyå€¼ä¸ºitemreviewdçš„èŠ‚ç‚¹ï¼Œ.stringè·å–å…¶æ–‡æœ¬å†…å®¹        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œä¸‹é¢çš„è§£ææ–¹æ³•å¤§åŒå°å¼‚    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?ä½œè€….*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;å‡ºç‰ˆç¤¾:&#x27;, &#x27;å‡ºç‰ˆå¹´:&#x27;, &#x27;é¡µæ•°:&#x27;, &#x27;å®šä»·:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # è·å–èŠ‚ç‚¹çš„srcå±æ€§å€¼        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # è¿™é‡Œç”¨çš„æ˜¯xpathçš„è§£ææ–¹æ³•ï¼Œè·å–æ‰€æœ‰å±æ€§å€¼ä¸ºcollectorä¸‹çš„æ‰€æœ‰divèŠ‚ç‚¹ä¸‹çš„ç¬¬äºŒä¸ªdivèŠ‚ç‚¹çš„aèŠ‚ç‚¹çš„hrefå±æ€§å€¼        self.data.setdefault(&#x27;readers&#x27;, readers)\n\ncrawlæ–‡ä»¶import randomimport timeimport requestsimport refrom lxml import etreefrom bs4 import BeautifulSoupfrom crawl_book import CrawlBookimport pymysqlconn = pymysql.connect(  # è¿æ¥æœ¬åœ°æ•°æ®åº“        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # è¦å¡«root        password=&quot;htht0928&quot;,  # å¡«ä¸Šè‡ªå·±çš„å¯†ç         database=&quot;doubanbook&quot;,  # æ•°æ®åº“å        charset=&quot;utf8&quot;    )cur = conn.cursor()# è¯·æ±‚å¤´headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# httpä»£ç†æ¥å…¥æœåŠ¡å™¨åœ°å€ç«¯å£proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#è´¦å·å¯†ç proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;è®¿é—®ç½‘å€ï¼Œè¿”å›å“åº”&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resdef crawl_tags(page):    &quot;&quot;&quot;è·å–æ¯ä¸ªæ ‡ç­¾ç½‘é¡µçš„pageé¡µ&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;å·²è·å–&#x27; + str(len(pages_url)) + &#x27;ä¸ªæ ‡ç­¾ç½‘é¡µ&#x27;)    return pages_urldef get_book_urls(tag_url):    &quot;&quot;&quot;è·å–æ¯ä¸ªæ ‡ç­¾ç½‘é¡µä¸­çš„ç¬¬ä¸€é¡µï¼Œ20æœ¬ä¹¦&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;å·²è·å–%dæœ¬ä¹¦&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;è¦åœä¸€ä¼š!ä¼‘æ¯2ç§’&#x27;)        time.sleep(2)    return list(l)def save_to_mysql(data):    &quot;&quot;&quot;dataæ˜¯ä¹¦ç±çš„ä¿¡æ¯ï¼Œjsonæ ¼å¼ï¼Œè¦æ’å…¥åˆ°releaseè¿™ä¸ªè¡¨&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;å‡ºç‰ˆç¤¾:&#x27;)    publishing_year = data.get(&#x27;å‡ºç‰ˆå¹´:&#x27;)    page_num = data.get(&#x27;é¡µæ•°:&#x27;)    price = data.get(&#x27;å®šä»·:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;    try:        # æ‰§è¡Œsqlè¯­å¥        cur.execute(insert_sql, insert_data)        # æäº¤æ‰§è¡Œ        conn.commit()        print(&#x27;ã€Š&#x27; + book_name + &#x27;ã€‹&#x27; + &#x27;ä¿¡æ¯å·²å­˜å‚¨æˆåŠŸ!&#x27;)    except Exception as e:        print(e)        conn.rollback()        print(&#x27;å­˜å‚¨å¤±è´¥!&#x27;)if __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(4)  # è·å–ç¬¬ä¸€é¡µ    for page in tags_url:        book_urls = get_book_urls(page)  # æŸä¸ªæ ‡ç­¾çš„ç¬¬ä¸€é¡µçš„ä¹¦é“¾æ¥        for book_url in book_urls:            try:                res = visit(book_url)  # å¯¹é‚£ä¸€é¡µçš„ä¸€æœ¬ä¹¦è¿›è¡Œè®¿é—®                book = CrawlBook(res)  # å»ºç«‹ä¸€ä¸ªä¹¦å¯¹è±¡ï¼Œdataå­˜æ”¾å…¶ä¿¡æ¯ï¼Œä»¥jsonå­˜å‚¨                save_to_mysql(book.data)  # å°†è¯¥ä¹¦ä¿¡æ¯æ’å…¥mysqlä¸­ï¼Œç»§ç»­ç¬¬äºŒæœ¬            except Exception as e:                print(e)                print(&#x27;æ­‡ä¸€ä¼šQAQï¼Œå°±2ç§’&#x27;)                time.sleep(3)    # æ¢åˆ°å¦å¤–ä¸€ä¸ªæ ‡ç­¾çš„ç¬¬1é¡µ    start = 20    for i in range(60):        new = start + i*20        url = &#x27;https://book.douban.com/tag/%E6%97%85%E8%A1%8C?start=&#x27; + str(new) + &#x27;&amp;type=T&#x27;        book_urls = get_book_urls(url)        for book_url in book_urls:            try:                res = visit(book_url)                book = CrawlBook(res)                save_to_mysql(book.data)            except Exception as e:                print(e)                print(&#x27;æ­‡ä¸€ä¼šQAQï¼Œå°±2ç§’&#x27;)                time.sleep(2)    print(&#x27;çˆ¬å–å®Œæˆ!&#x27;)    conn.close()  # å…³é—­è¿æ¥ï¼Œä¸ç„¶å¤šäº†ï¼Œæ•°æ®åº“ä¼šé”\n\ndownload_imageæ–‡ä»¶import timeimport pymysqlimport osimport requestsimport randomimport pandas as pdconn = pymysql.connect(  # è¿æ¥æœ¬åœ°æ•°æ®åº“        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # è¦å¡«root        password=&quot;htht0928&quot;,  # å¡«ä¸Šè‡ªå·±çš„å¯†ç         database=&quot;doubanbook&quot;,  # æ•°æ®åº“å        charset=&quot;utf8&quot;    )cur = conn.cursor()# è¯·æ±‚å¤´headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# httpä»£ç†æ¥å…¥æœåŠ¡å™¨åœ°å€ç«¯å£proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#è´¦å·å¯†ç proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;è®¿é—®ç½‘å€ï¼Œè¿”å›å“åº”&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot;    try:        cur.execute(sql_f)        results = cur.fetchall()        columnDes = cur.description  # è·å–è¿æ¥å¯¹è±¡çš„æè¿°ä¿¡æ¯        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # è·å–åˆ—å        # å¾—åˆ°çš„resultsä¸ºäºŒç»´å…ƒç»„ï¼Œé€è¡Œå–å‡ºï¼Œè½¬åŒ–ä¸ºåˆ—è¡¨ï¼Œå†è½¬åŒ–ä¸ºdf        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # å»é™¤cover_urlæœ‰ç¼ºå¤±å€¼çš„è¡Œ    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)    os.chdir(&#x27;books_cover&#x27;)    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])        except Exception as e:            print(e)            print(&quot;æ­‡ä¸€ä¼šå§QAQï¼Œå°±2ç§’&quot;)            time.sleep(2)        try:            print(&quot;æ­£åœ¨ä¿å­˜ç¬¬&quot; + str(i + 1) + &quot;å¼ å›¾ç‰‡...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)        except:            print(&#x27;ä¿å­˜å¤±è´¥!&#x27;)    conn.close()\n\n\n\næ€»ç»“è¿™æ˜¯æˆ‘5.14ä¸€å¤©çš„çˆ¬è™«è¿‡ç¨‹ï¼ˆä¸€å‘¨åçš„å›é¡¾ï¼‰â€¦è¿˜çœŸæ˜¯äººä¸é€¼è‡ªå·±ä¸€æŠŠï¼Œå°±ä¸çŸ¥é“è‡ªå·±çš„æ½œåŠ›æœ‰å¤šå¤§ã€‚\nè¿™æ˜¯ä»£ç†å¸®æˆ‘ç»Ÿè®¡æˆ‘ä¸€å¤©çš„è¯·æ±‚æ¬¡æ•°ï¼Œ1w8ï¼Œæˆ‘ä¹Ÿæ²¡æƒ³åˆ°hhh\n\nå…¶ä¸­é‡åˆ°äº†å¾ˆå¤šçš„å›°éš¾ï¼Œæ•°æ®æ’å…¥é—®é¢˜ï¼ŒæŠ¥é”™proxyerrorï¼Œmysqlé”ä½äº†â€¦æ‰€å¹¸éƒ½è§£å†³äº†\nè§£å†³çš„æ–¹å¼æˆ–æ˜¯æŸ¥é˜…èµ„æ–™ï¼Œæˆ–æ˜¯çµå…‰ä¹ç°â€¦\né‚£ä¸€å¤©å¤ªç´¯äº†ï¼ŒçœŸçš„å¤ªç´¯äº†ï¼Œå‡ºç°é—®é¢˜-&gt;è§£å†³é—®é¢˜-&gt;å‡ºç°é—®é¢˜-&gt;è§£å†³é—®é¢˜-&gt;â€¦\næ„Ÿè°¢æˆ‘çš„å¥½æœ‹å‹æ„¿æ„é™ªæˆ‘èŠå¤©ï¼ˆå½“æˆ‘çš„æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹ï¼‰ï¼Œåœ¨æˆ‘ä½è½çš„æ—¶å€™ç»™äºˆæˆ‘ç²¾ç¥ä¸Šçš„é¼“åŠ±\n\né¾™å“¥ï¼Œæˆ‘æ˜¯ä½ çš„ç²‰ä¸å•Šï¼ï¼ˆé£è¸¢é£æ‰‘ï¼‰\né‚£ä¹ˆï¼Œæ­¤æ¬¡çš„çˆ¬è™«å›é¡¾ç»“æŸğŸ”šå•¦ã€‚\n","categories":["python"],"tags":["çˆ¬è™«"]}]