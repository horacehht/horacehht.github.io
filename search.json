[{"title":"Clustering","url":"/2021/08/15/Trajectory%20Clustering%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","content":"论文阅读：Trajectory Clustering A Partition-and-Group Framework阅读论文的契机为暑期最终考核的主流向需求，需要阅读该篇论文进行实现。\n链接：http://hanj.cs.illinois.edu/pdf/sigmod07_jglee.pdf\n论文的组织结构如下：第1节，摘要。第2节概述了我们的轨迹聚类算法。第三节提出了第一阶段的轨迹划分算法。第四节提出了第二阶段的线段聚类算法。第5节介绍了实验评价的结果。第6节讨论了相关的工作。最后，第7节总结了论文。\n标题（Title）Trajectory Clustering: A Partition-and-Group Framework\n关键词：Partition-and-group framework, trajectory clustering, MDL principle, density-based clustering\n研究问题（Reasearch Question）轨迹聚类中相似的子轨迹，例子\n\n现有算法的缺陷（Shortcomings of existing algorithm）现有的轨迹聚类算法将类似的轨迹作为一个整体，从而发现共同的轨迹。\n但是，该文一个关键的发现是：这样的聚类方法会忽略掉一些共同的子轨迹，而这些子轨迹有着许多应用意义\n理论与方法（Theory and Method）提出了partition-and-group框架进行轨迹聚类\n将一个轨迹分成线段的集合，然后将类似的线段组成一个簇\n它能够从轨迹数据集中发现共同的子轨迹，而不是忽视了这些信息\n基于这个partition-and-group框架，再写出了一个TRA-CLUS的轨迹聚类算法\n该算法包含两个阶段：分区和分组。\n第一阶段：分区。提出了MDL（minimum description length）原则，辅助提出轨迹划分算法\n第二阶段：分组。提出基于密度的线段聚类算法，进行聚类\n算法步骤\n\n符号定义\n\n\n\n第i个轨迹点，d维\n\n\n\n​\n第i条轨迹数据\n\n\n\n第i条轨迹数据的长度\n\n\n\n轨迹数据集\n\n\n\n聚类形成的第i个类别\n\n\n​\n类别集合\n\n\n\n一条轨迹中的一条子线段\n\n\n一个轨迹可以属于多个类，因为一个轨迹可以被分为多条线段，我们在线段的角度上进行聚类\n我们假设出一条轨迹，它可以展现出线段的主要行为，那么我们称其为representative trajectory，它可以体现出共同的子轨迹\npartition-and-group首先每个轨迹被分为一组线段，然后根据我们的距离度量，彼此接近的线段被份组成一个簇，然后为每个集群生成一个有代表性的轨迹\n\n距离函数\n该函数由三个组件组成：\n（1）垂直距离\n（2）平行距离\n（3）角度距离\n现有两个d维线段 and ，这里的分别代表一个d维的点\n为不失一般性，视为长线段，为短线段\n垂直距离和分别为和在上的投影点\n令为与的欧几里德距离，为和​的欧几里得距离\n于是定义垂直距离为\n平行距离和分别为和在​上的投影点\n\n\n那么定义平行距离为\n角度距离是的长度，是和之间较小的那个夹角\n那么定义角度距离为$$d_{\\theta}(L_i,L_j) = \\left{\\right.$$\n总式\n根据应用程序决定，但是一般设定其默认值为1，一般都非常有效\nMDLminimum description length\n\n\n\n\n特征点的集合\n\n\n\n\n第i个轨迹的特征点集合的个数\n\n\n\n线段的长度\n\n\n包含两个成分：和 ​。H为hypothesis，D为data\n\n但时间复杂度太高了，所以寻求另一个近似解\nMDL近似解近似思想：将局部最优视为全局最优\n\n\n\n\n当与是线段间唯二的特征点时，计算线段的MDL代价\n\n\n\n​\n当和线段间没有特征点，该线段的MDL代价\n\n\n注：在​中​（根据定义，这是显然的，该段轨迹没有特征点，也就没有进行划分，所以划分后与划分前的距离差=0）\n（论文第5面）局部最优的情况：对于每个的k，满足最长的轨迹划分\n上面用人话来说，对于轨迹中的每一个点，如果将其作为特征点，它计算出来的代价比不将其作为特征点计算出来的代价小，那么就取其为特征点，反之则不取该点为特征点\n此外，尽可能地增加了轨迹划分的长度（我也不清楚）\n近似求解MDL的算法步骤\n\n还是很好理解的，思想就是保持这个MDL代价最小嘛。\n对轨迹中的每个点计算其划分代价和不划分代价，若该点划分代价大于不划分代价，则将其前一个点作为特征点；若划分代价小于不划分代价，那么就将长度+1。\nLemma 1（定理1）明确告诉了我们该算法时间复杂度为O(n)，n为轨迹中的点数。\n作者也提到这个近似解不一定能求到最优解，但是通过实验可知，该近似解的准确率高达80%，还是可以放心使用的\nLINE SEGMENT CLUSTERING线段密度（Density of Line Segments）是基于DBSCAN的密度聚类方法改进的，将点的概念换成线，并重新定义，在此不多赘述\n一些符号定义\n\n\n\n\n线段的集合\n\n\n\n$N_{\\varepsilon}(L_i) = {L_j \\in D\ndist(L_i,L_j) \\leq \\varepsilon }$\n\n\n\n一个人为设定的值\n\n\na core line segment\n核心线段\n\n\n\nw,r,t 是with respect to，是关于的意思\n\n核心线段：线段的​邻域中至少有MinLns个线段\n…\nClustering Algorithm与DBSCAN不同的是，并不是所有密度连通集都能成为一个簇。\n例如，考虑极端情况，一个密度连通集中的所有线段都是从一个轨迹中抽取出来的（可以想象一条线来回反复，在一个地方穿来穿去），我们为了防止这种聚类情况的出现（为什么要防止？因为我们的目标是发现不同轨迹间的共同之处，而不是一条轨迹），我们需要进行相关定义。\ntrajectory cardinality为一个聚类簇中的所涉及到的轨迹集合\n被称作聚类簇​的轨迹基数\n算法步骤：\n\nRepresentative Trajectory of a Cluster如何产生一个簇的代表性轨迹呢？\nRepresentative Trajectory\n一个代表性轨迹为一系列的点，此处的为聚类簇，范围为\n这些点是通过sweep line扫描线得以确定的。当沿簇群主轴方向扫描一条线时，我们数经过扫描线的线段条数\n何确定簇群的主轴（major axis）呢？为此，作者定义了平均方向向量。相加向量后归一化\n定义\nSuppose a set of vectors \n的平均方向向量定义如下其中，为V的基数（人话就是V有多少个向量）\n像上面说的那样，我们要计算与平均方向向量的平均坐标，为了简化运算，可以，利用一个旋转矩阵，计算完后再还原回去。\n\n生成代表性轨迹的算法步骤：\n\n计算平均方向向量，然后暂时性地变换轴\n根据旋转后的轴对起点和终点进行排序\n在按排序的顺序扫描起点和终点时，计算线段的数量，并计算这些线段的平均坐标\n\n\nHeuristic for Parameter Value Selection（参数选取的启发式方法）利用熵定义辅助找到最合适的线段密度聚类时的参数值\n$$where \\quad p(x_i) = \\frac {|N_{\\varepsilon}(x_i)|}{ \\sum {j=1} ^n |N{\\varepsilon}(x_j)|} \\quad and \\quad n=num_{ln}$$\n结论（conclusion）TRA-CLUS能够正确地从真实轨迹数据中发现共同的子轨迹\n实验（Experiment）欸嘿…暂时没看\n疑问（Question）\nMDL部分为什么要尽可能增加轨迹划分的长度\n\n一个小tips：\n基于密度的聚类方法是最适合线段的，因为它们可以发现任意形状的聚类，并且可以过滤掉噪声[11]。我们可以很容易地看到，线段簇通常是任意形状的，而轨迹数据库通常包含大量的噪声（也就是说异常值）\n","categories":["论文阅读记录"],"tags":["轨迹聚类"]},{"title":"win10 git bash 使用wget命令","url":"/2021/08/07/bash%E4%BD%BF%E7%94%A8wget%E5%91%BD%E4%BB%A4/","content":"Windows 10 git bash wget command not found因为我之前在git bash中试过敲Linux的命令，比如ls,cd,mkdir\n然后我搭服务器的时候就有用过wget来进行下载和tar来进行解压\n我就想着git bash应该也可以运行其他的Linux命令吧。\n于是敲了一下，报错wget command not found\n我就在网上查找资料，最后成功实现了git bash中运用wget命令\n步骤如下：\n1.进入该网址https://eternallybored.org/misc/wget/，根据你电脑的位数（32，64）来下载对应wget相关的exe\n\n（这里我下载了1.21.1版本的64-bit的exe）\n2.将该exe文件移至git目录下的mingw64的bin文件夹下\n我的是F:\\Git\\mingw64\\bin，这个因人而异\n3.将上面那个路径添加进环境变量\n\n之后就可以快乐地在git bash里敲wget命令啦！\n其他用不了的Linux命令也是这样解决\n","categories":["git"],"tags":["tips"]},{"title":"test","url":"/2021/05/27/test/","content":"测试终于搞好了555\n好开心…..\n","tags":["测试","试一下能不能多个标签"]},{"title":"利用hexo框架搭建个人博客","url":"/2021/05/29/%E5%88%A9%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","content":"利用hexo框架搭建个人博客安装git由于我之前安装了就不说了\n可以百度搜索🔍git官网。git是可以一直无脑下一步安装的。相信这不会难倒聪明的你\n安装nodejs进入官网选择对应的版本进行下载\n安装时一路next即可（路    径你自己选）\n安装成功后，win+R，输入cmd进入命令行，输入\nnode -vnpm -v\n\n检查是否安装成功\n如果呈现这样的就界面，则表示安装成功\n\n另外，可以用git bash代替命令行来敲命令\n安装hexo自己创建一个文件夹，我这里新建了一个blog文件夹，点进去后右键git bash here\ngit bash内输入hexo init name，这里的name你爱怎么填怎么填\n过程中可能会报错OpenSSL SSL_read: Connection was reset, errno 10054\n最有可能是网络不稳定，github经常这样，至于原因，大家懂得都懂，有时候科学上网之后也是这样。\n这里我输入了hexo init Horace\n\n\n出现这样的界面即初始化成功\n随后，cd Horace进入刚刚初始化的文件夹内，输入npm install\n\n随后输入\nhexo ghexo server\n\n或是输入hexo s\n打开hexo服务，浏览器输入localhost:4000，即可看到刚刚创建的博客了。长这样子：\n\n可以通过Ctrl+C停止hexo服务。\n停止服务后，再输入localhost:4000就登不上网站了。\n部署到云端这里可以看到，我们输入的网址是localhost:4000，并不是个静态链接\n而我们搭个人博客本身就是想别人来看的，如果别人都点不进来，那有什么意义啊？\n所以，要让我们的个人博客可以被别人访问到，我们可以将我们的网站部署到云端（不是唯一的方式）\n这里就让github托管我们的博客。\n创建github账户与创建对应仓库注册一个github账户，创建（new）一个仓库，仓库名叫用户名+.github.io\n我的用户名是horacehht，因此创建一个horacehht.github.io的仓库\n生成SSH添加到github参考廖雪峰博客git文章\n将hexo部署到云端通过npm install hexo-deployer-git --save命令下载部署的相应插件\n然后\nhexo cleanhexo generatehexo deploy\n\n但是之后访问http://horacehht.github.io报错404，说There isn&#39;t a GitHub Pages site here.\n最后通过github page的官方文档得知，如果要作为一个Github Pages仓库，需满足三个条件：\n\n仓库名为用户名+github.io\n仓库应设为public（公开）\n仓库内要创建一个README文档\n\nhexo的基本命令\n\n\n命令\n作用\n\n\n\nHexo init\n初始化博客\n\n\nHexo s\n运行博客\n\n\nHexo n title\n创建一篇新的文章，title为文章标题\n\n\nHexo c(clean)\n清理文件\n\n\nHexo g(GENERATE)\n生成静态文件\n\n\nHexo d(deploy)\n部署博客\n\n\n\n\nhexo项目的_config.yml是整个hexo项目的总配置文件，如果需要配置主题，则还有对应主题的配置文件。\n主题挑选挑选主题中…selecting\n1.Yun主题是我最喜欢的一个主题，但是因为一些特殊原因，怕审美疲劳，就不用了。demo：https://www.yunyoujun.cn/\ngithub地址：https://github.com/YunYouJun/hexo-theme-yun\n2.Anatole主题也可，很简洁，但不喜欢。demo：https://www.jixian.io/\ngithub地址：https://github.com/mrcore/hexo-theme-Anatole-Core\n3.Ayer主题。很全面的一个主题。demo：https://shen-yu.gitee.io/\ngithub地址：https://github.com/Shen-Yu/hexo-theme-ayer\n感觉非常不错。\n4.Particle主题。简洁。但文章没有目录。demo：https://korilin.com/\ngithub地址：https://github.com/korilin/hexo-theme-particle\n5.shoka主题。交互性强，字体好看，引用块好看。demo：https://shoka.lostyu.me/\ngithub地址：https://github.com/amehime/hexo-theme-shoka\n有点花\n6.Keep主题。切换自然。主页面简洁。demo：https://xpoet.cn/\ngithub地址：https://github.com/XPoet/hexo-theme-keep\n最终选定Keep这个主题！简洁又好看 \n使用Keep主题注：配置文件有两个：总配置文件，主题配置文件。下文提到时注意区分\n安装主题在git bash中执行命令\ngit clone https://github.com/XPoet/hexo-theme-keep themes/keep\n\n然后themes文件夹下出现我们想要的keep主题。\n\n这个为主题配置文件（这里踩了一些坑，通过npm安装的没有这个文件夹）\n使用主题安装完成后，在 Hexo项目的总配置文件中将 theme 设置为 keep。\ntheme: keep\n\n这个文件的路径为name\\_config.yml，我的为Horace\\_config.yml\n\nkeep会不定期更新版本，可通过如下命令更新Keep。\n\n通过 npm 安装最新版本：\n$ cd hexo-site（hexo项目的位置）$ npm update hexo-theme-keep\n\n或\n\n通过 git 更新到最新的 master 分支：\n$ cd themes/keep$ git pull\n\n配置指南复制主题配置文件。回到整个博客项目目录下的source文件夹，新建一个文件夹_data，将该文件粘贴进去\n\n将文件名更改为keep.yml，这很重要！\n然后通过该文件来进行相应的修改即可实现对主题的配置！\n这些是keep官方提供的配置资料，已经很全面了。\nhttps://keep-docs.xpoet.cn/usage-tutorial/quick-start.html#%E5%AE%89%E8%A3%85\nhttps://keep-docs.xpoet.cn/usage-tutorial/configuration-guide.html#base-info\nhttps://keep-docs.xpoet.cn/usage-tutorial/advanced.html\n这三个都是官方给的keep使用教程，分别为快速开始，配置指南和进阶使用，能满足大部分人的需求\n\n下面仅以base_info为例讲解，其他的配置项请参考官方资料\nbase_info项\n根据自己的内容进行填写\nbase_info:  title: Horaceの云端梦境  author: Horace  url: https://horacehht.github.io/  # 这里填上https://用户名.github.io\t如果自己注册了域名，就改成注册的  # 图标的链接，可以用本地的图片，也可用图片链接，或者不填  logo_img: https://horacehhtbucket.oss-cn-guangzhou.aliyuncs.com/img/网站图标.jpg  # 这里我填了一个网页链接\n\n读者可以稍微了解图床，这里我购买了阿里云的对象存储服务oss作为图床放置我的图片\n读者同样也可使用免费的图床：\n\nsm.ms\n\n路过图床\n\n\n利用阿里云作图床的文章：https://zhuanlan.zhihu.com/p/138878534\n小tips文章都放在source的_posts文件夹下\n文章如果想放在多个分类或多个标签下（前提是你开了分类和标签的功能），需要写成[a,b,c]的格式，如图：\n\n总结之后每次写新文章，就进git bash中敲\nhexo cleanhexo generatehexo deploy\n\n这样就能在自己的博客网站上看到新发布的文章了！\n如果嫌麻烦的可以参考这篇文章进行自动部署\n我的博客网站为：https://horacehht.github.io\n欢迎来访\n","categories":["hexo"]},{"title":"插件aplayer的使用","url":"/2021/06/11/%E6%8F%92%E4%BB%B6aplayer%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"github文档：https://github.com/MoePlayer/hexo-tag-aplayer/blob/master/docs/README-zh_cn.md\n敲如下命令进行aplayer插件的安装\nnpm install --save hexo-tag-aplayer\n\n此处使用的是MetingJS。MetingJS 是基于Meting API 的 APlayer 衍生播放器，引入 MetingJS 后，播放器将支持对于 QQ音乐、网易云音乐、虾米、酷狗、百度等平台的音乐播放。\n如要使用该功能，keep主题配置文件请不要启用pjax，即pjax: false，否则无法使用。\n以网易云音乐上花に亡霊这首音乐为例：\n{% meting \"1442466883\" \"netease\" \"song\" %}\n\n\n    \n\n\n\n有关 {% meting %} 的选项列表如下:\n\n\n\n选项\n默认值\n描述\n\n\n\nid\n必须值\n歌曲 id / 播放列表 id / 相册 id / 搜索关键字\n\n\nserver\n必须值\n音乐平台: netease, tencent, kugou, xiami, baidu\n\n\ntype\n必须值\nsong, playlist, album, search, artist\n\n\nfixed\nfalse\n开启固定模式\n\n\nmini\nfalse\n开启迷你模式\n\n\nloop\nall\n列表循环模式：all, one,none\n\n\norder\nlist\n列表播放模式： list, random\n\n\nvolume\n0.7\n播放器音量\n\n\nlrctype\n0\n歌词格式类型\n\n\nlistfolded\nfalse\n指定音乐播放列表是否折叠\n\n\nstoragename\nmetingjs\nLocalStorage 中存储播放器设定的键名\n\n\nautoplay\ntrue\n自动播放，移动端浏览器暂时不支持此功能\n\n\nmutex\ntrue\n该选项开启时，如果同页面有其他 aplayer 播放，该播放器会暂停\n\n\nlistmaxheight\n340px\n播放列表的最大长度\n\n\npreload\nauto\n音乐文件预载入模式，可选项： none, metadata, auto\n\n\ntheme\n#ad7a86\n播放器风格色彩设置\n\n\n试试我自己QQ音乐的歌单，因为是歌单，所以要写playlist\n{% meting \"7855838128\" \"tencent\" \"playlist\" %}\n\n    \n\n\n\n使用mmedia插件\n.bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-xCUPIURDFVTdxDmf\").style.height=document.getElementById(\"mmedia-xCUPIURDFVTdxDmf\").scrollWidth*0.76+\"px\";\n    window.onresize = function(){\n      document.getElementById(\"mmedia-xCUPIURDFVTdxDmf\").style.height=document.getElementById(\"mmedia-xCUPIURDFVTdxDmf\").scrollWidth*0.76+\"px\";\n    }; \n\n{% mmedia \"bilibili\" \"bvid:BV1B5411M7Zf\" %}\n","categories":["hexo"],"tags":["插件"]},{"title":"小老鼠走迷宫","url":"/2022/01/08/%E5%B0%8F%E8%80%81%E9%BC%A0%E8%B5%B0%E8%BF%B7%E5%AE%AB/","content":"简介一只可怜的老鼠被放到我们设置的迷宫里啦！\n有人看不下去了，一个敲代码的决定给小老鼠找到出口。\n效果如下：不会走死路的聪明小老鼠\n\n分析符号规定\ne：迷宫出口\nm：迷宫入口，也就是初始点\n.：表明某个位置已被访问\n0：过道，老鼠可以走的路\n1：墙壁，老鼠不可穿过墙壁\n\n输入及输出在这里，我们用上述符号来构建困住老鼠的迷宫，如\n10100em11\n\n根据上述符号规定，老鼠从初始点走到出口只需三步。如果用坐标的形式表达，则是(3,1)-&gt;(2,1)-&gt;(2,2)-&gt;(2,3)。另外，我们会在周围加上一圈墙（即一圈1）\n那么我们程序的输入是几行字符序列，输出是走出迷宫所经过的坐标点。同时，为了方便表达坐标，我们不妨建一个Cell类描述迷宫坐标。\n那么我们就可以写出以下代码\nclass Cell{public:    Cell(int i = 0, int j = 0){\t\tx = i;        y = j;    }    bool operator== (const Cell&amp; c) const{        return x == c.x &amp;&amp; y == c.y;    }//重载运算符==，判断两个cell是否相等private:    int x, y;}\n\n\n\n输入部分\n#include&lt;stack&gt;Stack&lt;char*&gt; mazeRows;//记录用于构建迷宫的字符串char str[80], *s;int col, row = 0;cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字Cell exitCell, entryCell;while(cin &gt;&gt; str){    //读取一行，cin读到空格就判断是下一个字符串了    row++;// row用于记录用户输入的行数    cols = strlen(str);// 列数\ts = new char[cols+3];//因为需要加上两边的墙（2），再加上\\0作为字符串结尾，所以是+3    mazeRows.push(s);    strcpy(s+1,str);    s[0] = s[cols+1] = wall;    s[cols+2] = '\\0';    if (strchr(s,exitMarker) != nullptr) {//如果该行中有出口，则记录出口的坐标信息       exitCell.x = row;       exitCell.y = strchr(s,exitMarker) - s;    }    if (strchr(s,entryMarker) != nullptr) {       entryCell.x = row;       entryCell.y = strchr(s,entryMarker) - s;    }}\n\n\n\n走出迷宫一个老鼠在迷宫中，它可以向4个方向运动：上、下、左、右。\n我们不可能让它每次都随机选一个方向走，那么我们不妨规定：老鼠总是倾向于走上面，如果没得走，那就走下面，其次左，其次右。显然，这样的规定可能会走向错误的道路，比如说走到死胡同，则需要老鼠整理以前的信息（可以建立一个二维字符数组来记录哪些点老鼠走过），退回分岔路口，重新寻找一条通往出口的路。\n在这里，用栈这样的数据结构是很合适的，规定操作顺序，也可以实现回退。那么我们使用一个栈来记录老鼠所走过的路信息。当走到死路后，对栈进行回退，并基于之前走过路的标记信息，选择另外一条路进行尝试，重复多次，最终走出迷宫\n因为，整个程序背景是迷宫，为了练习C++语法，不妨建立一个迷宫类\nclass Maze {public:    Maze();    void exitMaze();//走出迷宫private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;//存放可以走的点    Stack&lt;Cell&gt; pStack;//存放走过的点    Stack&lt;Cell&gt; nStack;//方便输出的一个栈，p是positive，n是negative    char **store;//记录迷宫每点是否访问过    bool PointVisited(int , int);//该点被访问过，返回true    void pushUnvisited(int, int);//自定义的入栈操作，如果该点可以走就入栈，不能就不入栈    bool AroundVisited(int, int);//如果该点附近都被访问过，返回true1    int rows, cols;        //输出重载    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};\n\n\n\n我们着重于走到死胡同的情况，它如何正确回退到该回退的地方呢？其实我们换个角度考虑，走到了死胡同，为什么需要回退？是因为没有路走了，如果还有路可以走的话，它还会继续往下尝试，因为不知道接下来的是不是出口，撞到了南墙才知道“噢，没路了”。所以，我们要回退到一个附近存在未访问点的点，那么我们可以以此为判断条件，如果当前点不满足则继续回退。其他内容则不详细讲述思路。\n整体代码用类对上述代码稍加修改，变得更加整体性\nmaze.h#ifndef MAZE_MAZE_H#define MAZE_MAZE_H#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;cstring&gt;using namespace std;template&lt;class T&gt;class Stack : public stack&lt;T&gt; {public:    T pop() {        T tmp = stack&lt;T&gt;::top();        stack&lt;T&gt;::pop();        return tmp;    }//自定义pop，删除栈顶元素还获取栈顶元素的值};class Cell {//一格public:    Cell(int i = 0, int j = 0) {        x = i; y = j;    }    bool operator== (const Cell&amp; c) const {        return x == c.x &amp;&amp; y == c.y;    }private:    int x, y;    friend class Maze;//声明友元类Maze，Maze可以访问到Cell的私有数据    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell);};class Maze {public:    Maze();    void exitMaze();private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;    Stack&lt;Cell&gt; pStack;    Stack&lt;Cell&gt; nStack;    char **store;         // array of strings;    bool PointVisited(int , int);    void pushUnvisited(int, int);    bool AroundVisited(int, int);    int rows, cols;    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};#endif //MAZE_MAZE_H1\n\n\n\nmaze.cpp#include \"maze.h\"//关于cell的输出重载ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell) {    out &lt;&lt; \"(\" &lt;&lt; cell.x &lt;&lt; \",\" &lt;&lt; cell.y &lt;&lt; \")\";    return out;}Maze::Maze() : exitMarker('e'), entryMarker('m'), visited('.'),               passage('0'), wall('1') {//冒号后面是初始化，用括号内的值初始化前面的变量    Stack&lt;char*&gt; mazeRows;    char str[80], *s;    int col, row = 0;    cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     \t &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t \t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t \t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t \t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字    while (cin &gt;&gt; str) {        row++;        cols = strlen(str);        s = new char[cols+3];        mazeRows.push(s);        strcpy(s+1,str);        s[0] = s[cols+1] = wall;        s[cols+2] = '\\0';        //strchr(a,b)，在a中寻找b，如果找不到则返回nullptr        if (strchr(s,exitMarker) != nullptr) {//如果该行有出口，则记录其坐标信息            exitCell.x = row;            exitCell.y = strchr(s,exitMarker) - s;        }        if (strchr(s,entryMarker) != nullptr) {            entryCell.x = row;            entryCell.y = strchr(s,entryMarker) - s;        }    }    rows = row;    store = new char*[rows+2];//store用于记录迷宫点信息         store[0] = new char[cols+3];          for ( ; !mazeRows.empty(); row--) {        store[row] = mazeRows.pop();    }    store[rows+1] = new char[cols+3];     store[0][cols+2] = store[rows+1][cols+2] = '\\0';    for (col = 0; col &lt;= cols+1; col++) {        store[0][col] = wall;               store[rows+1][col] = wall;    }}bool Maze::PointVisited(int row, int col) {    if (store[row][col] == passage || store[row][col] == exitMarker) {        return false;//还没有被经过    }    else        return true;}bool Maze::AroundVisited(int row, int col) {    int cnt = 4;//可以走的数量    if (PointVisited(row-1, col))        cnt--;    if(PointVisited(row+1,col))        cnt--;    if(PointVisited(row,col-1))        cnt--;    if(PointVisited(row,col+1))        cnt--;    if(cnt == 0)        return true;//周围都被走过了    else        return false;}void Maze::pushUnvisited(int row, int col) {    if (!PointVisited(row, col)) {        mazeStack.push(Cell(row, col));    }}void Maze::exitMaze() {    int row, col;    currentCell = entryCell;    while (!(currentCell == exitCell)) {        pStack.push(currentCell);        row = currentCell.x;        col = currentCell.y;        if (!(currentCell == entryCell))            store[row][col] = visited;        bool up, down, left, right;        pushUnvisited(row-1,col);//按上下左右的顺序压栈        pushUnvisited(row+1,col);        pushUnvisited(row,col-1);        pushUnvisited(row,col+1);        if(AroundVisited(row, col)){            pStack.pop();//回退多少步该怎么确定呢？            Cell tmp = pStack.top();//查询这个点附近是否可以走，如果不行则继续回退            while (AroundVisited(tmp.x, tmp.y)){                pStack.pop();                if (!pStack.empty())//这个判断是防止完全就走不到出口的迷宫使程序崩溃                    tmp = pStack.top();                else{                    cout &lt;&lt; \"Failure\\n\";                    return;                }            }        }        if (mazeStack.empty()) {            cout &lt;&lt; \"Failure\\n\";            return;        }        else            currentCell = mazeStack.pop();    }    while (!pStack.empty())    {        nStack.push(pStack.pop());    }    while (!nStack.empty()){        Cell cur_cell = nStack.pop();        cout &lt;&lt; cur_cell;    }    cout &lt;&lt; exitCell;    cout &lt;&lt; \"\\nSuccess\\n\";}\n\nmain.cpp#include \"maze.h\"int main() {    Maze().exitMaze();    system(\"pause\");    return 0;}\n\n","categories":["数据结构"],"tags":["栈","回溯"]},{"title":"聚类算法","url":"/2021/08/07/%E8%81%9A%E7%B1%BB%E5%AD%A6%E4%B9%A0/","content":"聚类参考资料：《机器学习》-周志华\n在无监督学习中，训练样本的标记信息是未知的，此类学习任务中研究最多、应用最广的是“聚类”\n聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”\n形式化地说，样本集包含m个无标记样本，每个样本​是一个n维特征向量\n聚类算法会将样本集D划分为k个不相交的簇，其中簇互斥，且所有簇的并集为数据集D\nK-Means算法步骤\n旁边还有一篇字：为避免运行时间过长，通常设置一个最大运行轮数或最小调整幅度阈值，若达到最大论述或调整幅度小于阈值，则停止进行\n我下面的代码实现为限制运行次数，sklearn也是限制迭代次数\n代码实现根据上述流程图，熟悉掌握numpy和pandas的函数使用，最后用seaborn进行聚类效果展示\n在jupyter notebook中分步运行\nimport numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom pandas import Seriesdf = pd.read_csv(\"points.txt\", sep=',', header=None)  # 我的点集数据为points.txt，分隔符为,df.columns=['x', 'y']sns.scatterplot('x','y',data=df)  # 绘制原图查看点的分布\n\n\n下面为我编写的一个的k_means\nclass k_means:    def __init__(self, X, k: int, max_iter=1000):        \"\"\"        X: 数据集，array形式        k: 聚类簇数        \"\"\"        self.X = np.array(X)        self.k = k        self.max_iter = max_iter        ori_index = np.random.choice(a=list(range(len(X))), size=self.k, replace=False, p=None)        self.points = X[ori_index]  # 存放均值向量    def cal_dist(self, x1, x2):        \"\"\"算两点间的欧氏距离\"\"\"        return np.sqrt(np.sum((x1 - x2) ** 2))    def classify(self):        \"\"\"        X: 点集，每一个样本是n维向量        \"\"\"        X = self.X        point_marks = dict()        for j in range(len(X)):            dist = []            for i in range(self.k):                # 算出样本x_j与各均值向量的距离                dist.append(self.cal_dist(X[j], self.points[i]))            mark = dist.index(min(dist))  # 选取距离最近的那个类作为标记            point_marks.setdefault(j, mark)  # 建立起联系，第j个点是第mark类        mark_record = Series(point_marks.values())  # 记录着每个点的类标记        # 计算新均值向量并更新        for i in range(self.k):            tem_points = X[mark_record[mark_record == i].index]  # 获取第i类样本点的数据            point_num, fea_num = tem_points.shape            for p in range(fea_num):                self.points[i][p] = tem_points[:, p].sum() / point_num        return mark_record    def fit(self):        \"\"\"不断进行迭代，最终获得一个记录划分的series对象\"\"\"        _series = None        for i in range(self.max_iter):            _dict = self.classify()        return _series\n\ndata = np.array(df)model = k_means(data, k=5, max_iter=500)record = model.fit()record.name = \"class\"new_df = pd.concat([df, record], axis=1)sns.scatterplot('x','y',data=new_df, hue=\"class\", palette=\"Set2\")\n\n\n\nk=5时的聚类效果图\n\n同时，我也使用了sklearn的KMeans聚类方法进行效果上的对比\nfrom sklearn.cluster import KMeansskmodel = KMeans(n_clusters=5, random_state=0,).fit(data)sk_label = Series(skmodel.labels_)sk_label.name = \"class\"sk_df = pd.concat([df, sk_label], axis=1)sns.scatterplot('x','y',data=sk_df, hue=\"class\", palette=\"Set2\")\n\n\n效果还没我的好\n高斯混合聚类首先要明白什么是（多元）高斯分布。\n对n维样本空间X中的向量，若服从高斯分布，其概率密度函数为其中，为n维均值向量，为的协方差矩阵。\n由上式可看出高斯分布完全由均值向量和协方差矩阵​这两个参数确定，为了明确其与相应参数的依赖关系，我们将概率密度函数记为\n我们可定义高斯混合分布该分布共由k个混合成分组成，每个混合成分对应一个高斯分布。其中为第i个高斯混合成份的参数，而为相应的混合系数\n算法步骤\n密度聚类顾名思义，基于密度的聚类，从样本密度的角度来考察样本之间的可连接性，并给予可连接样本不断扩展聚类簇以获得最终的聚类结果。\n著名算法有DBSCAN，它基于一组“领域”参数来刻画样本分布的紧密程度\n给定数据集，定义下面这几个概念：\n\n​邻域：对，其邻域包含样本集D中与的距离不大于的样本，即\n\n核心对象：若的邻域至少包含个样本，即，则为一个核心对象\n\n密度直达：若位于的邻域中，且为核心对象，则称由密度直达\n\n密度可达：对​和 ​，若存在样本序列，其中且由密度直达，则称由密度可达​\n\n密度相连：对​和 ​，若存在​使得​与​均有​密度可达，则称由与​密度项链\n\n\n\n基于这些定义，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。\n给定邻域参数，簇是满足以下性质的非空样本子集：\n连接性：与密度相连\n最大性：由密度可达\n算法步骤\n代码实现我自己写的那个总是会在步骤16出问题，我查了下网上的资料，也只是觉得跟他采用的数据结构不太一样，我用的字典，他用的列表，就是不知道为啥我！的！不！行！可！恶！\n\n参考资料：\n最后整体的代码是这样的\nimport randomimport copyimport numpy as npclass DBSCAN:    def __init__(self, eps=0.5, min_samples=5):        self.eps = eps        self.min_samples = min_samples    def fit(self, X):        m, n = X.shape        k = 0  # 聚类簇数        neighbor_list = []        gama = set([x for x in range(len(X))])  # 初始时将所有点标记为未访问        labels = [-1 for _ in range(len(X))]  # 初始化每个点的类标签为-1        def find_neighbor(j):            \"\"\"找到j的邻居，返回集合\"\"\"            eps_domain = list()            for i in range(m):                dist = self.cal_dist(X[j], X[i])                if dist &lt;= self.eps:                    eps_domain.append(i)            return set(eps_domain)        def find_object():            \"\"\"找到核心对象集合\"\"\"            omega_list = []            for i in range(m):                neighbor = find_neighbor(i)                neighbor_list.append(neighbor)                if len(neighbor) &gt;= self.min_samples:                    omega_list.append(i)            return set(omega_list)        core_obj = find_object()        while len(core_obj) &gt; 0:            gama_old = copy.deepcopy(gama)            j = random.choice(list(core_obj))  # 随机选取一个核心对象            k = k + 1            Q = list()            Q.append(j)            gama.remove(j)            while len(Q) &gt; 0:                q = Q[0]                Q.remove(q)                if len(neighbor_list[q]) &gt;= self.min_samples:                    delta = neighbor_list[q] &amp; gama                    deltalist = list(delta)                    for i in range(len(delta)):                        Q.append(deltalist[i])                        gama = gama - delta            Ck = gama_old - gama            Cklist = list(Ck)            for i in range(len(Ck)):                labels[Cklist[i]] = k            core_obj = core_obj - Ck        return labels    def cal_dist(self, x1, x2):        \"\"\"算两点间的欧氏距离\"\"\"        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n\n\n进行绘图\nmodel = DBSCAN(eps=2, min_samples=15)df = pd.read_csv(\"points.txt\", sep=',', header=None)df.columns = ['x', 'y']data = np.array(df)labels = model.fit(data)record = Series(labels)record.name = \"class\"new_df = pd.concat([df, record], axis=1)sns.scatterplot('x', 'y', data=new_df, hue=\"class\", palette=\"Set2\")\n\n效果如下图：\n\n我觉得这个是我用的数据集中分类效果最好的一个算法，不过值得注意的是分类的效果非常依赖于epsilon和min_samples两个参数值，我调了七次才调出这种效果。\n相对来说，聚类的算法比之前学习的算法好实现很多，又复习了许多python的语法知识，不戳！结束聚类部分的学习\n","categories":["机器学习算法"],"tags":["聚类"]},{"title":"小老鼠走迷宫","url":"/2022/01/08/%E8%BF%B7%E5%AE%AB/","content":"简介一只可怜的老鼠被放到我们设置的迷宫里啦！\n有人看不下去了，一个敲代码的决定给小老鼠找到出口。\n效果如下：不会走死路的聪明小老鼠\n\n分析符号规定\ne：迷宫出口\nm：迷宫入口，也就是初始点\n.：表明某个位置已被访问\n0：过道，老鼠可以走的路\n1：墙壁，老鼠不可穿过墙壁\n\n输入及输出在这里，我们用上述符号来构建困住老鼠的迷宫，如\n10100em11\n\n根据上述符号规定，老鼠从初始点走到出口只需三步。如果用坐标的形式表达，则是(3,1)-&gt;(2,1)-&gt;(2,2)-&gt;(2,3)。另外，我们会在周围加上一圈墙（即一圈1）\n那么我们程序的输入是几行字符序列，输出是走出迷宫所经过的坐标点。同时，为了方便表达坐标，我们不妨建一个Cell类描述迷宫坐标。\n那么我们就可以写出以下代码\nclass Cell{public:    Cell(int i = 0, int j = 0){\t\tx = i;        y = j;    }    bool operator== (const Cell&amp; c) const{        return x == c.x &amp;&amp; y == c.y;    }//重载运算符==，判断两个cell是否相等private:    int x, y;}\n\n\n\n输入部分\n#include&lt;stack&gt;Stack&lt;char*&gt; mazeRows;//记录用于构建迷宫的字符串char str[80], *s;int col, row = 0;cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字Cell exitCell, entryCell;while(cin &gt;&gt; str){    //读取一行，cin读到空格就判断是下一个字符串了    row++;// row用于记录用户输入的行数    cols = strlen(str);// 列数\ts = new char[cols+3];//因为需要加上两边的墙（2），再加上\\0作为字符串结尾，所以是+3    mazeRows.push(s);    strcpy(s+1,str);    s[0] = s[cols+1] = wall;    s[cols+2] = '\\0';    if (strchr(s,exitMarker) != nullptr) {//如果该行中有出口，则记录出口的坐标信息       exitCell.x = row;       exitCell.y = strchr(s,exitMarker) - s;    }    if (strchr(s,entryMarker) != nullptr) {       entryCell.x = row;       entryCell.y = strchr(s,entryMarker) - s;    }}\n\n\n\n走出迷宫一个老鼠在迷宫中，它可以向4个方向运动：上、下、左、右。\n我们不可能让它每次都随机选一个方向走，那么我们不妨规定：老鼠总是倾向于走上面，如果没得走，那就走下面，其次左，其次右。显然，这样的规定可能会走向错误的道路，比如说走到死胡同，则需要老鼠整理以前的信息（可以建立一个二维字符数组来记录哪些点老鼠走过），退回分岔路口，重新寻找一条通往出口的路。\n在这里，用栈这样的数据结构是很合适的，规定操作顺序，也可以实现回退。那么我们使用一个栈来记录老鼠所走过的路信息。当走到死路后，对栈进行回退，并基于之前走过路的标记信息，选择另外一条路进行尝试，重复多次，最终走出迷宫\n因为，整个程序背景是迷宫，为了练习C++语法，不妨建立一个迷宫类\nclass Maze {public:    Maze();    void exitMaze();//走出迷宫private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;//存放可以走的点    Stack&lt;Cell&gt; pStack;//存放走过的点    Stack&lt;Cell&gt; nStack;//方便输出的一个栈，p是positive，n是negative    char **store;//记录迷宫每点是否访问过    bool PointVisited(int , int);//该点被访问过，返回true    void pushUnvisited(int, int);//自定义的入栈操作，如果该点可以走就入栈，不能就不入栈    bool AroundVisited(int, int);//如果该点附近都被访问过，返回true1    int rows, cols;        //输出重载    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};\n\n\n\n我们着重于走到死胡同的情况，它如何正确回退到该回退的地方呢？其实我们换个角度考虑，走到了死胡同，为什么需要回退？是因为没有路走了，如果还有路可以走的话，它还会继续往下尝试，因为不知道接下来的是不是出口，撞到了南墙才知道“噢，没路了”。所以，我们要回退到一个附近存在未访问点的点，那么我们可以以此为判断条件，如果当前点不满足则继续回退。其他内容则不详细讲述思路。\n整体代码用类对上述代码稍加修改，变得更加整体性\nmaze.h#ifndef MAZE_MAZE_H#define MAZE_MAZE_H#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;cstring&gt;using namespace std;template&lt;class T&gt;class Stack : public stack&lt;T&gt; {public:    T pop() {        T tmp = stack&lt;T&gt;::top();        stack&lt;T&gt;::pop();        return tmp;    }//自定义pop，删除栈顶元素还获取栈顶元素的值};class Cell {//一格public:    Cell(int i = 0, int j = 0) {        x = i; y = j;    }    bool operator== (const Cell&amp; c) const {        return x == c.x &amp;&amp; y == c.y;    }private:    int x, y;    friend class Maze;//声明友元类Maze，Maze可以访问到Cell的私有数据    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell);};class Maze {public:    Maze();    void exitMaze();private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;    Stack&lt;Cell&gt; pStack;    Stack&lt;Cell&gt; nStack;    char **store;         // array of strings;    bool PointVisited(int , int);    void pushUnvisited(int, int);    bool AroundVisited(int, int);    int rows, cols;    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};#endif //MAZE_MAZE_H1\n\n\n\nmaze.cpp#include \"maze.h\"//关于cell的输出重载ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell) {    out &lt;&lt; \"(\" &lt;&lt; cell.x &lt;&lt; \",\" &lt;&lt; cell.y &lt;&lt; \")\";    return out;}Maze::Maze() : exitMarker('e'), entryMarker('m'), visited('.'),               passage('0'), wall('1') {//冒号后面是初始化，用括号内的值初始化前面的变量    Stack&lt;char*&gt; mazeRows;    char str[80], *s;    int col, row = 0;    cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     \t &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t \t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t \t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t \t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字    while (cin &gt;&gt; str) {        row++;        cols = strlen(str);        s = new char[cols+3];        mazeRows.push(s);        strcpy(s+1,str);        s[0] = s[cols+1] = wall;        s[cols+2] = '\\0';        //strchr(a,b)，在a中寻找b，如果找不到则返回nullptr        if (strchr(s,exitMarker) != nullptr) {//如果该行有出口，则记录其坐标信息            exitCell.x = row;            exitCell.y = strchr(s,exitMarker) - s;        }        if (strchr(s,entryMarker) != nullptr) {            entryCell.x = row;            entryCell.y = strchr(s,entryMarker) - s;        }    }    rows = row;    store = new char*[rows+2];//store用于记录迷宫点信息         store[0] = new char[cols+3];          for ( ; !mazeRows.empty(); row--) {        store[row] = mazeRows.pop();    }    store[rows+1] = new char[cols+3];     store[0][cols+2] = store[rows+1][cols+2] = '\\0';    for (col = 0; col &lt;= cols+1; col++) {        store[0][col] = wall;               store[rows+1][col] = wall;    }}bool Maze::PointVisited(int row, int col) {    if (store[row][col] == passage || store[row][col] == exitMarker) {        return false;//还没有被经过    }    else        return true;}bool Maze::AroundVisited(int row, int col) {    int cnt = 4;//可以走的数量    if (PointVisited(row-1, col))        cnt--;    if(PointVisited(row+1,col))        cnt--;    if(PointVisited(row,col-1))        cnt--;    if(PointVisited(row,col+1))        cnt--;    if(cnt == 0)        return true;//周围都被走过了    else        return false;}void Maze::pushUnvisited(int row, int col) {    if (!PointVisited(row, col)) {        mazeStack.push(Cell(row, col));    }}void Maze::exitMaze() {    int row, col;    currentCell = entryCell;    while (!(currentCell == exitCell)) {        pStack.push(currentCell);        row = currentCell.x;        col = currentCell.y;        if (!(currentCell == entryCell))            store[row][col] = visited;        bool up, down, left, right;        pushUnvisited(row-1,col);//按上下左右的顺序压栈        pushUnvisited(row+1,col);        pushUnvisited(row,col-1);        pushUnvisited(row,col+1);        if(AroundVisited(row, col)){            pStack.pop();//回退多少步该怎么确定呢？            Cell tmp = pStack.top();//查询这个点附近是否可以走，如果不行则继续回退            while (AroundVisited(tmp.x, tmp.y)){                pStack.pop();                if (!pStack.empty())//这个判断是防止完全就走不到出口的迷宫使程序崩溃                    tmp = pStack.top();                else{                    cout &lt;&lt; \"Failure\\n\";                    return;                }            }        }        if (mazeStack.empty()) {            cout &lt;&lt; \"Failure\\n\";            return;        }        else            currentCell = mazeStack.pop();    }    while (!pStack.empty())    {        nStack.push(pStack.pop());    }    while (!nStack.empty()){        Cell cur_cell = nStack.pop();        cout &lt;&lt; cur_cell;    }    cout &lt;&lt; exitCell;    cout &lt;&lt; \"\\nSuccess\\n\";}\n\n","categories":["数据结构"],"tags":["栈","回溯"]},{"title":"降维算法","url":"/2021/08/04/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/","content":"降维算法参考资料：【机器学习】【白板推导系列】\n引入背景降维，Dimensionality Reduction\n在机器学习过程中，我们更关注的是泛化误差，而不是训练误差。\n而过拟合问题解决的方法：\n\n数据量提升\n正则哈\n降维\n\n为什么降维会解决过拟合问题呢？又或者换一种问法：为什么维度高会造成一个过拟合呢？\n我们可以这样想：每增加一个维度，那我需要用来cover整个样本空间的量是以指数级别增长的（假设每个特征都是二值的，现有n个特征，再增加一个特征，那样本空间就增加了​个点，那就更别说多值的特征了）\n几何角度引入二维下，设正方形边长为1，那么他的面积则为1，其中包含一个内切圆，那么他的面积为\n三维下，设正方体边长为1，那么他的体积为1，其中包含一个内切球，那么他的体积为\n为了书写方便，我们令常数，则三维下的体积为\n那么，我们推广到D维空间，超立方体内含一个超球体，该超球体体积则为​，易得也就是说当维度不断增加这个超球体近乎一个空点（有点反直觉），那么很容易想到，数据会较容易分布在该球外，也就是分布在边缘，也就带来了数据稀疏性问题\n\n分类\n直接降维，即特征选择，选择出自己觉得重要的特征\n线性降维：PCA，MDS等\n非线性降维：流形学习，包括 Isomap，LLE 等\n\n预备知识假设数据集X为N×P形状，即N个样本，P个特征，那么一个样本则为p维向量\n\n多维下，样本均值​​（式）显然，样本均值为的p维向量\n协方差矩阵S（式）不难看出，协方差矩阵S为的矩阵\n根据之前的经验，我们可以通过线代的知识将连加号去掉\n\n由于只是差了一个转置，所以下面我只讨论左半边的式子，继续进行化简（式）其中，为形状为的全为1的列向量，即由式1可继续对式3进行化简\n式）\n不难得出形状为，我们将其记作，称作中心矩阵（centering matrix）\n于是，我们可以将式2化简成该中心矩阵H有一个良好的性质，​（读者可以自己试着推导一下），进而推导出\n因此，我们可以进一步化简式子，将式2化成（式）\nPCA主成分分析\n经典的主成分分析，顺口溜总结为一个中心，两个基本点\n一个中心将一组可能线性相关的变量通过线性变换变换成一组线性无关的变量\n换句话说就是对原始特征空间的重构，让特征正交变换一组线性无关的基\n两个基本点\n最大投影方差 \n最小重构距离\n\n两个是同一个东西\n图源自李航老师的《统计学习方法》，红框部分是PCA的选择新基的主要思想\n\n因此，我们就需要知道该投影方差怎么表示出来并最大化\n最大投影方差首先，我们拿到数据后，第一步要做数据中心化，数学表达为\n投影的表示在高中数学中，向量的数量积那么在上的投影​为\n如果我们**令被投影向量的模**，那么则恰好是在上的投影\n多维中在​上的投影则表示为\n投影方差我们记新基为，那么一个样本点的在上的投影为那么所有样本点的投影方差和在的约束条件下记为（二范式=1跟向量内积=1是等价的，所以约束条件换了一下）\n\n不难发现中间那项和式2很像，只差了一个\n所以我们可以近似地把写成那么现在的问题就转化为了\n\n于是采用拉格朗日乘数法，将约束问题化为无约束问题那么对其求偏导，令其=0得S为矩阵，为实数值，我们不难看出，​为协方差矩阵S的特征值，为对应的特征向量\n最小重构代价不想打公式了，直接在纸上写了，直接看图把\n\n那么现在的问题为\n\n实际上我们可以对每个基单独进行拉格朗日乘数法\n\n算法实现步骤参考资料：https://zhuanlan.zhihu.com/p/77151308\n设有 m 条 n 维数据。\n\n将原始数据按列组成 n 行 m 列矩阵 X；\n将 X 的每一行进行零均值化，即减去这一行的均值；\n求出协方差矩阵  ；\n求出协方差矩阵的特征值及对应的特征向量；\n将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；\n 即为降维到 k 维后的数据。\n\n稍微有点卡的步骤是5，其他倒也没有什么\n代码实现import numpy as npclass PCA:    def __init__(self, X):        \"\"\"输入一个numpy或pandas的特征矩阵\"\"\"        self.X = np.array(X).T        self.n, self.m = self.X.shape        self.equalization()        self.get_P()    def equalization(self):        \"\"\"数据均值化\"\"\"        for i in range(self.n):            self.X[i] = self.X[i] - self.X[i].mean()    def get_P(self):        \"\"\"求出矩阵P\"\"\"        C = (1/self.m)*np.dot(self.X, self.X.T)        w, v = np.linalg.eig(C)        # w是特征值，v是特征值对应的特征向量排列而成的矩阵        fea_rank_index = np.argsort(w)  # 返回的是一个指明特征值从小到大排序的索引        column = len(w)        pre_P = v[fea_rank_index[0]].reshape(-1, column)        for i in fea_rank_index[1:]:            # 根据特征值大小把对应的特征向量拼接上去            pre_P = np.concatenate([pre_P, v[fea_rank_index[i]].reshape(-1, column)])        self.pre_P = pre_P    def fit(self, k: int):        # 返回降到k维后的数据        Y = np.dot(self.pre_P[:k], self.X)        return Y.T\n\n算法评估去网上查了一下，降维算法没有一个固定的量化指标来评估，只能通过最终效果来判断（比如分类任务中，降维后的准确率，查全率是否提升）\n于是我采用乳腺癌数据集分类任务来做一下降维的评估\nfrom sklearn.datasets import load_breast_cancer  # 乳腺癌数据集from sklearn.model_selection import train_test_split  # 留出法from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix # 混淆矩阵import PCAdata = load_breast_cancer()X = data['data']Y = data['target'].reshape(-1, 1)  # 防止一个奇怪的Bugmethod = PCA.PCA(X)X_pca = method.fit(25) # 降成25维x_train, x_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2)  # 划分数据集skmodel = LogisticRegression(max_iter=2000)skmodel.fit(x_train, y_train)y_predict = skmodel.predict(x_test)test_score = skmodel.score(x_test, y_test)tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()print(\"——降维后——\")print(\"准确率：\" + str(test_score))print(\"查准率P：\" + str(tp/(tp+fp)))print(\"查全率R：\" + str(tp/(tp+fn)))x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  # 划分数据集skmodel.fit(x_train, y_train)y_predict = skmodel.predict(x_test)test_score = skmodel.score(x_test, y_test)tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()print(\"——降维前——\")print(\"准确率：\" + str(test_score))print(\"查准率P：\" + str(tp/(tp+fp)))print(\"查全率R：\" + str(tp/(tp+fn)))\n\n这是为数不多降到25维后，降维后效果比降维前效果好的一次\n\n我还打算画一个图，画出对于乳腺癌数据集降到多少维效果（可以是准确率，查准率，查全率）会最好？\n\n准确率\n\n\n\n查准率P\n\n\n\n查全率\n\n\n三个图分开画似乎太麻烦了，试着一起画？\n其中，红色为准确率，绿色为查准率，蓝色为查全率\n\nfrom sklearn.datasets import load_breast_cancer  # 乳腺癌数据集from sklearn.model_selection import train_test_split  # 留出法from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix # 混淆矩阵import PCAimport matplotlib.pyplot as pltfrom pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong']  # 指定默认字体mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题data = load_breast_cancer()X = data['data']Y = data['target'].reshape(-1, 1)  # 防止一个奇怪的Bugmethod = PCA.PCA(X)l = list(range(10, 30))accuracy = []P = []R = []for i in l:    X_pca = method.fit(i) # 降成i维    x_train, x_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2)  # 划分数据集    skmodel = LogisticRegression(max_iter=5000)    skmodel.fit(x_train, y_train)    y_predict = skmodel.predict(x_test)    test_score = skmodel.score(x_test, y_test)    tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()    accuracy.append(test_score)    P.append(tp/(tp+fp))    R.append(tp/(tp+fn))plt.plot(l, accuracy, color=\"r\", marker=\"o\")plt.plot(l, P, color=\"g\", marker=\"+\")plt.plot(l, R, color=\"b\", marker=\"*\")plt.xlabel('维数')plt.ylabel('评价效果')plt.show()\n\n性质\n缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；\n降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；\n过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；\n特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；\n\n细节零均值化当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来，不能使用验证集或者测试集的中心向量。\n其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。\n另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。\n其他降维方法MDS和核化线性降维，流形学习…\n由于PCA是最常用的降维算法，在此就不介绍其它了\n","categories":["机器学习算法"],"tags":["降维"]},{"title":"设置代理爬取豆瓣书籍","url":"/2021/05/29/%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E4%B9%A6%E7%B1%8D/","content":"设置代理爬取豆瓣书籍简介爬取网页大都可分为三个步骤：\n\n访问网页：一般使用requests库，不嫌麻烦的话可以使用python内置的urllib库\n解析网页：使用Xpath，BeautifulSoup，正则表达式等方式进行网页信息的提取\n存储数据：①存入IO流文件，如txt，csv等文件    ②存入数据库，主流的有MySQL，mongodb\n\n本文开发环境为python3.8，爬取的数据存入MySQL数据库中\n访问网页import requestsurl = &#x27;目标网址&#x27;res = requests.get(url)\n\n这样即可完成一次网页的访问。但一般都要加上请求头，称为headers。一般的网站请求的headers中加入User-Agent项参数即可。\n如果你用的是谷歌浏览器，可在网页栏输入chrome::version查看自己的User-Agent项，叫“用户代理”\n\n如果不是，也可以按F12，Ctrl+R，随便点进一个请求，Requests Headers项中的User-Agent就是你的User-Agent。\n于是代码应该改成这样\nheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;填入你的user-agent&#x27;    ...根据不同的网站加入不同的参数&#125;res = requests.get(url, headers=headers)\n\n不加的话，你的请求中User-Agent的值则是python爬虫，网站就会拒绝访问，这样你就无法得到网站返回的数据。\n设置代理为什么要设置代理？背景：本人要在某次考核任务最后一天中爬取数据量超过3k5的数据，时间紧，数据量说小不小，说大不大。如果采用平常的爬虫方法，每一次time.sleep几秒，这样久了，豆瓣自然会发现，然后把ip给封了，这样就没办法爬了，考核任务就泡汤了…\n题外话：time.sleep()设置的秒数最好是随机数，如果是固定的秒数，久而久之也很容易被封ip。\n所以，设置代理这种切换ip的方式就很适合短时间爬取大量数据\n代理有多种\n①自己去爬取免费的代理，建立自己的代理池，难度大，技术要求高，且大多免费代理都是用不了的。\n②使用付费代理\n本文中使用的是付费代理，叫多贝云代理，购买了http隧道代理中的套餐3，套餐的特点是：每个请求随机分配IP\n注：购买时需要实名认证\n获取分配的ip注：不同的付费代理不同的套餐获取ip的方式不同，根据官方指示即可\n购买后，多贝云会分配一个账号，密码和服务器地址给你\n\n根据他的指引构造代理参数即可\n\n于是我们向服务器地址端口请求，服务器即可返回一个可用的ip来伪装我们的ip\n访问网页函数于是我们定义一个访问网页的函数，返回响应。\ndef visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return res\n\n这里我还用了随机UA，就是找了不同的user-agent，每次访问从中随机选取一个ua。\n这里的headers是个列表\n\n解析网页按F12或右键检查进入“开发者选项”，利用图片中红框的功能可以迅速定位所提取信息的节点位置\n\n此处运用XPth和BeautifulSoup进行网页的解析，并将解析方法编写成类。\n注：建议对自己的解析方法多对几本书进行尝试，因为不同网页排版可能不同噢~\n爬取的字段为book_name（书名），author（作者），press（出版社），publishing_year（出版年份），page_num（页数），price（定价），ISBN，score（评分），rating_num（评分人数），content_introduction（内容简介），cover_url（封面图片网页链接），readers（读者的个人主页链接）\n# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\n注：字典的setdefault函数是添加键值对的一种方式，如果已有这个键则不添加，没有则添加。\n这可以窥探出我为什么在try-except语句的except中加上相应字段的setdefault。因为豆瓣的书籍间网页排布是不同的，它就是比较特殊，有些书没有ISBN，有些书没有评分和评论人数…所以以固定的方式去提取这些字段，必会报错，所以遇到这种报错时，进到except语句为数据赋上一些方便处理的空值。虽说我也不知道为什么之后还是有空值\n小插入一句，异常处理真的很重要！！！\ntry:    一些可能会出错的语句except Exception as e:    print(e)  # 这个print(e)可以让我们看到出错的原因，    pass  # 这里的pass你可以填入你异常处理的语句\n\n这样子所有的字段数据都存入了data中，之后我们实例化一个对象，对象.data即可查看我们爬取的数据啦。\n我们以《追风筝的人》为例：\n\n存储数据使用MySQL数据库进行数据的存储\n建表要根据爬取的字段建立相应的表（如果是新用户还要新建连接，这里就不多赘述了）\n可以用python建表，也可以用navicat（MySQL的一个可视化工具）建表。\n这里用python建表\npython通过第三方库pymysql与MySQL与数据库进行交互，对数据进行增删查改。\n首先要import pymysql，没有安装库的就去安装。\n直接贴代码：\nimport pymysqlconn = pymysql.connect(  # 连接本地数据库    host=&quot;localhost&quot;,    user=&quot;root&quot;,  # 要填root    password=&quot;htht0928&quot;,  # 填上自己的密码    database=&quot;doubanbook&quot;,  # 数据库名    charset=&quot;utf8&quot;)cur = conn.cursor()  # 获得光标create_books_table_sql = &quot;&quot;&quot;     CREATE TABLE `books`(    `book_name` VARCHAR(20) NOT NULL UNIQUE,    `author` VARCHAR(20) NOT NULL,    `press` VARCHAR(20),    `publishing_year` VARCHAR(10),    `score` FLOAT,    `rating_num` INTEGER,    `page_num` VARCHAR(10),    `price` VARCHAR(10),    `ISBN` VARCHAR(30),    `content_introduction` VARCHAR(2000),    `cover_url` VARCHAR(100),    `readers` VARCHAR (400)    )&quot;&quot;&quot;  # sql语句try:    cur.execute(create_books_table_sql)  # 执行sql语句except Exception as e:    print(e)    conn.rollback()  # 发生错误则回滚\n\n运行后即可建立相应的表。\n插入数据编写save_to_mysql函数\ndef save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到books这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;  # 向字段中增添相应的数据    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)  # 方便我们看到爬取的过程    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)\n\n注：提取字典数据时没用dict[‘key’]的方法获取的原因是如果没有这个字段，会直接报错，整个程序直接停下来。\n如果用.get，即是没有这个字段，get这个字段会返回None，而不是报错。\n在爬取过程中，我遇到了爬取成功但是却发现数据并没有存入数据库中的情况，查阅资料后发现是mysql锁住了，进入win系统重启MySQL服务即可。\n进行爬取流程介绍爬取的整个过程：进入每个标签页，获取该页的20本书的url，再进入每本书的url进行信息的提取\n提取标签\n\n进入标签页提取这页中20本书的url\n\n随后就是进入书籍页面爬取数据。\n编写相关函数def crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的第page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_url\n\ndef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)  # 因为爬了一定数据量后，代理会跳出proxyerror错误，停一会即可        time.sleep(2)    return list(l)\n\n之前没try-except语句总是爬一会就停，郁闷死我了。这种写法是我顿悟出来的，这样写之后，真的是飞快地爬。\n下面还会用到这样的写法\n主函数\nif __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(1)  # 获取每个标签页的第一页    # 我们可以通过对crawl_tags传入不同的页数，获取每个标签页的第n页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    # 换到另外一个标签的第1页\n\n爬取过程截图\n这是我保存下来的截图之一，可见，存储数据时经常会遇到一些我们意向不到的报错，所以异常处理真的很重要啊！！！\n少年，一定要学会用try-except语句啊！！！你刚开始学异常处理觉得没什么用，等你遭受过毒打就知道有多重要了！！！\n这是数据库中的部分数据（用了navicat）\n\n下载书籍图片因为我考核有一个界面要做书籍信息的展示，要贴书的封面图，所以还要下载下来。\n我们数据库中存储的字段里有图片链接（cover_url），我们提取出来，对每个链接进行访问，进行图片的下载。\n直接贴代码：（visit函数跟之前是一样的）\nimport osif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot; # 选取所有书    try:        cur.execute(sql_f)        results = cur.fetchall()  # 获得匹配结果        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)  # 创建books_cover目录，负责存放书籍封面图    os.chdir(&#x27;books_cover&#x27;)  # 切换到books_cover目录    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])  # 访问图片链接        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)  # 以书名为文件名下载图片        except:            print(&#x27;保存失败!&#x27;)    conn.close()  # 关闭python与mysql的连接\n\n示例截图：\n\n\n整体代码crawl_book文件# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\ncrawl文件import randomimport timeimport requestsimport refrom lxml import etreefrom bs4 import BeautifulSoupfrom crawl_book import CrawlBookimport pymysqlconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resdef crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_urldef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)        time.sleep(2)    return list(l)def save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到release这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)if __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(4)  # 获取第一页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(3)    # 换到另外一个标签的第1页    start = 20    for i in range(60):        new = start + i*20        url = &#x27;https://book.douban.com/tag/%E6%97%85%E8%A1%8C?start=&#x27; + str(new) + &#x27;&amp;type=T&#x27;        book_urls = get_book_urls(url)        for book_url in book_urls:            try:                res = visit(book_url)                book = CrawlBook(res)                save_to_mysql(book.data)            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    print(&#x27;爬取完成!&#x27;)    conn.close()  # 关闭连接，不然多了，数据库会锁\n\ndownload_image文件import timeimport pymysqlimport osimport requestsimport randomimport pandas as pdconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot;    try:        cur.execute(sql_f)        results = cur.fetchall()        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)    os.chdir(&#x27;books_cover&#x27;)    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)        except:            print(&#x27;保存失败!&#x27;)    conn.close()\n\n\n\n总结这是我5.14一天的爬虫过程（一周后的回顾）…还真是人不逼自己一把，就不知道自己的潜力有多大。\n这是代理帮我统计我一天的请求次数，1w8，我也没想到hhh\n\n其中遇到了很多的困难，数据插入问题，报错proxyerror，mysql锁住了…所幸都解决了\n解决的方式或是查阅资料，或是灵光乍现…\n那一天太累了，真的太累了，出现问题-&gt;解决问题-&gt;出现问题-&gt;解决问题-&gt;…\n感谢我的好朋友愿意陪我聊天（当我的文件传输助手），在我低落的时候给予我精神上的鼓励\n\n龙哥，我是你的粉丝啊！（飞踢飞扑）\n那么，此次的爬虫回顾结束🔚啦。\n","categories":["python"],"tags":["爬虫"]}]