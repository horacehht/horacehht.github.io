[{"title":"test","url":"/2021/05/27/test/","content":"测试终于搞好了555\n好开心…..\n","tags":["测试","试一下能不能多个标签"]},{"title":"利用hexo框架搭建个人博客","url":"/2021/05/29/%E5%88%A9%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","content":"利用hexo框架搭建个人博客安装git由于我之前安装了就不说了\n可以百度搜索🔍git官网。git是可以一直无脑下一步安装的。相信这不会难倒聪明的你\n安装nodejs进入官网选择对应的版本进行下载\n安装时一路next即可（路    径你自己选）\n安装成功后，win+R，输入cmd进入命令行，输入\nnode -vnpm -v\n\n检查是否安装成功\n如果呈现这样的就界面，则表示安装成功\n\n另外，可以用git bash代替命令行来敲命令\n安装hexo自己创建一个文件夹，我这里新建了一个blog文件夹，点进去后右键git bash here\ngit bash内输入hexo init name，这里的name你爱怎么填怎么填\n过程中可能会报错OpenSSL SSL_read: Connection was reset, errno 10054\n最有可能是网络不稳定，github经常这样，至于原因，大家懂得都懂，有时候科学上网之后也是这样。\n这里我输入了hexo init Horace\n\n\n出现这样的界面即初始化成功\n随后，cd Horace进入刚刚初始化的文件夹内，输入npm install\n\n随后输入\nhexo ghexo server\n\n或是输入hexo s\n打开hexo服务，浏览器输入localhost:4000，即可看到刚刚创建的博客了。长这样子：\n\n可以通过Ctrl+C停止hexo服务。\n停止服务后，再输入localhost:4000就登不上网站了。\n部署到云端这里可以看到，我们输入的网址是localhost:4000，并不是个静态链接\n而我们搭个人博客本身就是想别人来看的，如果别人都点不进来，那有什么意义啊？\n所以，要让我们的个人博客可以被别人访问到，我们可以将我们的网站部署到云端（不是唯一的方式）\n这里就让github托管我们的博客。\n创建github账户与创建对应仓库注册一个github账户，创建（new）一个仓库，仓库名叫用户名+.github.io\n我的用户名是horacehht，因此创建一个horacehht.github.io的仓库\n生成SSH添加到github参考廖雪峰博客git文章\n将hexo部署到云端通过npm install hexo-deployer-git --save命令下载部署的相应插件\n然后\nhexo cleanhexo generatehexo deploy\n\n但是之后访问http://horacehht.github.io报错404，说There isn&#39;t a GitHub Pages site here.\n最后通过github page的官方文档得知，如果要作为一个Github Pages仓库，需满足三个条件：\n\n仓库名为用户名+github.io\n仓库应设为public（公开）\n仓库内要创建一个README文档\n\nhexo的基本命令\n\n\n命令\n作用\n\n\n\nHexo init\n初始化博客\n\n\nHexo s\n运行博客\n\n\nHexo n title\n创建一篇新的文章，title为文章标题\n\n\nHexo c(clean)\n清理文件\n\n\nHexo g(GENERATE)\n生成静态文件\n\n\nHexo d(deploy)\n部署博客\n\n\n\n\nhexo项目的_config.yml是整个hexo项目的总配置文件，如果需要配置主题，则还有对应主题的配置文件。\n主题挑选挑选主题中…selecting\n1.Yun主题是我最喜欢的一个主题，但是因为一些特殊原因，怕审美疲劳，就不用了。demo：https://www.yunyoujun.cn/\ngithub地址：https://github.com/YunYouJun/hexo-theme-yun\n2.Anatole主题也可，很简洁，但不喜欢。demo：https://www.jixian.io/\ngithub地址：https://github.com/mrcore/hexo-theme-Anatole-Core\n3.Ayer主题。很全面的一个主题。demo：https://shen-yu.gitee.io/\ngithub地址：https://github.com/Shen-Yu/hexo-theme-ayer\n感觉非常不错。\n4.Particle主题。简洁。但文章没有目录。demo：https://korilin.com/\ngithub地址：https://github.com/korilin/hexo-theme-particle\n5.shoka主题。交互性强，字体好看，引用块好看。demo：https://shoka.lostyu.me/\ngithub地址：https://github.com/amehime/hexo-theme-shoka\n有点花\n6.Keep主题。切换自然。主页面简洁。demo：https://xpoet.cn/\ngithub地址：https://github.com/XPoet/hexo-theme-keep\n最终选定Keep这个主题！简洁又好看 \n使用Keep主题注：配置文件有两个：总配置文件，主题配置文件。下文提到时注意区分\n安装主题在git bash中执行命令\ngit clone https://github.com/XPoet/hexo-theme-keep themes/keep\n\n然后themes文件夹下出现我们想要的keep主题。\n\n这个为主题配置文件（这里踩了一些坑，通过npm安装的没有这个文件夹）\n使用主题安装完成后，在 Hexo项目的总配置文件中将 theme 设置为 keep。\ntheme: keep\n\n这个文件的路径为name\\_config.yml，我的为Horace\\_config.yml\n\nkeep会不定期更新版本，可通过如下命令更新Keep。\n\n通过 npm 安装最新版本：\n$ cd hexo-site（hexo项目的位置）$ npm update hexo-theme-keep\n\n或\n\n通过 git 更新到最新的 master 分支：\n$ cd themes/keep$ git pull\n\n配置指南复制主题配置文件。回到整个博客项目目录下的source文件夹，新建一个文件夹_data，将该文件粘贴进去\n\n将文件名更改为keep.yml，这很重要！\n然后通过该文件来进行相应的修改即可实现对主题的配置！\n这些是keep官方提供的配置资料，已经很全面了。\nhttps://keep-docs.xpoet.cn/usage-tutorial/quick-start.html#%E5%AE%89%E8%A3%85\nhttps://keep-docs.xpoet.cn/usage-tutorial/configuration-guide.html#base-info\nhttps://keep-docs.xpoet.cn/usage-tutorial/advanced.html\n这三个都是官方给的keep使用教程，分别为快速开始，配置指南和进阶使用，能满足大部分人的需求\n\n下面仅以base_info为例讲解，其他的配置项请参考官方资料\nbase_info项\n根据自己的内容进行填写\nbase_info:  title: Horaceの云端梦境  author: Horace  url: https://horacehht.github.io/  # 这里填上https://用户名.github.io\t如果自己注册了域名，就改成注册的  # 图标的链接，可以用本地的图片，也可用图片链接，或者不填  logo_img: https://horacehhtbucket.oss-cn-guangzhou.aliyuncs.com/img/网站图标.jpg  # 这里我填了一个网页链接\n\n读者可以稍微了解图床，这里我购买了阿里云的对象存储服务oss作为图床放置我的图片\n读者同样也可使用免费的图床：\n\nsm.ms\n\n路过图床\n\n\n利用阿里云作图床的文章：https://zhuanlan.zhihu.com/p/138878534\n小tips文章都放在source的_posts文件夹下\n文章如果想放在多个分类或多个标签下（前提是你开了分类和标签的功能），需要写成[a,b,c]的格式，如图：\n\n总结之后每次写新文章，就进git bash中敲\nhexo cleanhexo generatehexo deploy\n\n这样就能在自己的博客网站上看到新发布的文章了！\n如果嫌麻烦的可以参考这篇文章进行自动部署\n我的博客网站为：https://horacehht.github.io\n欢迎来访\n","categories":["hexo"]},{"title":"设置代理爬取豆瓣书籍","url":"/2021/05/29/%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E4%B9%A6%E7%B1%8D/","content":"设置代理爬取豆瓣书籍简介爬取网页大都可分为三个步骤：\n\n访问网页：一般使用requests库，不嫌麻烦的话可以使用python内置的urllib库\n解析网页：使用Xpath，BeautifulSoup，正则表达式等方式进行网页信息的提取\n存储数据：①存入IO流文件，如txt，csv等文件    ②存入数据库，主流的有MySQL，mongodb\n\n本文开发环境为python3.8，爬取的数据存入MySQL数据库中\n访问网页import requestsurl = &#x27;目标网址&#x27;res = requests.get(url)\n\n这样即可完成一次网页的访问。但一般都要加上请求头，称为headers。一般的网站请求的headers中加入User-Agent项参数即可。\n如果你用的是谷歌浏览器，可在网页栏输入chrome::version查看自己的User-Agent项，叫“用户代理”\n\n如果不是，也可以按F12，Ctrl+R，随便点进一个请求，Requests Headers项中的User-Agent就是你的User-Agent。\n于是代码应该改成这样\nheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;填入你的user-agent&#x27;    ...根据不同的网站加入不同的参数&#125;res = requests.get(url, headers=headers)\n\n不加的话，你的请求中User-Agent的值则是python爬虫，网站就会拒绝访问，这样你就无法得到网站返回的数据。\n设置代理为什么要设置代理？背景：本人要在某次考核任务最后一天中爬取数据量超过3k5的数据，时间紧，数据量说小不小，说大不大。如果采用平常的爬虫方法，每一次time.sleep几秒，这样久了，豆瓣自然会发现，然后把ip给封了，这样就没办法爬了，考核任务就泡汤了…\n题外话：time.sleep()设置的秒数最好是随机数，如果是固定的秒数，久而久之也很容易被封ip。\n所以，设置代理这种切换ip的方式就很适合短时间爬取大量数据\n代理有多种\n①自己去爬取免费的代理，建立自己的代理池，难度大，技术要求高，且大多免费代理都是用不了的。\n②使用付费代理\n本文中使用的是付费代理，叫多贝云代理，购买了http隧道代理中的套餐3，套餐的特点是：每个请求随机分配IP\n注：购买时需要实名认证\n获取分配的ip注：不同的付费代理不同的套餐获取ip的方式不同，根据官方指示即可\n购买后，多贝云会分配一个账号，密码和服务器地址给你\n\n根据他的指引构造代理参数即可\n\n于是我们向服务器地址端口请求，服务器即可返回一个可用的ip来伪装我们的ip\n访问网页函数于是我们定义一个访问网页的函数，返回响应。\ndef visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return res\n\n这里我还用了随机UA，就是找了不同的user-agent，每次访问从中随机选取一个ua。\n这里的headers是个列表\n\n解析网页按F12或右键检查进入“开发者选项”，利用图片中红框的功能可以迅速定位所提取信息的节点位置\n\n此处运用XPth和BeautifulSoup进行网页的解析，并将解析方法编写成类。\n注：建议对自己的解析方法多对几本书进行尝试，因为不同网页排版可能不同噢~\n爬取的字段为book_name（书名），author（作者），press（出版社），publishing_year（出版年份），page_num（页数），price（定价），ISBN，score（评分），rating_num（评分人数），content_introduction（内容简介），cover_url（封面图片网页链接），readers（读者的个人主页链接）\n# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\n注：字典的setdefault函数是添加键值对的一种方式，如果已有这个键则不添加，没有则添加。\n这可以窥探出我为什么在try-except语句的except中加上相应字段的setdefault。因为豆瓣的书籍间网页排布是不同的，它就是比较特殊，有些书没有ISBN，有些书没有评分和评论人数…所以以固定的方式去提取这些字段，必会报错，所以遇到这种报错时，进到except语句为数据赋上一些方便处理的空值。虽说我也不知道为什么之后还是有空值\n小插入一句，异常处理真的很重要！！！\ntry:    一些可能会出错的语句except Exception as e:    print(e)  # 这个print(e)可以让我们看到出错的原因，    pass  # 这里的pass你可以填入你异常处理的语句\n\n这样子所有的字段数据都存入了data中，之后我们实例化一个对象，对象.data即可查看我们爬取的数据啦。\n我们以《追风筝的人》为例：\n\n存储数据使用MySQL数据库进行数据的存储\n建表要根据爬取的字段建立相应的表（如果是新用户还要新建连接，这里就不多赘述了）\n可以用python建表，也可以用navicat（MySQL的一个可视化工具）建表。\n这里用python建表\npython通过第三方库pymysql与MySQL与数据库进行交互，对数据进行增删查改。\n首先要import pymysql，没有安装库的就去安装。\n直接贴代码：\nimport pymysqlconn = pymysql.connect(  # 连接本地数据库    host=&quot;localhost&quot;,    user=&quot;root&quot;,  # 要填root    password=&quot;htht0928&quot;,  # 填上自己的密码    database=&quot;doubanbook&quot;,  # 数据库名    charset=&quot;utf8&quot;)cur = conn.cursor()  # 获得光标create_books_table_sql = &quot;&quot;&quot;     CREATE TABLE `books`(    `book_name` VARCHAR(20) NOT NULL UNIQUE,    `author` VARCHAR(20) NOT NULL,    `press` VARCHAR(20),    `publishing_year` VARCHAR(10),    `score` FLOAT,    `rating_num` INTEGER,    `page_num` VARCHAR(10),    `price` VARCHAR(10),    `ISBN` VARCHAR(30),    `content_introduction` VARCHAR(2000),    `cover_url` VARCHAR(100),    `readers` VARCHAR (400)    )&quot;&quot;&quot;  # sql语句try:    cur.execute(create_books_table_sql)  # 执行sql语句except Exception as e:    print(e)    conn.rollback()  # 发生错误则回滚\n\n运行后即可建立相应的表。\n插入数据编写save_to_mysql函数\ndef save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到books这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;  # 向字段中增添相应的数据    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)  # 方便我们看到爬取的过程    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)\n\n注：提取字典数据时没用dict[‘key’]的方法获取的原因是如果没有这个字段，会直接报错，整个程序直接停下来。\n如果用.get，即是没有这个字段，get这个字段会返回None，而不是报错。\n在爬取过程中，我遇到了爬取成功但是却发现数据并没有存入数据库中的情况，查阅资料后发现是mysql锁住了，进入win系统重启MySQL服务即可。\n进行爬取流程介绍爬取的整个过程：进入每个标签页，获取该页的20本书的url，再进入每本书的url进行信息的提取\n提取标签\n\n进入标签页提取这页中20本书的url\n\n随后就是进入书籍页面爬取数据。\n编写相关函数def crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的第page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_url\n\ndef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)  # 因为爬了一定数据量后，代理会跳出proxyerror错误，停一会即可        time.sleep(2)    return list(l)\n\n之前没try-except语句总是爬一会就停，郁闷死我了。这种写法是我顿悟出来的，这样写之后，真的是飞快地爬。\n下面还会用到这样的写法\n主函数\nif __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(1)  # 获取每个标签页的第一页    # 我们可以通过对crawl_tags传入不同的页数，获取每个标签页的第n页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    # 换到另外一个标签的第1页\n\n爬取过程截图\n这是我保存下来的截图之一，可见，存储数据时经常会遇到一些我们意向不到的报错，所以异常处理真的很重要啊！！！\n少年，一定要学会用try-except语句啊！！！你刚开始学异常处理觉得没什么用，等你遭受过毒打就知道有多重要了！！！\n这是数据库中的部分数据（用了navicat）\n\n下载书籍图片因为我考核有一个界面要做书籍信息的展示，要贴书的封面图，所以还要下载下来。\n我们数据库中存储的字段里有图片链接（cover_url），我们提取出来，对每个链接进行访问，进行图片的下载。\n直接贴代码：（visit函数跟之前是一样的）\nimport osif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot; # 选取所有书    try:        cur.execute(sql_f)        results = cur.fetchall()  # 获得匹配结果        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)  # 创建books_cover目录，负责存放书籍封面图    os.chdir(&#x27;books_cover&#x27;)  # 切换到books_cover目录    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])  # 访问图片链接        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)  # 以书名为文件名下载图片        except:            print(&#x27;保存失败!&#x27;)    conn.close()  # 关闭python与mysql的连接\n\n示例截图：\n\n\n整体代码crawl_book文件# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\ncrawl文件import randomimport timeimport requestsimport refrom lxml import etreefrom bs4 import BeautifulSoupfrom crawl_book import CrawlBookimport pymysqlconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resdef crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_urldef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)        time.sleep(2)    return list(l)def save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到release这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)if __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(4)  # 获取第一页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(3)    # 换到另外一个标签的第1页    start = 20    for i in range(60):        new = start + i*20        url = &#x27;https://book.douban.com/tag/%E6%97%85%E8%A1%8C?start=&#x27; + str(new) + &#x27;&amp;type=T&#x27;        book_urls = get_book_urls(url)        for book_url in book_urls:            try:                res = visit(book_url)                book = CrawlBook(res)                save_to_mysql(book.data)            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    print(&#x27;爬取完成!&#x27;)    conn.close()  # 关闭连接，不然多了，数据库会锁\n\ndownload_image文件import timeimport pymysqlimport osimport requestsimport randomimport pandas as pdconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot;    try:        cur.execute(sql_f)        results = cur.fetchall()        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)    os.chdir(&#x27;books_cover&#x27;)    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)        except:            print(&#x27;保存失败!&#x27;)    conn.close()\n\n\n\n总结这是我5.14一天的爬虫过程（一周后的回顾）…还真是人不逼自己一把，就不知道自己的潜力有多大。\n这是代理帮我统计我一天的请求次数，1w8，我也没想到hhh\n\n其中遇到了很多的困难，数据插入问题，报错proxyerror，mysql锁住了…所幸都解决了\n解决的方式或是查阅资料，或是灵光乍现…\n那一天太累了，真的太累了，出现问题-&gt;解决问题-&gt;出现问题-&gt;解决问题-&gt;…\n感谢我的好朋友愿意陪我聊天（当我的文件传输助手），在我低落的时候给予我精神上的鼓励\n\n龙哥，我是你的粉丝啊！（飞踢飞扑）\n那么，此次的爬虫回顾结束🔚啦。\n","categories":["python"],"tags":["爬虫"]}]