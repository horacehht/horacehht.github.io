[{"title":"Linux学习","url":"/2022/02/15/Linux%E5%AD%A6%E4%B9%A0/","content":"Linux文件系统windows下有C，D盘的盘符区别，Linux没有盘符概念，只有一个根目录，以/为标识\n根目录下有这几个目录：/bin,/etc,/home,/lib,/usr\n作用详见：https://www.cnblogs.com/duanji/p/yueding2.html\n/home/用户名：存储用户相关的一些文档\n/usr：usr是user的缩写，安装的应用程序\n命令命令的格式为：cmd-option parameter\ncmd是命令，就是一个操作，必须要有\nparameter一般是要操作的对象\noption一般是用来修饰parameter的，用于表明parameter是怎样的对象\ncmd --help即可获得命令cmd的文档，介绍了命令的用法和作用，了解各参数的意义\nman rm：man是manual的缩写，可以打开rm的用户手册，用上箭头和下箭头进行移动，退出按q\npwd：（print work directory）可查看当前所在目录\nls：（list）列出当前所有的文件，蓝色的代表文件夹，白色的代表文件\ncd：（change directory）切换目录，\nclear：清空整个终端界面的输出内容\ncdcd ..：切换到上一级目录，也可跳两级，要用左斜杠/cd .：则是当前目录\ncd ../..：连跳两级目录\ncd ~：回到home目录\ncd -：切换回上次操作所在的目录\nlsls -l：以列表的形式列出（可以缩写成ll）\n\n可以看到文件创建的时间\n数字是目录/文件所占的字节大小\ntotal是目录下的文件占的总大小（单位为kB）\n\n但有隐藏的文件，那么怎么通过ls看到隐藏的文件呢？\nls -a：a是all的缩写，显示所有文件（包括隐藏文件）\n-a和-l可以一起使用，而-h要和-l一起使用才能起作用，不同在于显示的单位转换成了K\n不同的参数可以写在一块，比如说-lh，对顺序不敏感\nls -lha，其中有两个特殊的地方，出现了.，代表当前目录，出现也了..，代表上级目录，之所以出现了这两个，是需要记录文件夹的层级关系\n我们发现隐藏文件都是以.开头的\n通配符缩小范围进行ls查找\n*，匹配零个或多个任意字符，ls *.txt，则要求只列出以txt结尾的文件\n可以这么理解，ls后面可跟正则表达式\n?，匹配一个字符，ls ???.odt，只列出三个字母或数字的odt文件\n可以根据*？组合出不同的正则表达式来进行文件的查询\n[]，匹配中括号内任意一个字符，[1234]，匹配1或者2或者3或者4 ，也可以[1-4]匹配1到4内的任意一个数字\n[ar1]，匹配三个字符中的任意一个，[a-z]匹配任意小写字符，然而大写的字母并不行\ntouch用于创建文件，不能创建目录，touch 文件名.文件格式\nmkdir用于创建目录，mkdir 目录名，只能一级一级的创建目录\nrmremove的缩写，用于删除文件和目录\nrm test.txt：删除test.txt文件\n删除目录时需要添加一个参数-d，d是directory的意思，rm -d bb删除一个bb的目录\nmvmove的缩写，用于移动文件\nmv 123.txt bb：把123.txt文件移动到bb目录下，这是基于相对路径进行移动\nmv 123.txt /home/用户名/Desktop/bb：把123.txt文件移动到桌面的bb目录下，这基于绝对路径进行移动\n也可以移动目录，mv bb aa把bb目录移动到aa目录\ncpcopy的缩写，用于复制文件和目录\ncp a23.txt bb：把a23.txt拷贝到bb目录下\ncp -r bb/aa .：把bb目录下的aa目录拷贝到当前目录（移动目录时需要指定-r）\ncp a23.txt bb/a29.txt：把a23.txt拷贝到bb目录下，并重命名为a29\nwhich每个命令执行的时候，都会去执行一个程序。而which命令可以查看到命令程序所处的目录\n命令一般都放到/bin中，bin是binary二进制的意思\n但which cd是空的，这是因为cd是shell内置的命令\n带s的命令，则需要更高的权限\nfind具有文件搜索的功能，功能更强，速度受计算机影响\n精确搜索格式：find 搜索目录 搜索条件\n比如说要在/home目录下，搜索一个125.odt文件，此时需要加一个-name选项\n模糊搜索此时搜索条件中加上通配符即可，即写成正则表达式。\nfind /home 12*\nLinux中查找对大小写敏感，所以如果要忽略大小写，加一个-iname选项\n\n按文件大小搜索find 路径 -size +1，其中数字的单位为一个数据块，一个数据块为512k，而+号表示要搜索大于1数据块的文件/目录，-号则是小于\n按文件所属人find 路径 -user 用户名\n按文件被修改时间假如说电脑被黑客入侵了，知道大概的入侵时间，想知道什么文件被访问过了，就可以按文件修改时间来进行搜索\nfind 路径 -mmin -5 ，第一个m是modified的缩写，min说明单位是分钟，-号代表想查询小于5分钟之内的\nmmin是文件内容被修改的时间\namin选项用来查询上次被访问的时间\ncmin是文件属性（如权限）被修改的时间\n按照文件类型find 目录 -type 字母\n根据字母的不同，可以查询不同的文件类型\nf：（file）文件\nd：（directory）目录\nl：（link）软链接\n\n搜索条件复合利用-a连接两个条件达到and效果，连接的条件都需要符合\n-o连接两个条件达到or效果，连接的条件符合一个即可\n根据id进行搜索id是不重复的，首先我们要知道文件的id，此时我们可以用ls -i显示文件的id\nfind 路径 -inum id，inum是id number\ncat用于查看文件内容，cat 123.txt，查看123.txt文件内容\n-b：输出时文件内容时标出行号，空行不计入行数计算\n-n：与-b类似，但空行计入行数计算\nmore用于查看文件内容，与cat不同的点在于more分页展示。\ngrep用于文件内容的搜索，会显示出文件中包含关键字的那行内容\ngerp user 123.txt：在123.txt中查询user行的内容\n-n：搜索出来时标出搜索内容所在的行\n-i：搜索时忽略大小写\n-v：反向搜索，搜索不包含关键字的内容，如grep '#' 123.txt，在123.txt中输出不含#号的内容（即去掉注释）\n如果我们只想去掉以#开头的那部分内容，那就在#号前加上一个**尖号^**，表示以什么开头，如gerp ^'#' 123.txt\n如果只搜以s结尾的，则在内容后加上一个**美元符号$**，如gerp s$ 123.txt，表示从123.txt中搜索以s结尾的行\necho回显命令\necho hello，则会在控制台上输出hello\n还可以将内容写入一些文件中，如echo hello &gt; 123.txt，把hello写入123.txt中，但会覆盖掉原文件内容\n如果我们只是想追加内容到文件中，不覆盖原文件，我们把&gt;改成&gt;&gt;即可，echo hello &gt;&gt; 123.txt\nsudosudo可以获取到超管（root）权限，常加载一些需要超管权限的命令前\nsed用于文本内容的编辑\n比如要在train.py文件的最后一行追加一行，则可以这样写\nsed \"$a print('done!')\" train.py\n\na是addition\ntar对文件进行打包压缩\nps此命令用于显示当前进程的状态\nchmod用于更改文件权限\ntouch test.sh\n\n生成一个sh文件\n接着在.sh文件中输入\n#!bin/bashecho \"Hello World!\"\n\nchmod +x ./test.sh ./test.sh\n\n此时控制台则可输出Hello world\n+：增加文件的某个权限\n-：去除文件的某个权限\n读，写，执行权限分别对应r,w,x三个字母\nwget是一个文件下载工具，用于下载网络上的各种资源和软件\nTricktab按tab可自动补全，假如说cd Des，此时按下tab则可自动补全\n但如果按D，有多个匹配的，此时我们按两次tab，终端可输出搜索到的D开头的目录，此时会更方便我们进入想进入的目录\n上下方向键可回到上一条命令或下一条命令\n~波浪号~是home目录，cd ~可直接回到home目录\n隐藏文件/目录以.开头的文件和目录是隐藏的，如.test2.txt是隐藏文件\n文件或目录包含空格需要对这些文件或目录进行操作时，需要使用''对文件名或目录名进行包裹，即可进行操作\n重命名文件在使用mv命令时，可以进行重命名\nmv 123.txt ./124.txt：重命名为124.txt\n复制的时候页同样可以进行此操作\n命令结果存入相应文件中有时我们想将命令输出的内容保存到某个文件当中作记录，只需在原本命令后加入&gt; 文件名或&gt;&gt; 文件名\n详见如下图片\n\n\n管道为什么要用管道？\n把一个命令的输出，通过管道连接，可作为另一个命令的输入\n这里的输出指的是命令的结果，输入指的是命令的参数（cat xxx.txt  这个txt文件就是cat的参数）\n利用符号|进行管道连接\n\n硬链接与软链接Linux下的链接类似于windows下的快捷方式，Linux下分有两种链接方式：硬链接，软链接\n硬链接在Linux中，一个文件被创建时，文件也有其自身的inode（称为索引节点，相当于文件的地址），而硬链接是通过索引节点进行的链接。硬链接是创建一个具有相同inode但文件名不同的文件，所以删除硬链接对应的源文件，硬链接文件仍然存在，可以起到防误删的作用。\n与复制操作的区别相当于：复制会生成一个inode不同的文件。\n其通过ln或link命令创建，如ln oldfile newfile\n软链接也称符号链接，生成的软链接文件存放的内容是另一文件的路径的指向，可以对文件或目录创建。\n软链接主要应用于以下两个方面：\n一是方便管理，例如可以把一个复杂路径下的文件链接到一个简单路径下方便用户访问；\n另一方面就是解决文件系统磁盘空间不足的情况。例如某个文件文件系统空间已经用完了，但是现在必须在该文件系统下创建一个新的目录并存储大量的文件，那么可以把另一个剩余空间较多的文件系统中的目录链接到该文件系统中，这样就可以很好的解决空间不足问题。删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接就变成了死链接。\nln -s 目标目录 软链接地址，如ln -s /var/www/test test，当前路径创建test，其指向/var/www/test目录\n目标目录为软链接指向的目标目录，软链接地址为“快捷键”文件名称\n删除软链接用rm -rf即可\n修改软链接ln -snf 新目标目录 软链接地址\n下面为演示样例，桌面下有bb目录，bb目录下有aa目录\n此时我们在桌面下创建一个指向bb目录的软链接，命名为soft\n\n此时桌面上出现了一个soft，我们点开，发现进入了bb目录，目录显示为soft\n此时我们修改soft软链接，使其指向bb目录下的aa目录，ln -snf bb/aa soft，再重新打开soft\n\n成功地进入了aa目录\n其实也可以让软链接指向一个文件，这里我们修改soft指向bb目录下的125.txt文件\n\n但一般都不会让软链接指向文件\n关于用户Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。\n用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。\n添加用户useradduseradd [option] username\noption\n-g：指定用户所属的用户组\n如-g group1，则该用户属于group1组\n-d：指定用户主目录，若该目录不存在 ，则加上-m选项\nuseradd -d /home/sam -m sam，创建一个用户sam，其主目录在/home/sam下\n删除账号userdeluserdel [option] username\n常用的选项是 -r，它的作用是把用户的主目录一起删除。\n例如：\nuserdel -r sam\n\n\n\n用户口令管理passwd用户账号刚创建时账号被锁定，必须为其指定口令后方可使用\npasswd [option] username\n\n-l 锁定口令，即禁用账号。\n-u 口令解锁。\n-d 使账号无口令。\n-f 强迫用户下次登录时修改口令。\n\n","categories":["Linux"]},{"title":"win10 git bash 使用wget命令","url":"/2021/08/07/bash%E4%BD%BF%E7%94%A8wget%E5%91%BD%E4%BB%A4/","content":"Windows 10 git bash wget command not found因为我之前在git bash中试过敲Linux的命令，比如ls,cd,mkdir\n然后我搭服务器的时候就有用过wget来进行下载和tar来进行解压\n我就想着git bash应该也可以运行其他的Linux命令吧。\n于是敲了一下，报错wget command not found\n我就在网上查找资料，最后成功实现了git bash中运用wget命令\n步骤如下：\n1.进入该网址https://eternallybored.org/misc/wget/，根据你电脑的位数（32，64）来下载对应wget相关的exe\n\n（这里我下载了1.21.1版本的64-bit的exe）\n2.将该exe文件移至git目录下的mingw64的bin文件夹下\n我的是F:\\Git\\mingw64\\bin，这个因人而异\n3.将上面那个路径添加进环境变量\n\n之后就可以快乐地在git bash里敲wget命令啦！\n其他用不了的Linux命令也是这样解决\n","categories":["git"],"tags":["tips"]},{"title":"Clustering","url":"/2021/08/15/Trajectory%20Clustering%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","content":"论文阅读：Trajectory Clustering A Partition-and-Group Framework阅读论文的契机为暑期最终考核的主流向需求，需要阅读该篇论文进行实现。\n链接：http://hanj.cs.illinois.edu/pdf/sigmod07_jglee.pdf\n论文的组织结构如下：第1节，摘要。第2节概述了我们的轨迹聚类算法。第三节提出了第一阶段的轨迹划分算法。第四节提出了第二阶段的线段聚类算法。第5节介绍了实验评价的结果。第6节讨论了相关的工作。最后，第7节总结了论文。\n标题（Title）Trajectory Clustering: A Partition-and-Group Framework\n关键词：Partition-and-group framework, trajectory clustering, MDL principle, density-based clustering\n研究问题（Reasearch Question）轨迹聚类中相似的子轨迹，例子\n\n现有算法的缺陷（Shortcomings of existing algorithm）现有的轨迹聚类算法将类似的轨迹作为一个整体，从而发现共同的轨迹。\n但是，该文一个关键的发现是：这样的聚类方法会忽略掉一些共同的子轨迹，而这些子轨迹有着许多应用意义\n理论与方法（Theory and Method）提出了partition-and-group框架进行轨迹聚类\n将一个轨迹分成线段的集合，然后将类似的线段组成一个簇\n它能够从轨迹数据集中发现共同的子轨迹，而不是忽视了这些信息\n基于这个partition-and-group框架，再写出了一个TRA-CLUS的轨迹聚类算法\n该算法包含两个阶段：分区和分组。\n第一阶段：分区。提出了MDL（minimum description length）原则，辅助提出轨迹划分算法\n第二阶段：分组。提出基于密度的线段聚类算法，进行聚类\n算法步骤\n\n符号定义\n\n\n\n第i个轨迹点，d维\n\n\n\n​\n第i条轨迹数据\n\n\n\n第i条轨迹数据的长度\n\n\n\n轨迹数据集\n\n\n\n聚类形成的第i个类别\n\n\n​\n类别集合\n\n\n\n一条轨迹中的一条子线段\n\n\n一个轨迹可以属于多个类，因为一个轨迹可以被分为多条线段，我们在线段的角度上进行聚类\n我们假设出一条轨迹，它可以展现出线段的主要行为，那么我们称其为representative trajectory，它可以体现出共同的子轨迹\npartition-and-group首先每个轨迹被分为一组线段，然后根据我们的距离度量，彼此接近的线段被份组成一个簇，然后为每个集群生成一个有代表性的轨迹\n\n距离函数\n该函数由三个组件组成：\n（1）垂直距离\n（2）平行距离\n（3）角度距离\n现有两个d维线段 and ，这里的分别代表一个d维的点\n为不失一般性，视为长线段，为短线段\n垂直距离和分别为和在上的投影点\n令为与的欧几里德距离，为和​的欧几里得距离\n于是定义垂直距离为\n平行距离和分别为和在​上的投影点\n\n\n那么定义平行距离为\n角度距离是的长度，是和之间较小的那个夹角\n那么定义角度距离为$$d_{\\theta}(L_i,L_j) = \\left{\\right.$$\n总式\n根据应用程序决定，但是一般设定其默认值为1，一般都非常有效\nMDLminimum description length\n\n\n\n\n特征点的集合\n\n\n\n\n第i个轨迹的特征点集合的个数\n\n\n\n线段的长度\n\n\n包含两个成分：和 ​。H为hypothesis，D为data\n\n但时间复杂度太高了，所以寻求另一个近似解\nMDL近似解近似思想：将局部最优视为全局最优\n\n\n\n\n当与是线段间唯二的特征点时，计算线段的MDL代价\n\n\n\n​\n当和线段间没有特征点，该线段的MDL代价\n\n\n注：在​中​（根据定义，这是显然的，该段轨迹没有特征点，也就没有进行划分，所以划分后与划分前的距离差=0）\n（论文第5面）局部最优的情况：对于每个的k，满足最长的轨迹划分\n上面用人话来说，对于轨迹中的每一个点，如果将其作为特征点，它计算出来的代价比不将其作为特征点计算出来的代价小，那么就取其为特征点，反之则不取该点为特征点\n此外，尽可能地增加了轨迹划分的长度（我也不清楚）\n近似求解MDL的算法步骤\n\n还是很好理解的，思想就是保持这个MDL代价最小嘛。\n对轨迹中的每个点计算其划分代价和不划分代价，若该点划分代价大于不划分代价，则将其前一个点作为特征点；若划分代价小于不划分代价，那么就将长度+1。\nLemma 1（定理1）明确告诉了我们该算法时间复杂度为O(n)，n为轨迹中的点数。\n作者也提到这个近似解不一定能求到最优解，但是通过实验可知，该近似解的准确率高达80%，还是可以放心使用的\nLINE SEGMENT CLUSTERING线段密度（Density of Line Segments）是基于DBSCAN的密度聚类方法改进的，将点的概念换成线，并重新定义，在此不多赘述\n一些符号定义\n\n\n\n\n线段的集合\n\n\n\n$N_{\\varepsilon}(L_i) = {L_j \\in D\ndist(L_i,L_j) \\leq \\varepsilon }$\n\n\n\n一个人为设定的值\n\n\na core line segment\n核心线段\n\n\n\nw,r,t 是with respect to，是关于的意思\n\n核心线段：线段的​邻域中至少有MinLns个线段\n…\nClustering Algorithm与DBSCAN不同的是，并不是所有密度连通集都能成为一个簇。\n例如，考虑极端情况，一个密度连通集中的所有线段都是从一个轨迹中抽取出来的（可以想象一条线来回反复，在一个地方穿来穿去），我们为了防止这种聚类情况的出现（为什么要防止？因为我们的目标是发现不同轨迹间的共同之处，而不是一条轨迹），我们需要进行相关定义。\ntrajectory cardinality为一个聚类簇中的所涉及到的轨迹集合\n被称作聚类簇​的轨迹基数\n算法步骤：\n\nRepresentative Trajectory of a Cluster如何产生一个簇的代表性轨迹呢？\nRepresentative Trajectory\n一个代表性轨迹为一系列的点，此处的为聚类簇，范围为\n这些点是通过sweep line扫描线得以确定的。当沿簇群主轴方向扫描一条线时，我们数经过扫描线的线段条数\n何确定簇群的主轴（major axis）呢？为此，作者定义了平均方向向量。相加向量后归一化\n定义\nSuppose a set of vectors \n的平均方向向量定义如下其中，为V的基数（人话就是V有多少个向量）\n像上面说的那样，我们要计算与平均方向向量的平均坐标，为了简化运算，可以，利用一个旋转矩阵，计算完后再还原回去。\n\n生成代表性轨迹的算法步骤：\n\n计算平均方向向量，然后暂时性地变换轴\n根据旋转后的轴对起点和终点进行排序\n在按排序的顺序扫描起点和终点时，计算线段的数量，并计算这些线段的平均坐标\n\n\nHeuristic for Parameter Value Selection（参数选取的启发式方法）利用熵定义辅助找到最合适的线段密度聚类时的参数值\n$$where \\quad p(x_i) = \\frac {|N_{\\varepsilon}(x_i)|}{ \\sum {j=1} ^n |N{\\varepsilon}(x_j)|} \\quad and \\quad n=num_{ln}$$\n结论（conclusion）TRA-CLUS能够正确地从真实轨迹数据中发现共同的子轨迹\n实验（Experiment）欸嘿…暂时没看\n疑问（Question）\nMDL部分为什么要尽可能增加轨迹划分的长度\n\n一个小tips：\n基于密度的聚类方法是最适合线段的，因为它们可以发现任意形状的聚类，并且可以过滤掉噪声[11]。我们可以很容易地看到，线段簇通常是任意形状的，而轨迹数据库通常包含大量的噪声（也就是说异常值）\n","categories":["论文阅读记录"],"tags":["轨迹聚类"]},{"title":"test","url":"/2021/05/27/test/","content":"测试终于搞好了555\n好开心…..\n","tags":["测试","试一下能不能多个标签"]},{"title":"利用hexo框架搭建个人博客","url":"/2021/05/29/%E5%88%A9%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","content":"利用hexo框架搭建个人博客安装git由于我之前安装了就不说了\n可以百度搜索🔍git官网。git是可以一直无脑下一步安装的。相信这不会难倒聪明的你\n安装nodejs进入官网选择对应的版本进行下载\n安装时一路next即可（路    径你自己选）\n安装成功后，win+R，输入cmd进入命令行，输入\nnode -vnpm -v\n\n检查是否安装成功\n如果呈现这样的就界面，则表示安装成功\n\n另外，可以用git bash代替命令行来敲命令\n安装hexo自己创建一个文件夹，我这里新建了一个blog文件夹，点进去后右键git bash here\ngit bash内输入hexo init name，这里的name你爱怎么填怎么填\n过程中可能会报错OpenSSL SSL_read: Connection was reset, errno 10054\n最有可能是网络不稳定，github经常这样，至于原因，大家懂得都懂，有时候科学上网之后也是这样。\n这里我输入了hexo init Horace\n\n\n出现这样的界面即初始化成功\n随后，cd Horace进入刚刚初始化的文件夹内，输入npm install\n\n随后输入\nhexo ghexo server\n\n或是输入hexo s\n打开hexo服务，浏览器输入localhost:4000，即可看到刚刚创建的博客了。长这样子：\n\n可以通过Ctrl+C停止hexo服务。\n停止服务后，再输入localhost:4000就登不上网站了。\n部署到云端这里可以看到，我们输入的网址是localhost:4000，并不是个静态链接\n而我们搭个人博客本身就是想别人来看的，如果别人都点不进来，那有什么意义啊？\n所以，要让我们的个人博客可以被别人访问到，我们可以将我们的网站部署到云端（不是唯一的方式）\n这里就让github托管我们的博客。\n创建github账户与创建对应仓库注册一个github账户，创建（new）一个仓库，仓库名叫用户名+.github.io\n我的用户名是horacehht，因此创建一个horacehht.github.io的仓库\n生成SSH添加到github参考廖雪峰博客git文章\n将hexo部署到云端通过npm install hexo-deployer-git --save命令下载部署的相应插件\n然后\nhexo cleanhexo generatehexo deploy\n\n但是之后访问http://horacehht.github.io报错404，说There isn&#39;t a GitHub Pages site here.\n最后通过github page的官方文档得知，如果要作为一个Github Pages仓库，需满足三个条件：\n\n仓库名为用户名+github.io\n仓库应设为public（公开）\n仓库内要创建一个README文档\n\nhexo的基本命令\n\n\n命令\n作用\n\n\n\nHexo init\n初始化博客\n\n\nHexo s\n运行博客\n\n\nHexo n title\n创建一篇新的文章，title为文章标题\n\n\nHexo c(clean)\n清理文件\n\n\nHexo g(GENERATE)\n生成静态文件\n\n\nHexo d(deploy)\n部署博客\n\n\n\n\nhexo项目的_config.yml是整个hexo项目的总配置文件，如果需要配置主题，则还有对应主题的配置文件。\n主题挑选挑选主题中…selecting\n1.Yun主题是我最喜欢的一个主题，但是因为一些特殊原因，怕审美疲劳，就不用了。demo：https://www.yunyoujun.cn/\ngithub地址：https://github.com/YunYouJun/hexo-theme-yun\n2.Anatole主题也可，很简洁，但不喜欢。demo：https://www.jixian.io/\ngithub地址：https://github.com/mrcore/hexo-theme-Anatole-Core\n3.Ayer主题。很全面的一个主题。demo：https://shen-yu.gitee.io/\ngithub地址：https://github.com/Shen-Yu/hexo-theme-ayer\n感觉非常不错。\n4.Particle主题。简洁。但文章没有目录。demo：https://korilin.com/\ngithub地址：https://github.com/korilin/hexo-theme-particle\n5.shoka主题。交互性强，字体好看，引用块好看。demo：https://shoka.lostyu.me/\ngithub地址：https://github.com/amehime/hexo-theme-shoka\n有点花\n6.Keep主题。切换自然。主页面简洁。demo：https://xpoet.cn/\ngithub地址：https://github.com/XPoet/hexo-theme-keep\n最终选定Keep这个主题！简洁又好看 \n使用Keep主题注：配置文件有两个：总配置文件，主题配置文件。下文提到时注意区分\n安装主题在git bash中执行命令\ngit clone https://github.com/XPoet/hexo-theme-keep themes/keep\n\n然后themes文件夹下出现我们想要的keep主题。\n\n这个为主题配置文件（这里踩了一些坑，通过npm安装的没有这个文件夹）\n使用主题安装完成后，在 Hexo项目的总配置文件中将 theme 设置为 keep。\ntheme: keep\n\n这个文件的路径为name\\_config.yml，我的为Horace\\_config.yml\n\nkeep会不定期更新版本，可通过如下命令更新Keep。\n\n通过 npm 安装最新版本：\n$ cd hexo-site（hexo项目的位置）$ npm update hexo-theme-keep\n\n或\n\n通过 git 更新到最新的 master 分支：\n$ cd themes/keep$ git pull\n\n配置指南复制主题配置文件。回到整个博客项目目录下的source文件夹，新建一个文件夹_data，将该文件粘贴进去\n\n将文件名更改为keep.yml，这很重要！\n然后通过该文件来进行相应的修改即可实现对主题的配置！\n这些是keep官方提供的配置资料，已经很全面了。\nhttps://keep-docs.xpoet.cn/usage-tutorial/quick-start.html#%E5%AE%89%E8%A3%85\nhttps://keep-docs.xpoet.cn/usage-tutorial/configuration-guide.html#base-info\nhttps://keep-docs.xpoet.cn/usage-tutorial/advanced.html\n这三个都是官方给的keep使用教程，分别为快速开始，配置指南和进阶使用，能满足大部分人的需求\n\n下面仅以base_info为例讲解，其他的配置项请参考官方资料\nbase_info项\n根据自己的内容进行填写\nbase_info:  title: Horaceの云端梦境  author: Horace  url: https://horacehht.github.io/  # 这里填上https://用户名.github.io\t如果自己注册了域名，就改成注册的  # 图标的链接，可以用本地的图片，也可用图片链接，或者不填  logo_img: https://horacehhtbucket.oss-cn-guangzhou.aliyuncs.com/img/网站图标.jpg  # 这里我填了一个网页链接\n\n读者可以稍微了解图床，这里我购买了阿里云的对象存储服务oss作为图床放置我的图片\n读者同样也可使用免费的图床：\n\nsm.ms\n\n路过图床\n\n\n利用阿里云作图床的文章：https://zhuanlan.zhihu.com/p/138878534\n小tips文章都放在source的_posts文件夹下\n文章如果想放在多个分类或多个标签下（前提是你开了分类和标签的功能），需要写成[a,b,c]的格式，如图：\n\n总结之后每次写新文章，就进git bash中敲\nhexo cleanhexo generatehexo deploy\n\n这样就能在自己的博客网站上看到新发布的文章了！\n如果嫌麻烦的可以参考这篇文章进行自动部署\n我的博客网站为：https://horacehht.github.io\n欢迎来访\n","categories":["hexo"]},{"title":"小老鼠走迷宫","url":"/2022/01/08/%E5%B0%8F%E8%80%81%E9%BC%A0%E8%B5%B0%E8%BF%B7%E5%AE%AB/","content":"简介一只可怜的老鼠被放到我们设置的迷宫里啦！\n有人看不下去了，一个敲代码的决定给小老鼠找到出口。\n效果如下：不会走死路的聪明小老鼠\n\n分析符号规定\ne：迷宫出口\nm：迷宫入口，也就是初始点\n.：表明某个位置已被访问\n0：过道，老鼠可以走的路\n1：墙壁，老鼠不可穿过墙壁\n\n输入及输出在这里，我们用上述符号来构建困住老鼠的迷宫，如\n10100em11\n\n根据上述符号规定，老鼠从初始点走到出口只需三步。如果用坐标的形式表达，则是(3,1)-&gt;(2,1)-&gt;(2,2)-&gt;(2,3)。另外，我们会在周围加上一圈墙（即一圈1）\n那么我们程序的输入是几行字符序列，输出是走出迷宫所经过的坐标点。同时，为了方便表达坐标，我们不妨建一个Cell类描述迷宫坐标。\n那么我们就可以写出以下代码\nclass Cell{public:    Cell(int i = 0, int j = 0){\t\tx = i;        y = j;    }    bool operator== (const Cell&amp; c) const{        return x == c.x &amp;&amp; y == c.y;    }//重载运算符==，判断两个cell是否相等private:    int x, y;}\n\n\n\n输入部分\n#include&lt;stack&gt;Stack&lt;char*&gt; mazeRows;//记录用于构建迷宫的字符串char str[80], *s;int col, row = 0;cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字Cell exitCell, entryCell;while(cin &gt;&gt; str){    //读取一行，cin读到空格就判断是下一个字符串了    row++;// row用于记录用户输入的行数    cols = strlen(str);// 列数\ts = new char[cols+3];//因为需要加上两边的墙（2），再加上\\0作为字符串结尾，所以是+3    mazeRows.push(s);    strcpy(s+1,str);    s[0] = s[cols+1] = wall;    s[cols+2] = '\\0';    if (strchr(s,exitMarker) != nullptr) {//如果该行中有出口，则记录出口的坐标信息       exitCell.x = row;       exitCell.y = strchr(s,exitMarker) - s;    }    if (strchr(s,entryMarker) != nullptr) {       entryCell.x = row;       entryCell.y = strchr(s,entryMarker) - s;    }}\n\n\n\n走出迷宫一个老鼠在迷宫中，它可以向4个方向运动：上、下、左、右。\n我们不可能让它每次都随机选一个方向走，那么我们不妨规定：老鼠总是倾向于走上面，如果没得走，那就走下面，其次左，其次右。显然，这样的规定可能会走向错误的道路，比如说走到死胡同，则需要老鼠整理以前的信息（可以建立一个二维字符数组来记录哪些点老鼠走过），退回分岔路口，重新寻找一条通往出口的路。\n在这里，用栈这样的数据结构是很合适的，规定操作顺序，也可以实现回退。那么我们使用一个栈来记录老鼠所走过的路信息。当走到死路后，对栈进行回退，并基于之前走过路的标记信息，选择另外一条路进行尝试，重复多次，最终走出迷宫\n因为，整个程序背景是迷宫，为了练习C++语法，不妨建立一个迷宫类\nclass Maze {public:    Maze();    void exitMaze();//走出迷宫private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;//存放可以走的点    Stack&lt;Cell&gt; pStack;//存放走过的点    Stack&lt;Cell&gt; nStack;//方便输出的一个栈，p是positive，n是negative    char **store;//记录迷宫每点是否访问过    bool PointVisited(int , int);//该点被访问过，返回true    void pushUnvisited(int, int);//自定义的入栈操作，如果该点可以走就入栈，不能就不入栈    bool AroundVisited(int, int);//如果该点附近都被访问过，返回true1    int rows, cols;        //输出重载    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};\n\n\n\n我们着重于走到死胡同的情况，它如何正确回退到该回退的地方呢？其实我们换个角度考虑，走到了死胡同，为什么需要回退？是因为没有路走了，如果还有路可以走的话，它还会继续往下尝试，因为不知道接下来的是不是出口，撞到了南墙才知道“噢，没路了”。所以，我们要回退到一个附近存在未访问点的点，那么我们可以以此为判断条件，如果当前点不满足则继续回退。其他内容则不详细讲述思路。\n整体代码用类对上述代码稍加修改，变得更加整体性\nmaze.h#ifndef MAZE_MAZE_H#define MAZE_MAZE_H#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;cstring&gt;using namespace std;template&lt;class T&gt;class Stack : public stack&lt;T&gt; {public:    T pop() {        T tmp = stack&lt;T&gt;::top();        stack&lt;T&gt;::pop();        return tmp;    }//自定义pop，删除栈顶元素还获取栈顶元素的值};class Cell {//一格public:    Cell(int i = 0, int j = 0) {        x = i; y = j;    }    bool operator== (const Cell&amp; c) const {        return x == c.x &amp;&amp; y == c.y;    }private:    int x, y;    friend class Maze;//声明友元类Maze，Maze可以访问到Cell的私有数据    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell);};class Maze {public:    Maze();    void exitMaze();private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;    Stack&lt;Cell&gt; pStack;    Stack&lt;Cell&gt; nStack;    char **store;         // array of strings;    bool PointVisited(int , int);    void pushUnvisited(int, int);    bool AroundVisited(int, int);    int rows, cols;    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};#endif //MAZE_MAZE_H1\n\n\n\nmaze.cpp#include \"maze.h\"//关于cell的输出重载ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell) {    out &lt;&lt; \"(\" &lt;&lt; cell.x &lt;&lt; \",\" &lt;&lt; cell.y &lt;&lt; \")\";    return out;}Maze::Maze() : exitMarker('e'), entryMarker('m'), visited('.'),               passage('0'), wall('1') {//冒号后面是初始化，用括号内的值初始化前面的变量    Stack&lt;char*&gt; mazeRows;    char str[80], *s;    int col, row = 0;    cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     \t &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t \t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t \t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t \t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字    while (cin &gt;&gt; str) {        row++;        cols = strlen(str);        s = new char[cols+3];        mazeRows.push(s);        strcpy(s+1,str);        s[0] = s[cols+1] = wall;        s[cols+2] = '\\0';        //strchr(a,b)，在a中寻找b，如果找不到则返回nullptr        if (strchr(s,exitMarker) != nullptr) {//如果该行有出口，则记录其坐标信息            exitCell.x = row;            exitCell.y = strchr(s,exitMarker) - s;        }        if (strchr(s,entryMarker) != nullptr) {            entryCell.x = row;            entryCell.y = strchr(s,entryMarker) - s;        }    }    rows = row;    store = new char*[rows+2];//store用于记录迷宫点信息         store[0] = new char[cols+3];          for ( ; !mazeRows.empty(); row--) {        store[row] = mazeRows.pop();    }    store[rows+1] = new char[cols+3];     store[0][cols+2] = store[rows+1][cols+2] = '\\0';    for (col = 0; col &lt;= cols+1; col++) {        store[0][col] = wall;               store[rows+1][col] = wall;    }}bool Maze::PointVisited(int row, int col) {    if (store[row][col] == passage || store[row][col] == exitMarker) {        return false;//还没有被经过    }    else        return true;}bool Maze::AroundVisited(int row, int col) {    int cnt = 4;//可以走的数量    if (PointVisited(row-1, col))        cnt--;    if(PointVisited(row+1,col))        cnt--;    if(PointVisited(row,col-1))        cnt--;    if(PointVisited(row,col+1))        cnt--;    if(cnt == 0)        return true;//周围都被走过了    else        return false;}void Maze::pushUnvisited(int row, int col) {    if (!PointVisited(row, col)) {        mazeStack.push(Cell(row, col));    }}void Maze::exitMaze() {    int row, col;    currentCell = entryCell;    while (!(currentCell == exitCell)) {        pStack.push(currentCell);        row = currentCell.x;        col = currentCell.y;        if (!(currentCell == entryCell))            store[row][col] = visited;        bool up, down, left, right;        pushUnvisited(row-1,col);//按上下左右的顺序压栈        pushUnvisited(row+1,col);        pushUnvisited(row,col-1);        pushUnvisited(row,col+1);        if(AroundVisited(row, col)){            pStack.pop();//回退多少步该怎么确定呢？            Cell tmp = pStack.top();//查询这个点附近是否可以走，如果不行则继续回退            while (AroundVisited(tmp.x, tmp.y)){                pStack.pop();                if (!pStack.empty())//这个判断是防止完全就走不到出口的迷宫使程序崩溃                    tmp = pStack.top();                else{                    cout &lt;&lt; \"Failure\\n\";                    return;                }            }        }        if (mazeStack.empty()) {            cout &lt;&lt; \"Failure\\n\";            return;        }        else            currentCell = mazeStack.pop();    }    while (!pStack.empty())    {        nStack.push(pStack.pop());    }    while (!nStack.empty()){        Cell cur_cell = nStack.pop();        cout &lt;&lt; cur_cell;    }    cout &lt;&lt; exitCell;    cout &lt;&lt; \"\\nSuccess\\n\";}\n\nmain.cpp#include \"maze.h\"int main() {    Maze().exitMaze();    system(\"pause\");    return 0;}\n\n注意最后，在试玩程序的同时，需要注意的是，本程序需在.exe中运行，这是一位内ide内无法打出^Z，所以程序无法判断输入已停止\n输入完一行迷宫后记得按enter~\n","categories":["数据结构"],"tags":["栈","回溯"]},{"title":"可能是全网最详细的线性回归原理讲解！！！","url":"/2022/03/26/%E5%8F%AF%E8%83%BD%E6%98%AF%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3%EF%BC%81%EF%BC%81%EF%BC%81/","content":"可能是全网最详细的线性回归原理讲解！！！同内容讲解视频，不想看文字的可以看视频~\n线性回归，线性回归，首先我们要搞懂这四个字里两个词的意思\n何为线性？何为回归？\n线性线性，包括可加性和齐次性\n①可加性，也称叠加性。函数若满足下式则称函数具有可加性\n②齐次性，也称均匀性。若函数若满足下式其中，a为与x无关的常数。则称函数具有齐次性\n我们其实也可以用一个式子来描述这可加性与齐次性，如下当函数同时具有可加性与齐次性时，我们则称函数为线性函数\n回归回归是确定多个变量间相互依赖的定量关系\n在机器学习中，回归往往指预测的输出为连续值，而线性回归也确实是解决此类任务的\n分类任务则是预测的输出为离散型\n损失函数解决回归任务，实际就是找到一条线/超平面来拟合这些样本点，使他们之间的误差尽可能的小。而不同的线/超平面（其中不同的参数值形成的）在同一个数据集下对应着不同的误差，我们则需要找到让误差最小的线/超平面。\n现以形如y=kx+b的一元线性函数为例\n\n淡蓝色为样本点，深蓝色和红色的线为生成的两条线。那你们觉得哪条线跟样本点更加符合整体的趋势呢？也就是哪条线更拟合呢？\n你：这想都不用想的好吧！那肯定是蓝色那条线效果好啊！\n但它为什么好呢？我们要定量得去描述它。此时则需要引入损失函数（又称误差函数）来衡量误差\n回归任务中常用的损失函数有：\n\n均方误差MSE：\n\n均方根误差RMSE：\n\n平均绝对误差MAE：\n\nR-squared：\n\n\n\n可以这么理解：将TSS理解为全部按平均值预测，RSS理解为按模型预测，这就相当于去比较你模型预测和全部按平均值预测的比例，这个比例越小，则模型越精确。当然该指标存在负数的情况，即模型预测还不如全部按平均值预测。越接近1，模型拟合得就越好\n最小化损失函数再次强调：回归任务是拟合样本点，使误差尽可能的小\n最小二乘法（一元线性函数）我们用一元线性函数为例讲解线性回归，其次再引入多元线性回归\n此节以一元线性函数y=kx+b为例，采用均方误差MSE作为损失函数，那么损失函数就是关于变量k,b的函数其中，m为样本个数。此时任务为最小化L(k, b)函数\n相信大家在高中或是大学都做过求函数最小值的题，当时是怎么做的呢？求导！让导数=0，求出此时的x，此时的x让函数取得最小值点。但这里是两个变量，那么则求偏导，让偏导=0，求出此时的各个参数，此时的各个参数让损失函数取得最小值，也就是误差最小，也就是拟合效果最好！\n对b求偏导此时，我们对L函数求b的偏导，使用链式求导法则①第一行到第二行：k跟i无关，乘积项可直接提到连加号外面\n第二行到第三行：联想一下求均值的公式，实际上把m乘过去就是上面的替换\n同时，我们记第三行的式子为①，后续推导有用\n对k求偏导接着，我们对L函数求k的偏导，稍微复杂一些②记第三行得式子为②\n接下来式子①*得式子③③接着，②-③得我们现在就求出了使得函数值最小的k, b参数\n最小二乘法（多元线性函数）定义损失函数如今，我们将一元变量推广到多原变量，设多元函数式为这个式子太长了，我们使用线性代数的向量概念对该式进行整理，为方便，记（可以把上函数式的b视为b*1，下面会讲这样做的原因）\n此时稍微提一下，在线性代数中，见到一个向量，默认均为列向量，上标为T（转置）的才为行向量（至于为什么要这样规定，是因为竖着写很占版面…你知道它本身是竖着写的就好了）\n那么此时，我们构造一个权重向量和特征向量那么此时，我们上述的多元函数式则可以写成或，是完全等价的\n此时，我们可以损失函数写成如下形式其中，**为第i个真实值，为第i个样本的特征向量**\n注：真实值，标签值，样本值这三个词意思是相同的，后续阐述上可能会混用\n进一步化简损失函数此时，我们觉得还是觉得式子不够简洁，这个连加符号也太影响观感了！\n于是，此时可以继续利用线性代数，把连加号去掉，提升观感，同时也方便后续推导\n为了化简，我们需要定义两个东西\n①标签向量\n将m个样本标签值堆叠成一个标签向量（再提一遍没有T的是列向量）②样本矩阵X\n定义样本矩阵X，形状为(m, n+1)，m个样本，n+1个特征（其中，第1个特征为1）这里每一行即是一个样本，每一列即是某一个特征\n顺便解释一下上下标：上标(i)代表是第i个样本，下标j代表第j个特征（也可以理解成第j个维度）\n那么此时，样本矩阵X乘上权重向量w可得预测值向量，如图$$\\left(\\right)\n\\left(\\right) \n= \n\\left(\\right)$$\n我们还需要借助l2范数进行化简，此处简单介绍\nl2范数l2范数：向量各元素的平方和的平方根，即$|x|2 = \\sqrt{\\sum\\limits{i=1}^n x_i^2}$\n|| ||是范数符号，下标2表示其为2范数。\nl2范数有以下公式成立\n再再再化简损失函数损失函数可写成如下形式（如果不懂的话建议多看几遍）\n**注意矩阵和向量的大小，X：(m, n+1)，w：(n+1, 1)，y：(m, 1)**，开始化简第一行到第二行：利用公式\n第二行到第三行：利用转置的两个运算公式和\n第三行到第四行：矩阵满足分配律\n第四行到第五行：为什么和可以合并呢？这里就要回归到他们的大小上了\n:(1, n+1)，:(n+1, m)，:(m, 1)，他们最终相乘的大小是(1, 1)，(1, 1)实际上是一个数\n:(1, m)， X：(m, n+1)，w：(n+1, 1)，最终大小也是(1, 1)，也是一个数\n而，而一个实数的转置本就等于其本身，所以这俩是同一个数\n对L(w)求偏导\n矩阵求导的相关公式参考了该资料：https://zhuanlan.zhihu.com/p/273729929\n第二行到第三行的第一个偏导数：\n要用到这个公式\n\n此时将视为整体，也就是公式里的A，其大小为(n+1, n+1)，是常数方阵，符合公式的形式，于是\n第二行到第三行的第二个偏导数：\n要用到这个公式\n\n将视为整体，于是第四行到第五行为逆矩阵的知识\n梯度下降法梯度下降也是线性回归算法的一种求解方式\n关于梯度下降，马同学的回答非常直观且详细\nhttps://www.zhihu.com/question/305638940/answer/1639782992\n那么，当你看完这个回答后，可以理解，通过求梯度（这里也就是求对每个参数求偏导）和设定一个学习率（步长），经过指定次数迭代后可到达函数取到最小值的点，也就是获取了让误差值最小的权重向量w\n关于求L对w的偏导数，最小二乘法时已提及那么只需要经过多次迭代这就是利用梯度下降的线性回归原理\n","categories":["机器学习算法"],"tags":["线性回归","最小二乘法","梯度下降法"]},{"title":"插件aplayer的使用","url":"/2021/06/11/%E6%8F%92%E4%BB%B6aplayer%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"github文档：https://github.com/MoePlayer/hexo-tag-aplayer/blob/master/docs/README-zh_cn.md\n敲如下命令进行aplayer插件的安装\nnpm install --save hexo-tag-aplayer\n\n此处使用的是MetingJS。MetingJS 是基于Meting API 的 APlayer 衍生播放器，引入 MetingJS 后，播放器将支持对于 QQ音乐、网易云音乐、虾米、酷狗、百度等平台的音乐播放。\n如要使用该功能，keep主题配置文件请不要启用pjax，即pjax: false，否则无法使用。\n以网易云音乐上花に亡霊这首音乐为例：\n{% meting \"1442466883\" \"netease\" \"song\" %}\n\n\n    \n\n\n\n有关 {% meting %} 的选项列表如下:\n\n\n\n选项\n默认值\n描述\n\n\n\nid\n必须值\n歌曲 id / 播放列表 id / 相册 id / 搜索关键字\n\n\nserver\n必须值\n音乐平台: netease, tencent, kugou, xiami, baidu\n\n\ntype\n必须值\nsong, playlist, album, search, artist\n\n\nfixed\nfalse\n开启固定模式\n\n\nmini\nfalse\n开启迷你模式\n\n\nloop\nall\n列表循环模式：all, one,none\n\n\norder\nlist\n列表播放模式： list, random\n\n\nvolume\n0.7\n播放器音量\n\n\nlrctype\n0\n歌词格式类型\n\n\nlistfolded\nfalse\n指定音乐播放列表是否折叠\n\n\nstoragename\nmetingjs\nLocalStorage 中存储播放器设定的键名\n\n\nautoplay\ntrue\n自动播放，移动端浏览器暂时不支持此功能\n\n\nmutex\ntrue\n该选项开启时，如果同页面有其他 aplayer 播放，该播放器会暂停\n\n\nlistmaxheight\n340px\n播放列表的最大长度\n\n\npreload\nauto\n音乐文件预载入模式，可选项： none, metadata, auto\n\n\ntheme\n#ad7a86\n播放器风格色彩设置\n\n\n试试我自己QQ音乐的歌单，因为是歌单，所以要写playlist\n{% meting \"7855838128\" \"tencent\" \"playlist\" %}\n\n    \n\n\n\n使用mmedia插件\n.bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-xKolBjxFuTLODYAi\").style.height=document.getElementById(\"mmedia-xKolBjxFuTLODYAi\").scrollWidth*0.76+\"px\";\n    window.onresize = function(){\n      document.getElementById(\"mmedia-xKolBjxFuTLODYAi\").style.height=document.getElementById(\"mmedia-xKolBjxFuTLODYAi\").scrollWidth*0.76+\"px\";\n    }; \n\n{% mmedia \"bilibili\" \"bvid:BV1B5411M7Zf\" %}\n","categories":["hexo"],"tags":["插件"]},{"title":"聚类算法","url":"/2021/08/07/%E8%81%9A%E7%B1%BB%E5%AD%A6%E4%B9%A0/","content":"聚类参考资料：《机器学习》-周志华\n在无监督学习中，训练样本的标记信息是未知的，此类学习任务中研究最多、应用最广的是“聚类”\n聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”\n形式化地说，样本集包含m个无标记样本，每个样本​是一个n维特征向量\n聚类算法会将样本集D划分为k个不相交的簇，其中簇互斥，且所有簇的并集为数据集D\nK-Means算法步骤\n旁边还有一篇字：为避免运行时间过长，通常设置一个最大运行轮数或最小调整幅度阈值，若达到最大论述或调整幅度小于阈值，则停止进行\n我下面的代码实现为限制运行次数，sklearn也是限制迭代次数\n代码实现根据上述流程图，熟悉掌握numpy和pandas的函数使用，最后用seaborn进行聚类效果展示\n在jupyter notebook中分步运行\nimport numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom pandas import Seriesdf = pd.read_csv(\"points.txt\", sep=',', header=None)  # 我的点集数据为points.txt，分隔符为,df.columns=['x', 'y']sns.scatterplot('x','y',data=df)  # 绘制原图查看点的分布\n\n\n下面为我编写的一个的k_means\nclass k_means:    def __init__(self, X, k: int, max_iter=1000):        \"\"\"        X: 数据集，array形式        k: 聚类簇数        \"\"\"        self.X = np.array(X)        self.k = k        self.max_iter = max_iter        ori_index = np.random.choice(a=list(range(len(X))), size=self.k, replace=False, p=None)        self.points = X[ori_index]  # 存放均值向量    def cal_dist(self, x1, x2):        \"\"\"算两点间的欧氏距离\"\"\"        return np.sqrt(np.sum((x1 - x2) ** 2))    def classify(self):        \"\"\"        X: 点集，每一个样本是n维向量        \"\"\"        X = self.X        point_marks = dict()        for j in range(len(X)):            dist = []            for i in range(self.k):                # 算出样本x_j与各均值向量的距离                dist.append(self.cal_dist(X[j], self.points[i]))            mark = dist.index(min(dist))  # 选取距离最近的那个类作为标记            point_marks.setdefault(j, mark)  # 建立起联系，第j个点是第mark类        mark_record = Series(point_marks.values())  # 记录着每个点的类标记        # 计算新均值向量并更新        for i in range(self.k):            tem_points = X[mark_record[mark_record == i].index]  # 获取第i类样本点的数据            point_num, fea_num = tem_points.shape            for p in range(fea_num):                self.points[i][p] = tem_points[:, p].sum() / point_num        return mark_record    def fit(self):        \"\"\"不断进行迭代，最终获得一个记录划分的series对象\"\"\"        _series = None        for i in range(self.max_iter):            _dict = self.classify()        return _series\n\ndata = np.array(df)model = k_means(data, k=5, max_iter=500)record = model.fit()record.name = \"class\"new_df = pd.concat([df, record], axis=1)sns.scatterplot('x','y',data=new_df, hue=\"class\", palette=\"Set2\")\n\n\n\nk=5时的聚类效果图\n\n同时，我也使用了sklearn的KMeans聚类方法进行效果上的对比\nfrom sklearn.cluster import KMeansskmodel = KMeans(n_clusters=5, random_state=0,).fit(data)sk_label = Series(skmodel.labels_)sk_label.name = \"class\"sk_df = pd.concat([df, sk_label], axis=1)sns.scatterplot('x','y',data=sk_df, hue=\"class\", palette=\"Set2\")\n\n\n效果还没我的好\n高斯混合聚类首先要明白什么是（多元）高斯分布。\n对n维样本空间X中的向量，若服从高斯分布，其概率密度函数为其中，为n维均值向量，为的协方差矩阵。\n由上式可看出高斯分布完全由均值向量和协方差矩阵​这两个参数确定，为了明确其与相应参数的依赖关系，我们将概率密度函数记为\n我们可定义高斯混合分布该分布共由k个混合成分组成，每个混合成分对应一个高斯分布。其中为第i个高斯混合成份的参数，而为相应的混合系数\n算法步骤\n密度聚类顾名思义，基于密度的聚类，从样本密度的角度来考察样本之间的可连接性，并给予可连接样本不断扩展聚类簇以获得最终的聚类结果。\n著名算法有DBSCAN，它基于一组“领域”参数来刻画样本分布的紧密程度\n给定数据集，定义下面这几个概念：\n\n​邻域：对，其邻域包含样本集D中与的距离不大于的样本，即\n\n核心对象：若的邻域至少包含个样本，即，则为一个核心对象\n\n密度直达：若位于的邻域中，且为核心对象，则称由密度直达\n\n密度可达：对​和 ​，若存在样本序列，其中且由密度直达，则称由密度可达​\n\n密度相连：对​和 ​，若存在​使得​与​均有​密度可达，则称由与​密度项链\n\n\n\n基于这些定义，DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。\n给定邻域参数，簇是满足以下性质的非空样本子集：\n连接性：与密度相连\n最大性：由密度可达\n算法步骤\n代码实现我自己写的那个总是会在步骤16出问题，我查了下网上的资料，也只是觉得跟他采用的数据结构不太一样，我用的字典，他用的列表，就是不知道为啥我！的！不！行！可！恶！\n\n参考资料：\n最后整体的代码是这样的\nimport randomimport copyimport numpy as npclass DBSCAN:    def __init__(self, eps=0.5, min_samples=5):        self.eps = eps        self.min_samples = min_samples    def fit(self, X):        m, n = X.shape        k = 0  # 聚类簇数        neighbor_list = []        gama = set([x for x in range(len(X))])  # 初始时将所有点标记为未访问        labels = [-1 for _ in range(len(X))]  # 初始化每个点的类标签为-1        def find_neighbor(j):            \"\"\"找到j的邻居，返回集合\"\"\"            eps_domain = list()            for i in range(m):                dist = self.cal_dist(X[j], X[i])                if dist &lt;= self.eps:                    eps_domain.append(i)            return set(eps_domain)        def find_object():            \"\"\"找到核心对象集合\"\"\"            omega_list = []            for i in range(m):                neighbor = find_neighbor(i)                neighbor_list.append(neighbor)                if len(neighbor) &gt;= self.min_samples:                    omega_list.append(i)            return set(omega_list)        core_obj = find_object()        while len(core_obj) &gt; 0:            gama_old = copy.deepcopy(gama)            j = random.choice(list(core_obj))  # 随机选取一个核心对象            k = k + 1            Q = list()            Q.append(j)            gama.remove(j)            while len(Q) &gt; 0:                q = Q[0]                Q.remove(q)                if len(neighbor_list[q]) &gt;= self.min_samples:                    delta = neighbor_list[q] &amp; gama                    deltalist = list(delta)                    for i in range(len(delta)):                        Q.append(deltalist[i])                        gama = gama - delta            Ck = gama_old - gama            Cklist = list(Ck)            for i in range(len(Ck)):                labels[Cklist[i]] = k            core_obj = core_obj - Ck        return labels    def cal_dist(self, x1, x2):        \"\"\"算两点间的欧氏距离\"\"\"        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n\n\n进行绘图\nmodel = DBSCAN(eps=2, min_samples=15)df = pd.read_csv(\"points.txt\", sep=',', header=None)df.columns = ['x', 'y']data = np.array(df)labels = model.fit(data)record = Series(labels)record.name = \"class\"new_df = pd.concat([df, record], axis=1)sns.scatterplot('x', 'y', data=new_df, hue=\"class\", palette=\"Set2\")\n\n效果如下图：\n\n我觉得这个是我用的数据集中分类效果最好的一个算法，不过值得注意的是分类的效果非常依赖于epsilon和min_samples两个参数值，我调了七次才调出这种效果。\n相对来说，聚类的算法比之前学习的算法好实现很多，又复习了许多python的语法知识，不戳！结束聚类部分的学习\n","categories":["机器学习算法"],"tags":["聚类"]},{"title":"降维算法","url":"/2021/08/04/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/","content":"降维算法参考资料：【机器学习】【白板推导系列】\n引入背景降维，Dimensionality Reduction\n在机器学习过程中，我们更关注的是泛化误差，而不是训练误差。\n而过拟合问题解决的方法：\n\n数据量提升\n正则哈\n降维\n\n为什么降维会解决过拟合问题呢？又或者换一种问法：为什么维度高会造成一个过拟合呢？\n我们可以这样想：每增加一个维度，那我需要用来cover整个样本空间的量是以指数级别增长的（假设每个特征都是二值的，现有n个特征，再增加一个特征，那样本空间就增加了​个点，那就更别说多值的特征了）\n几何角度引入二维下，设正方形边长为1，那么他的面积则为1，其中包含一个内切圆，那么他的面积为\n三维下，设正方体边长为1，那么他的体积为1，其中包含一个内切球，那么他的体积为\n为了书写方便，我们令常数，则三维下的体积为\n那么，我们推广到D维空间，超立方体内含一个超球体，该超球体体积则为​，易得也就是说当维度不断增加这个超球体近乎一个空点（有点反直觉），那么很容易想到，数据会较容易分布在该球外，也就是分布在边缘，也就带来了数据稀疏性问题\n\n分类\n直接降维，即特征选择，选择出自己觉得重要的特征\n线性降维：PCA，MDS等\n非线性降维：流形学习，包括 Isomap，LLE 等\n\n预备知识假设数据集X为N×P形状，即N个样本，P个特征，那么一个样本则为p维向量\n\n多维下，样本均值​​（式）显然，样本均值为的p维向量\n协方差矩阵S（式）不难看出，协方差矩阵S为的矩阵\n根据之前的经验，我们可以通过线代的知识将连加号去掉\n\n由于只是差了一个转置，所以下面我只讨论左半边的式子，继续进行化简（式）其中，为形状为的全为1的列向量，即由式1可继续对式3进行化简\n式）\n不难得出形状为，我们将其记作，称作中心矩阵（centering matrix）\n于是，我们可以将式2化简成该中心矩阵H有一个良好的性质，​（读者可以自己试着推导一下），进而推导出\n因此，我们可以进一步化简式子，将式2化成（式）\nPCA主成分分析\n经典的主成分分析，顺口溜总结为一个中心，两个基本点\n一个中心将一组可能线性相关的变量通过线性变换变换成一组线性无关的变量\n换句话说就是对原始特征空间的重构，让特征正交变换一组线性无关的基\n两个基本点\n最大投影方差 \n最小重构距离\n\n两个是同一个东西\n图源自李航老师的《统计学习方法》，红框部分是PCA的选择新基的主要思想\n\n因此，我们就需要知道该投影方差怎么表示出来并最大化\n最大投影方差首先，我们拿到数据后，第一步要做数据中心化，数学表达为\n投影的表示在高中数学中，向量的数量积那么在上的投影​为\n如果我们**令被投影向量的模**，那么则恰好是在上的投影\n多维中在​上的投影则表示为\n投影方差我们记新基为，那么一个样本点的在上的投影为那么所有样本点的投影方差和在的约束条件下记为（二范式=1跟向量内积=1是等价的，所以约束条件换了一下）\n\n不难发现中间那项和式2很像，只差了一个\n所以我们可以近似地把写成那么现在的问题就转化为了\n\n于是采用拉格朗日乘数法，将约束问题化为无约束问题那么对其求偏导，令其=0得S为矩阵，为实数值，我们不难看出，​为协方差矩阵S的特征值，为对应的特征向量\n最小重构代价不想打公式了，直接在纸上写了，直接看图把\n\n那么现在的问题为\n\n实际上我们可以对每个基单独进行拉格朗日乘数法\n\n算法实现步骤参考资料：https://zhuanlan.zhihu.com/p/77151308\n设有 m 条 n 维数据。\n\n将原始数据按列组成 n 行 m 列矩阵 X；\n将 X 的每一行进行零均值化，即减去这一行的均值；\n求出协方差矩阵  ；\n求出协方差矩阵的特征值及对应的特征向量；\n将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；\n 即为降维到 k 维后的数据。\n\n稍微有点卡的步骤是5，其他倒也没有什么\n代码实现import numpy as npclass PCA:    def __init__(self, X):        \"\"\"输入一个numpy或pandas的特征矩阵\"\"\"        self.X = np.array(X).T        self.n, self.m = self.X.shape        self.equalization()        self.get_P()    def equalization(self):        \"\"\"数据均值化\"\"\"        for i in range(self.n):            self.X[i] = self.X[i] - self.X[i].mean()    def get_P(self):        \"\"\"求出矩阵P\"\"\"        C = (1/self.m)*np.dot(self.X, self.X.T)        w, v = np.linalg.eig(C)        # w是特征值，v是特征值对应的特征向量排列而成的矩阵        fea_rank_index = np.argsort(w)  # 返回的是一个指明特征值从小到大排序的索引        column = len(w)        pre_P = v[fea_rank_index[0]].reshape(-1, column)        for i in fea_rank_index[1:]:            # 根据特征值大小把对应的特征向量拼接上去            pre_P = np.concatenate([pre_P, v[fea_rank_index[i]].reshape(-1, column)])        self.pre_P = pre_P    def fit(self, k: int):        # 返回降到k维后的数据        Y = np.dot(self.pre_P[:k], self.X)        return Y.T\n\n算法评估去网上查了一下，降维算法没有一个固定的量化指标来评估，只能通过最终效果来判断（比如分类任务中，降维后的准确率，查全率是否提升）\n于是我采用乳腺癌数据集分类任务来做一下降维的评估\nfrom sklearn.datasets import load_breast_cancer  # 乳腺癌数据集from sklearn.model_selection import train_test_split  # 留出法from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix # 混淆矩阵import PCAdata = load_breast_cancer()X = data['data']Y = data['target'].reshape(-1, 1)  # 防止一个奇怪的Bugmethod = PCA.PCA(X)X_pca = method.fit(25) # 降成25维x_train, x_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2)  # 划分数据集skmodel = LogisticRegression(max_iter=2000)skmodel.fit(x_train, y_train)y_predict = skmodel.predict(x_test)test_score = skmodel.score(x_test, y_test)tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()print(\"——降维后——\")print(\"准确率：\" + str(test_score))print(\"查准率P：\" + str(tp/(tp+fp)))print(\"查全率R：\" + str(tp/(tp+fn)))x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)  # 划分数据集skmodel.fit(x_train, y_train)y_predict = skmodel.predict(x_test)test_score = skmodel.score(x_test, y_test)tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()print(\"——降维前——\")print(\"准确率：\" + str(test_score))print(\"查准率P：\" + str(tp/(tp+fp)))print(\"查全率R：\" + str(tp/(tp+fn)))\n\n这是为数不多降到25维后，降维后效果比降维前效果好的一次\n\n我还打算画一个图，画出对于乳腺癌数据集降到多少维效果（可以是准确率，查准率，查全率）会最好？\n\n准确率\n\n\n\n查准率P\n\n\n\n查全率\n\n\n三个图分开画似乎太麻烦了，试着一起画？\n其中，红色为准确率，绿色为查准率，蓝色为查全率\n\nfrom sklearn.datasets import load_breast_cancer  # 乳腺癌数据集from sklearn.model_selection import train_test_split  # 留出法from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix # 混淆矩阵import PCAimport matplotlib.pyplot as pltfrom pylab import mplmpl.rcParams['font.sans-serif'] = ['FangSong']  # 指定默认字体mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题data = load_breast_cancer()X = data['data']Y = data['target'].reshape(-1, 1)  # 防止一个奇怪的Bugmethod = PCA.PCA(X)l = list(range(10, 30))accuracy = []P = []R = []for i in l:    X_pca = method.fit(i) # 降成i维    x_train, x_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2)  # 划分数据集    skmodel = LogisticRegression(max_iter=5000)    skmodel.fit(x_train, y_train)    y_predict = skmodel.predict(x_test)    test_score = skmodel.score(x_test, y_test)    tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_predict).ravel()    accuracy.append(test_score)    P.append(tp/(tp+fp))    R.append(tp/(tp+fn))plt.plot(l, accuracy, color=\"r\", marker=\"o\")plt.plot(l, P, color=\"g\", marker=\"+\")plt.plot(l, R, color=\"b\", marker=\"*\")plt.xlabel('维数')plt.ylabel('评价效果')plt.show()\n\n性质\n缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；\n降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；\n过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；\n特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；\n\n细节零均值化当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来，不能使用验证集或者测试集的中心向量。\n其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。\n另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。\n其他降维方法MDS和核化线性降维，流形学习…\n由于PCA是最常用的降维算法，在此就不介绍其它了\n","categories":["机器学习算法"],"tags":["降维"]},{"title":"小老鼠走迷宫","url":"/2022/01/08/%E8%BF%B7%E5%AE%AB/","content":"简介一只可怜的老鼠被放到我们设置的迷宫里啦！\n有人看不下去了，一个敲代码的决定给小老鼠找到出口。\n效果如下：不会走死路的聪明小老鼠\n\n分析符号规定\ne：迷宫出口\nm：迷宫入口，也就是初始点\n.：表明某个位置已被访问\n0：过道，老鼠可以走的路\n1：墙壁，老鼠不可穿过墙壁\n\n输入及输出在这里，我们用上述符号来构建困住老鼠的迷宫，如\n10100em11\n\n根据上述符号规定，老鼠从初始点走到出口只需三步。如果用坐标的形式表达，则是(3,1)-&gt;(2,1)-&gt;(2,2)-&gt;(2,3)。另外，我们会在周围加上一圈墙（即一圈1）\n那么我们程序的输入是几行字符序列，输出是走出迷宫所经过的坐标点。同时，为了方便表达坐标，我们不妨建一个Cell类描述迷宫坐标。\n那么我们就可以写出以下代码\nclass Cell{public:    Cell(int i = 0, int j = 0){\t\tx = i;        y = j;    }    bool operator== (const Cell&amp; c) const{        return x == c.x &amp;&amp; y == c.y;    }//重载运算符==，判断两个cell是否相等private:    int x, y;}\n\n\n\n输入部分\n#include&lt;stack&gt;Stack&lt;char*&gt; mazeRows;//记录用于构建迷宫的字符串char str[80], *s;int col, row = 0;cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字Cell exitCell, entryCell;while(cin &gt;&gt; str){    //读取一行，cin读到空格就判断是下一个字符串了    row++;// row用于记录用户输入的行数    cols = strlen(str);// 列数\ts = new char[cols+3];//因为需要加上两边的墙（2），再加上\\0作为字符串结尾，所以是+3    mazeRows.push(s);    strcpy(s+1,str);    s[0] = s[cols+1] = wall;    s[cols+2] = '\\0';    if (strchr(s,exitMarker) != nullptr) {//如果该行中有出口，则记录出口的坐标信息       exitCell.x = row;       exitCell.y = strchr(s,exitMarker) - s;    }    if (strchr(s,entryMarker) != nullptr) {       entryCell.x = row;       entryCell.y = strchr(s,entryMarker) - s;    }}\n\n\n\n走出迷宫一个老鼠在迷宫中，它可以向4个方向运动：上、下、左、右。\n我们不可能让它每次都随机选一个方向走，那么我们不妨规定：老鼠总是倾向于走上面，如果没得走，那就走下面，其次左，其次右。显然，这样的规定可能会走向错误的道路，比如说走到死胡同，则需要老鼠整理以前的信息（可以建立一个二维字符数组来记录哪些点老鼠走过），退回分岔路口，重新寻找一条通往出口的路。\n在这里，用栈这样的数据结构是很合适的，规定操作顺序，也可以实现回退。那么我们使用一个栈来记录老鼠所走过的路信息。当走到死路后，对栈进行回退，并基于之前走过路的标记信息，选择另外一条路进行尝试，重复多次，最终走出迷宫\n因为，整个程序背景是迷宫，为了练习C++语法，不妨建立一个迷宫类\nclass Maze {public:    Maze();    void exitMaze();//走出迷宫private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;//存放可以走的点    Stack&lt;Cell&gt; pStack;//存放走过的点    Stack&lt;Cell&gt; nStack;//方便输出的一个栈，p是positive，n是negative    char **store;//记录迷宫每点是否访问过    bool PointVisited(int , int);//该点被访问过，返回true    void pushUnvisited(int, int);//自定义的入栈操作，如果该点可以走就入栈，不能就不入栈    bool AroundVisited(int, int);//如果该点附近都被访问过，返回true1    int rows, cols;        //输出重载    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};\n\n\n\n我们着重于走到死胡同的情况，它如何正确回退到该回退的地方呢？其实我们换个角度考虑，走到了死胡同，为什么需要回退？是因为没有路走了，如果还有路可以走的话，它还会继续往下尝试，因为不知道接下来的是不是出口，撞到了南墙才知道“噢，没路了”。所以，我们要回退到一个附近存在未访问点的点，那么我们可以以此为判断条件，如果当前点不满足则继续回退。其他内容则不详细讲述思路。\n整体代码用类对上述代码稍加修改，变得更加整体性\nmaze.h#ifndef MAZE_MAZE_H#define MAZE_MAZE_H#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;cstring&gt;using namespace std;template&lt;class T&gt;class Stack : public stack&lt;T&gt; {public:    T pop() {        T tmp = stack&lt;T&gt;::top();        stack&lt;T&gt;::pop();        return tmp;    }//自定义pop，删除栈顶元素还获取栈顶元素的值};class Cell {//一格public:    Cell(int i = 0, int j = 0) {        x = i; y = j;    }    bool operator== (const Cell&amp; c) const {        return x == c.x &amp;&amp; y == c.y;    }private:    int x, y;    friend class Maze;//声明友元类Maze，Maze可以访问到Cell的私有数据    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell);};class Maze {public:    Maze();    void exitMaze();private:    Cell currentCell, exitCell, entryCell;    const char exitMarker, entryMarker, visited, passage, wall;    Stack&lt;Cell&gt; mazeStack;    Stack&lt;Cell&gt; pStack;    Stack&lt;Cell&gt; nStack;    char **store;         // array of strings;    bool PointVisited(int , int);    void pushUnvisited(int, int);    bool AroundVisited(int, int);    int rows, cols;    friend ostream&amp; operator&lt;&lt; (ostream&amp; out, const Maze&amp; maze) {        for (int row = 0; row &lt;= maze.rows+2; row++)            out &lt;&lt; maze.store[row] &lt;&lt; endl;        out &lt;&lt; endl;        return out;    }};#endif //MAZE_MAZE_H1\n\n\n\nmaze.cpp#include \"maze.h\"//关于cell的输出重载ostream&amp; operator&lt;&lt; (ostream&amp; out, Cell&amp; cell) {    out &lt;&lt; \"(\" &lt;&lt; cell.x &lt;&lt; \",\" &lt;&lt; cell.y &lt;&lt; \")\";    return out;}Maze::Maze() : exitMarker('e'), entryMarker('m'), visited('.'),               passage('0'), wall('1') {//冒号后面是初始化，用括号内的值初始化前面的变量    Stack&lt;char*&gt; mazeRows;    char str[80], *s;    int col, row = 0;    cout &lt;&lt; \"请用以下符号来输入一个迷宫\"     \t &lt;&lt; \"m：初始点\" &lt;&lt; endl;\t \t &lt;&lt; \"e：出口\" &lt;&lt; endl;\t \t &lt;&lt; \"1：墙\" &lt;&lt; endl;\t \t &lt;&lt; \"0:过道\" &lt;&lt; endl;//一些提示性文字    while (cin &gt;&gt; str) {        row++;        cols = strlen(str);        s = new char[cols+3];        mazeRows.push(s);        strcpy(s+1,str);        s[0] = s[cols+1] = wall;        s[cols+2] = '\\0';        //strchr(a,b)，在a中寻找b，如果找不到则返回nullptr        if (strchr(s,exitMarker) != nullptr) {//如果该行有出口，则记录其坐标信息            exitCell.x = row;            exitCell.y = strchr(s,exitMarker) - s;        }        if (strchr(s,entryMarker) != nullptr) {            entryCell.x = row;            entryCell.y = strchr(s,entryMarker) - s;        }    }    rows = row;    store = new char*[rows+2];//store用于记录迷宫点信息         store[0] = new char[cols+3];          for ( ; !mazeRows.empty(); row--) {        store[row] = mazeRows.pop();    }    store[rows+1] = new char[cols+3];     store[0][cols+2] = store[rows+1][cols+2] = '\\0';    for (col = 0; col &lt;= cols+1; col++) {        store[0][col] = wall;               store[rows+1][col] = wall;    }}bool Maze::PointVisited(int row, int col) {    if (store[row][col] == passage || store[row][col] == exitMarker) {        return false;//还没有被经过    }    else        return true;}bool Maze::AroundVisited(int row, int col) {    int cnt = 4;//可以走的数量    if (PointVisited(row-1, col))        cnt--;    if(PointVisited(row+1,col))        cnt--;    if(PointVisited(row,col-1))        cnt--;    if(PointVisited(row,col+1))        cnt--;    if(cnt == 0)        return true;//周围都被走过了    else        return false;}void Maze::pushUnvisited(int row, int col) {    if (!PointVisited(row, col)) {        mazeStack.push(Cell(row, col));    }}void Maze::exitMaze() {    int row, col;    currentCell = entryCell;    while (!(currentCell == exitCell)) {        pStack.push(currentCell);        row = currentCell.x;        col = currentCell.y;        if (!(currentCell == entryCell))            store[row][col] = visited;        bool up, down, left, right;        pushUnvisited(row-1,col);//按上下左右的顺序压栈        pushUnvisited(row+1,col);        pushUnvisited(row,col-1);        pushUnvisited(row,col+1);        if(AroundVisited(row, col)){            pStack.pop();//回退多少步该怎么确定呢？            Cell tmp = pStack.top();//查询这个点附近是否可以走，如果不行则继续回退            while (AroundVisited(tmp.x, tmp.y)){                pStack.pop();                if (!pStack.empty())//这个判断是防止完全就走不到出口的迷宫使程序崩溃                    tmp = pStack.top();                else{                    cout &lt;&lt; \"Failure\\n\";                    return;                }            }        }        if (mazeStack.empty()) {            cout &lt;&lt; \"Failure\\n\";            return;        }        else            currentCell = mazeStack.pop();    }    while (!pStack.empty())    {        nStack.push(pStack.pop());    }    while (!nStack.empty()){        Cell cur_cell = nStack.pop();        cout &lt;&lt; cur_cell;    }    cout &lt;&lt; exitCell;    cout &lt;&lt; \"\\nSuccess\\n\";}\n\n","categories":["数据结构"],"tags":["栈","回溯"]},{"title":"设置代理爬取豆瓣书籍","url":"/2021/05/29/%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E4%B9%A6%E7%B1%8D/","content":"设置代理爬取豆瓣书籍简介爬取网页大都可分为三个步骤：\n\n访问网页：一般使用requests库，不嫌麻烦的话可以使用python内置的urllib库\n解析网页：使用Xpath，BeautifulSoup，正则表达式等方式进行网页信息的提取\n存储数据：①存入IO流文件，如txt，csv等文件    ②存入数据库，主流的有MySQL，mongodb\n\n本文开发环境为python3.8，爬取的数据存入MySQL数据库中\n访问网页import requestsurl = &#x27;目标网址&#x27;res = requests.get(url)\n\n这样即可完成一次网页的访问。但一般都要加上请求头，称为headers。一般的网站请求的headers中加入User-Agent项参数即可。\n如果你用的是谷歌浏览器，可在网页栏输入chrome::version查看自己的User-Agent项，叫“用户代理”\n\n如果不是，也可以按F12，Ctrl+R，随便点进一个请求，Requests Headers项中的User-Agent就是你的User-Agent。\n于是代码应该改成这样\nheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;填入你的user-agent&#x27;    ...根据不同的网站加入不同的参数&#125;res = requests.get(url, headers=headers)\n\n不加的话，你的请求中User-Agent的值则是python爬虫，网站就会拒绝访问，这样你就无法得到网站返回的数据。\n设置代理为什么要设置代理？背景：本人要在某次考核任务最后一天中爬取数据量超过3k5的数据，时间紧，数据量说小不小，说大不大。如果采用平常的爬虫方法，每一次time.sleep几秒，这样久了，豆瓣自然会发现，然后把ip给封了，这样就没办法爬了，考核任务就泡汤了…\n题外话：time.sleep()设置的秒数最好是随机数，如果是固定的秒数，久而久之也很容易被封ip。\n所以，设置代理这种切换ip的方式就很适合短时间爬取大量数据\n代理有多种\n①自己去爬取免费的代理，建立自己的代理池，难度大，技术要求高，且大多免费代理都是用不了的。\n②使用付费代理\n本文中使用的是付费代理，叫多贝云代理，购买了http隧道代理中的套餐3，套餐的特点是：每个请求随机分配IP\n注：购买时需要实名认证\n获取分配的ip注：不同的付费代理不同的套餐获取ip的方式不同，根据官方指示即可\n购买后，多贝云会分配一个账号，密码和服务器地址给你\n\n根据他的指引构造代理参数即可\n\n于是我们向服务器地址端口请求，服务器即可返回一个可用的ip来伪装我们的ip\n访问网页函数于是我们定义一个访问网页的函数，返回响应。\ndef visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return res\n\n这里我还用了随机UA，就是找了不同的user-agent，每次访问从中随机选取一个ua。\n这里的headers是个列表\n\n解析网页按F12或右键检查进入“开发者选项”，利用图片中红框的功能可以迅速定位所提取信息的节点位置\n\n此处运用XPth和BeautifulSoup进行网页的解析，并将解析方法编写成类。\n注：建议对自己的解析方法多对几本书进行尝试，因为不同网页排版可能不同噢~\n爬取的字段为book_name（书名），author（作者），press（出版社），publishing_year（出版年份），page_num（页数），price（定价），ISBN，score（评分），rating_num（评分人数），content_introduction（内容简介），cover_url（封面图片网页链接），readers（读者的个人主页链接）\n# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\n注：字典的setdefault函数是添加键值对的一种方式，如果已有这个键则不添加，没有则添加。\n这可以窥探出我为什么在try-except语句的except中加上相应字段的setdefault。因为豆瓣的书籍间网页排布是不同的，它就是比较特殊，有些书没有ISBN，有些书没有评分和评论人数…所以以固定的方式去提取这些字段，必会报错，所以遇到这种报错时，进到except语句为数据赋上一些方便处理的空值。虽说我也不知道为什么之后还是有空值\n小插入一句，异常处理真的很重要！！！\ntry:    一些可能会出错的语句except Exception as e:    print(e)  # 这个print(e)可以让我们看到出错的原因，    pass  # 这里的pass你可以填入你异常处理的语句\n\n这样子所有的字段数据都存入了data中，之后我们实例化一个对象，对象.data即可查看我们爬取的数据啦。\n我们以《追风筝的人》为例：\n\n存储数据使用MySQL数据库进行数据的存储\n建表要根据爬取的字段建立相应的表（如果是新用户还要新建连接，这里就不多赘述了）\n可以用python建表，也可以用navicat（MySQL的一个可视化工具）建表。\n这里用python建表\npython通过第三方库pymysql与MySQL与数据库进行交互，对数据进行增删查改。\n首先要import pymysql，没有安装库的就去安装。\n直接贴代码：\nimport pymysqlconn = pymysql.connect(  # 连接本地数据库    host=&quot;localhost&quot;,    user=&quot;root&quot;,  # 要填root    password=&quot;htht0928&quot;,  # 填上自己的密码    database=&quot;doubanbook&quot;,  # 数据库名    charset=&quot;utf8&quot;)cur = conn.cursor()  # 获得光标create_books_table_sql = &quot;&quot;&quot;     CREATE TABLE `books`(    `book_name` VARCHAR(20) NOT NULL UNIQUE,    `author` VARCHAR(20) NOT NULL,    `press` VARCHAR(20),    `publishing_year` VARCHAR(10),    `score` FLOAT,    `rating_num` INTEGER,    `page_num` VARCHAR(10),    `price` VARCHAR(10),    `ISBN` VARCHAR(30),    `content_introduction` VARCHAR(2000),    `cover_url` VARCHAR(100),    `readers` VARCHAR (400)    )&quot;&quot;&quot;  # sql语句try:    cur.execute(create_books_table_sql)  # 执行sql语句except Exception as e:    print(e)    conn.rollback()  # 发生错误则回滚\n\n运行后即可建立相应的表。\n插入数据编写save_to_mysql函数\ndef save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到books这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;  # 向字段中增添相应的数据    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)  # 方便我们看到爬取的过程    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)\n\n注：提取字典数据时没用dict[‘key’]的方法获取的原因是如果没有这个字段，会直接报错，整个程序直接停下来。\n如果用.get，即是没有这个字段，get这个字段会返回None，而不是报错。\n在爬取过程中，我遇到了爬取成功但是却发现数据并没有存入数据库中的情况，查阅资料后发现是mysql锁住了，进入win系统重启MySQL服务即可。\n进行爬取流程介绍爬取的整个过程：进入每个标签页，获取该页的20本书的url，再进入每本书的url进行信息的提取\n提取标签\n\n进入标签页提取这页中20本书的url\n\n随后就是进入书籍页面爬取数据。\n编写相关函数def crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的第page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_url\n\ndef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)  # 因为爬了一定数据量后，代理会跳出proxyerror错误，停一会即可        time.sleep(2)    return list(l)\n\n之前没try-except语句总是爬一会就停，郁闷死我了。这种写法是我顿悟出来的，这样写之后，真的是飞快地爬。\n下面还会用到这样的写法\n主函数\nif __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(1)  # 获取每个标签页的第一页    # 我们可以通过对crawl_tags传入不同的页数，获取每个标签页的第n页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    # 换到另外一个标签的第1页\n\n爬取过程截图\n这是我保存下来的截图之一，可见，存储数据时经常会遇到一些我们意向不到的报错，所以异常处理真的很重要啊！！！\n少年，一定要学会用try-except语句啊！！！你刚开始学异常处理觉得没什么用，等你遭受过毒打就知道有多重要了！！！\n这是数据库中的部分数据（用了navicat）\n\n下载书籍图片因为我考核有一个界面要做书籍信息的展示，要贴书的封面图，所以还要下载下来。\n我们数据库中存储的字段里有图片链接（cover_url），我们提取出来，对每个链接进行访问，进行图片的下载。\n直接贴代码：（visit函数跟之前是一样的）\nimport osif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot; # 选取所有书    try:        cur.execute(sql_f)        results = cur.fetchall()  # 获得匹配结果        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)  # 创建books_cover目录，负责存放书籍封面图    os.chdir(&#x27;books_cover&#x27;)  # 切换到books_cover目录    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])  # 访问图片链接        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)  # 以书名为文件名下载图片        except:            print(&#x27;保存失败!&#x27;)    conn.close()  # 关闭python与mysql的连接\n\n示例截图：\n\n\n整体代码crawl_book文件# 库导入部分import refrom lxml import etreefrom bs4 import BeautifulSoupclass CrawlBook:    def __init__(self, res):        try:            self.soup = BeautifulSoup(res.text, &#x27;lxml&#x27;)  # 初始化            self.html = etree.HTML(res.text)  # 初始化            self.data = dict()  # 生成一个空字典            self.get_book_name()            self.get_author()            self.get_many()            self.get_score()            self.get_rating_num()            self.get_content()            self.get_image()            self.get_readers()        except Exception as e:            self.data.setdefault(&#x27;author&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版社:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;出版年:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;页数:&#x27;, &#x27;0&#x27;)            self.data.setdefault(&#x27;定价:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;ISBN:&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;score&#x27;, 0.0)            self.data.setdefault(&#x27;rating_num&#x27;, 0)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)            self.data.setdefault(&#x27;readers&#x27;, &#x27;[]&#x27;)            print(e)    def get_book_name(self):        book_name = self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:itemreviewed&#x27;&#125;).string        # 找到节点名为span，属性property值为itemreviewd的节点，.string获取其文本内容        self.data.setdefault(&#x27;book_name&#x27;, book_name)  # 将其添加到字典中，下面的解析方法大同小异    def get_author(self):        author = self.soup.find(name=&#x27;span&#x27;, text=re.compile(&#x27;.*?作者.*?&#x27;)).next_sibling.next_sibling\\            .string.replace(&#x27;\\n&#x27;, &#x27;&#x27;).replace(&#x27; &#x27;, &#x27;&#x27;)        self.data.setdefault(&#x27;author&#x27;, author)    def get_many(self):        want_to_spider = [&#x27;出版社:&#x27;, &#x27;出版年:&#x27;, &#x27;页数:&#x27;, &#x27;定价:&#x27;, &#x27;ISBN:&#x27;]        for i in self.soup.find_all(name=&#x27;span&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;pl&#x27;&#125;):            if i.string in want_to_spider:                self.data.setdefault(i.string, i.next_sibling.replace(&#x27; &#x27;, &#x27;&#x27;))    def get_score(self):        score = float(self.soup.find(name=&#x27;strong&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:average&#x27;&#125;).string)        self.data.setdefault(&#x27;score&#x27;, score)    def get_rating_num(self):        rating_num = int(self.soup.find(name=&#x27;span&#x27;, attrs=&#123;&#x27;property&#x27;: &#x27;v:votes&#x27;&#125;).string)        self.data.setdefault(&#x27;rating_num&#x27;, rating_num)    def get_content(self):        l = &#x27;&#x27;        try:            for i in self.soup.find(name=&#x27;div&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;intro&#x27;&#125;).contents:                l += str(i)            self.data.setdefault(&#x27;content_introduction&#x27;, l.strip())        except Exception as e:            print(e)            self.data.setdefault(&#x27;content_introduction&#x27;, &#x27;&#x27;)    def get_image(self):        image_url = self.soup.find(name=&#x27;img&#x27;, attrs=&#123;&#x27;rel&#x27;: &#x27;v:photo&#x27;&#125;)[&#x27;src&#x27;]  # 获取节点的src属性值        self.data.setdefault(&#x27;cover_url&#x27;, image_url)    def get_readers(self):        readers = str(self.html.xpath(&#x27;//*[@id=&quot;collector&quot;]//div/div[2]/a/@href&#x27;))          # 这里用的是xpath的解析方法，获取所有属性值为collector下的所有div节点下的第二个div节点的a节点的href属性值        self.data.setdefault(&#x27;readers&#x27;, readers)\n\ncrawl文件import randomimport timeimport requestsimport refrom lxml import etreefrom bs4 import BeautifulSoupfrom crawl_book import CrawlBookimport pymysqlconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resdef crawl_tags(page):    &quot;&quot;&quot;获取每个标签网页的page页&quot;&quot;&quot;    url = &#x27;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&#x27;    res = visit(url)    html = etree.HTML(res.text)    tags = html.xpath(&#x27;//*[@id=&quot;content&quot;]//tr/td/a/text()&#x27;)    pages_url = [&#x27;https://book.douban.com/tag/&#x27; + tag + &#x27;?=&#x27; + str((page-1)*20) + &#x27;&amp;type=T&#x27; for tag in tags]    print(&#x27;已获取&#x27; + str(len(pages_url)) + &#x27;个标签网页&#x27;)    return pages_urldef get_book_urls(tag_url):    &quot;&quot;&quot;获取每个标签网页中的第一页，20本书&quot;&quot;&quot;    l = set()    i = 20    try:        res = visit(tag_url)        html = etree.HTML(res.text)        books_urls = html.xpath(&#x27;//*[@id=&quot;subject_list&quot;]/ul//li/div[2]/h2/a/@href&#x27;)        for book_url in books_urls:            l.add(book_url)        print(&#x27;已获取%d本书&#x27; % i)        i += 20    except Exception as e:        print(e)        print(&#x27;要停一会!休息2秒&#x27;)        time.sleep(2)    return list(l)def save_to_mysql(data):    &quot;&quot;&quot;data是书籍的信息，json格式，要插入到release这个表&quot;&quot;&quot;    book_name = data.get(&#x27;book_name&#x27;)    author = data.get(&#x27;author&#x27;)    press = data.get(&#x27;出版社:&#x27;)    publishing_year = data.get(&#x27;出版年:&#x27;)    page_num = data.get(&#x27;页数:&#x27;)    price = data.get(&#x27;定价:&#x27;)    ISBN = data.get(&#x27;ISBN:&#x27;)    score = data.get(&#x27;score&#x27;)    rating_num = data.get(&#x27;rating_num&#x27;)    content_introduction = data.get(&#x27;content_introduction&#x27;)    cover_url = data.get(&#x27;cover_url&#x27;)    readers = data.get(&#x27;readers&#x27;)    insert_data = (book_name, author, press, publishing_year, score, rating_num, page_num, price, ISBN,                   content_introduction, cover_url, readers)    insert_sql = &quot;&quot;&quot;        INSERT INTO books(book_name, author, press, publishing_year, score, rating_num, page_num, price,        ISBN, content_introduction, cover_url, readers)        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)    &quot;&quot;&quot;    try:        # 执行sql语句        cur.execute(insert_sql, insert_data)        # 提交执行        conn.commit()        print(&#x27;《&#x27; + book_name + &#x27;》&#x27; + &#x27;信息已存储成功!&#x27;)    except Exception as e:        print(e)        conn.rollback()        print(&#x27;存储失败!&#x27;)if __name__ == &#x27;__main__&#x27;:    tags_url = crawl_tags(4)  # 获取第一页    for page in tags_url:        book_urls = get_book_urls(page)  # 某个标签的第一页的书链接        for book_url in book_urls:            try:                res = visit(book_url)  # 对那一页的一本书进行访问                book = CrawlBook(res)  # 建立一个书对象，data存放其信息，以json存储                save_to_mysql(book.data)  # 将该书信息插入mysql中，继续第二本            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(3)    # 换到另外一个标签的第1页    start = 20    for i in range(60):        new = start + i*20        url = &#x27;https://book.douban.com/tag/%E6%97%85%E8%A1%8C?start=&#x27; + str(new) + &#x27;&amp;type=T&#x27;        book_urls = get_book_urls(url)        for book_url in book_urls:            try:                res = visit(book_url)                book = CrawlBook(res)                save_to_mysql(book.data)            except Exception as e:                print(e)                print(&#x27;歇一会QAQ，就2秒&#x27;)                time.sleep(2)    print(&#x27;爬取完成!&#x27;)    conn.close()  # 关闭连接，不然多了，数据库会锁\n\ndownload_image文件import timeimport pymysqlimport osimport requestsimport randomimport pandas as pdconn = pymysql.connect(  # 连接本地数据库        host=&quot;localhost&quot;,        user=&quot;root&quot;,  # 要填root        password=&quot;htht0928&quot;,  # 填上自己的密码        database=&quot;doubanbook&quot;,  # 数据库名        charset=&quot;utf8&quot;    )cur = conn.cursor()# 请求头headers = [    &#123;        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#x27;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;,    &#123;        &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36&quot;,        &#x27;Connection&#x27;: &#x27;close&#x27;&#125;    ]# http代理接入服务器地址端口proxyHost = &quot;http-proxy-t3.dobel.cn&quot;proxyPort = &quot;9180&quot;#账号密码proxyUser = &quot;HORACEC0JB9ONL0&quot;proxyPass = &quot;t7PG9y5o&quot;proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123;    &quot;host&quot; : proxyHost,    &quot;port&quot; : proxyPort,    &quot;user&quot; : proxyUser,    &quot;pass&quot; : proxyPass,&#125;proxies = &#123;    &quot;http&quot;  : proxyMeta,    &quot;https&quot; : proxyMeta,&#125;def visit(targetUrl):    &quot;&quot;&quot;访问网址，返回响应&quot;&quot;&quot;    session = requests.Session()    session.keep_alive = False    res = session.get(targetUrl, proxies=proxies, headers=random.choice(headers))    return resif __name__ == &#x27;__main__&#x27;:    sql_f = &quot;SELECT * FROM books&quot;    try:        cur.execute(sql_f)        results = cur.fetchall()        columnDes = cur.description  # 获取连接对象的描述信息        columnNames = [columnDes[i][0] for i in range(len(columnDes))]  # 获取列名        # 得到的results为二维元组，逐行取出，转化为列表，再转化为df        books_df = pd.DataFrame([list(i) for i in results], columns=columnNames)    except Exception as e:        print(e)    books_df = books_df.dropna(axis=0, subset=[&quot;cover_url&quot;])  # 去除cover_url有缺失值的行    if &#x27;books_cover&#x27; in os.listdir(os.getcwd()):        pass    else:        os.mkdir(&#x27;books_cover&#x27;)    os.chdir(&#x27;books_cover&#x27;)    for i in range(len(books_df)):        try:            res = visit(books_df.iloc[i][10])        except Exception as e:            print(e)            print(&quot;歇一会吧QAQ，就2秒&quot;)            time.sleep(2)        try:            print(&quot;正在保存第&quot; + str(i + 1) + &quot;张图片...&quot;)            with open(books_df.iloc[i][0] + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as f:                f.write(res.content)        except:            print(&#x27;保存失败!&#x27;)    conn.close()\n\n\n\n总结这是我5.14一天的爬虫过程（一周后的回顾）…还真是人不逼自己一把，就不知道自己的潜力有多大。\n这是代理帮我统计我一天的请求次数，1w8，我也没想到hhh\n\n其中遇到了很多的困难，数据插入问题，报错proxyerror，mysql锁住了…所幸都解决了\n解决的方式或是查阅资料，或是灵光乍现…\n那一天太累了，真的太累了，出现问题-&gt;解决问题-&gt;出现问题-&gt;解决问题-&gt;…\n感谢我的好朋友愿意陪我聊天（当我的文件传输助手），在我低落的时候给予我精神上的鼓励\n\n龙哥，我是你的粉丝啊！（飞踢飞扑）\n那么，此次的爬虫回顾结束🔚啦。\n","categories":["python"],"tags":["爬虫"]}]